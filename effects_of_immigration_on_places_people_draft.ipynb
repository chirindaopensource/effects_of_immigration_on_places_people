{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VivLqvgtwgQV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "# A General Decomposability Toolkit for Auditing the True Human Impact of Localized Economic Shocks\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2510.24225v1-b31b1b.svg)](https://arxiv.org/abs/2510.24225)\n",
        "[![Journal](https://img.shields.io/badge/Journal-Journal%20of%20Labor%20Economics-003366)](https://www.journals.uchicago.edu/loi/jole)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/effects_of_immigration_on_places_people)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Labor%20Economics-00529B)](https://github.com/chirindaopensource/effects_of_immigration_on_places_people)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-German_Social_Security_Records_--_IEB-lightgrey)](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SCIOLX)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Decomposition%20%7C%20FD--IV%20%7C%202SLS-orange)](https://github.com/chirindaopensource/effects_of_immigration_on_places_people)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Causal%20Inference%20%7C%20Immigration%20Economics-red)](https://github.com/chirindaopensource/effects_of_immigration_on_places_people)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![Matplotlib](https://img.shields.io/badge/matplotlib-%2311557c.svg?style=flat&logo=matplotlib&logoColor=white)](https://matplotlib.org/)\n",
        "[![linearmodels](https://img.shields.io/badge/linearmodels-003F72-blue)](https://bashtage.github.io/linearmodels/)\n",
        "[![Pydantic](https://img.shields.io/badge/Pydantic-E92063?logo=pydantic&logoColor=white)](https://pydantic-docs.helpmanual.io/)\n",
        "[![PyYAML](https://img.shields.io/badge/PyYAML-gray?logo=yaml&logoColor=white)](https://pyyaml.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/effects_of_immigration_on_places_people`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"The Effects of Immigration on Places and People - Identification and Interpretation\"** by:\n",
        "\n",
        "*   Christian Dustmann\n",
        "*   Sebastian Otten\n",
        "*   Uta Schönberg\n",
        "*   Jan Stuhler\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous data validation and preparation to the core econometric decompositions, heterogeneity analyses, structural parameter recovery, and the final generation of all tables and figures.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `execute_decomposition_toolkit_pipeline`](#key-callable-execute_decomposition_toolkit_pipeline)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the analytical framework presented in Dustmann, Otten, Schönberg, and Stuhler (2025). The core of this repository is the iPython Notebook `effects_of_immigration_on_places_people_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings. The pipeline is designed to be a generalizable toolkit for decomposing the aggregate (\"places-based\") effects of any localized economic shock into its underlying micro-level (\"people-based\") components.\n",
        "\n",
        "The paper's central argument is that standard estimates of immigration's regional effects are composites that mask distinct, policy-relevant mechanisms. This codebase operationalizes the paper's unifying framework, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a single `config.yaml` file.\n",
        "-   Process raw longitudinal spell data, applying a sequence of cleansing, imputation, and panel construction steps.\n",
        "-   Decompose the regional employment effect into **displacement**, **crowding-out**, and **relocation** components.\n",
        "-   Decompose the regional wage effect into a **pure price effect** and **compositional effects** from selective worker flows.\n",
        "-   Estimate all models using a robust 2SLS framework with a from-scratch **Wild Cluster Bootstrap** for inference.\n",
        "-   Run a full suite of heterogeneity analyses (e.g., by age, skill) and robustness checks.\n",
        "-   Recover underlying **structural economic parameters** (labor demand/supply elasticities) from the estimated coefficients.\n",
        "-   Automatically generate all key tables and figures from the paper.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in modern labor econometrics and causal inference, extending the canonical supply-and-demand model of a local labor market.\n",
        "\n",
        "**1. Regional vs. Pure Effects:**\n",
        "The framework distinguishes between the regional wage effect (`γ^R`), which is what is typically estimated with repeated cross-sections, and the pure wage effect (`γ^W`), which is the true change in the price of labor. The two are linked by the composition effect:\n",
        "$$\n",
        "\\frac{d\\log w^R}{dI^P} = \\frac{d\\log w}{dI^P} \\times (1 + \\tilde{\\eta}^E - \\tilde{\\eta}^P)\n",
        "$$\n",
        "where `(1 + ῆ^E - ῆ^P)` is the \"selectivity bias\" term, driven by the difference between efficiency-weighted (`ῆ^E`) and population-weighted (`ῆ^P`) labor supply elasticities.\n",
        "\n",
        "**2. Employment Decomposition:**\n",
        "The change in regional native employment is an accounting identity of three micro-level flows:\n",
        "$$\n",
        "\\frac{E_{r1} - E_{r0}}{E_{r0}} = -\\frac{E_{r,N}}{E_{r0}} (\\text{Displacement}) + \\frac{E_{\\{\\tilde{r},N\\},r}}{E_{r0}} (\\text{Crowding-Out}) - \\frac{E_{r,\\tilde{r}}}{E_{r0}} (\\text{Relocation})\n",
        "$$\n",
        "This implementation estimates the causal effect of the immigration shock on each of these components.\n",
        "\n",
        "**3. Identification Strategy:**\n",
        "-   **Instrumental Variable (2SLS):** To address the endogeneity of immigrant inflows (`ΔI_r`), the study uses distance to the border and its square as instrumental variables.\n",
        "-   **First-Difference IV (FD-IV):** To identify the pure wage effect (`γ^W`), the implementation uses an individual-level FD-IV model on the panel of \"stayers\" (workers who remain in the same region). This differences out time-invariant individual heterogeneity (`θ_i`).\n",
        "    $$\n",
        "    \\Delta \\log w_{ir} = \\gamma^W \\Delta I_r + \\delta' \\Delta X_i + \\Delta e_{ir}\n",
        "    $$\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`effects_of_immigration_on_places_people_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The entire pipeline is broken down into 25 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All study parameters are managed in an external `config.yaml` file, validated by a `Pydantic` schema.\n",
        "-   **Rigorous Data Validation:** A multi-stage validation process checks the schema, logical integrity, and consistency of all input data.\n",
        "-   **Advanced Data Preparation:** Includes Tobit-style imputation for censored wages and a high-performance, vectorized method for resolving spell data into a worker-year panel.\n",
        "-   **Robust Econometric Engine:** All estimations are performed using a master 2SLS function with a from-scratch Wild Cluster Bootstrap for reliable inference.\n",
        "-   **Complete Decomposition Suite:** Implements the full employment and wage decompositions, including the complex routine-task decomposition with occupational switching.\n",
        "-   **Comprehensive Heterogeneity & Robustness Analysis:** Includes dedicated modules for analyzing subgroups (older workers, non-employed) and for running a full suite of robustness checks (pre-trends, sensitivity, pseudo-panel).\n",
        "-   **Structural Parameter Recovery:** Concludes by using the estimated reduced-form coefficients to solve for underlying economic parameters.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation & Cleansing (Tasks 1-3):** Ingests and validates all inputs, normalizes dtypes, and transforms raw spell data into a canonical worker-year panel.\n",
        "2.  **Variable Construction (Tasks 4-8):** Imputes censored wages, builds the final analysis panel, aggregates to the regional level, and constructs the immigration shock and instrumental variables.\n",
        "3.  **Main Analysis (Tasks 9-13):** Prepares the final estimation dataset and runs the main regional employment and wage decompositions.\n",
        "4.  **Robustness & Heterogeneity (Tasks 14-19):** Executes selection bounding, analyzes key subgroups (non-employed, older workers, by task), and performs the detailed routine employment decomposition and apprenticeship analysis.\n",
        "5.  **Synthesis & Reporting (Tasks 20-25):** Recovers structural parameters, orchestrates the full pipeline, runs final robustness checks, and compiles all results into publication-quality tables and figures.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `effects_of_immigration_on_places_people_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 25 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `execute_decomposition_toolkit_pipeline`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`execute_decomposition_toolkit_pipeline`:** This master orchestrator function, located in the final section of the notebook, runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `pyyaml`, `faker`, `statsmodels`, `linearmodels`, `pydantic`, `pyproj`, `matplotlib`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/effects_of_immigration_on_places_people.git\n",
        "    cd effects_of_immigration_on_places_people\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy pyyaml Faker statsmodels linearmodels pydantic pyproj matplotlib\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires a main dataset and several auxiliary files, all with specific schemas that are rigorously validated. A synthetic data generator is included in the notebook for a self-contained demonstration.\n",
        "1.  **`consolidated_df_raw`**: The primary spell-level dataset (Parquet format).\n",
        "2.  **Auxiliary Files**: `border_crossings.csv`, `bibb_iab_task_mapping.csv`, `matched_controls.csv`.\n",
        "\n",
        "All other parameters are controlled by the `config.yaml` file.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `effects_of_immigration_on_places_people_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which demonstrates how to use the top-level `execute_decomposition_toolkit_pipeline` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # 1. Generate a full set of synthetic data files for the demonstration.\n",
        "    generate_synthetic_data(output_dir=\"./study_data/\")\n",
        "    \n",
        "    # 2. Load the master configuration from the YAML file.\n",
        "    with open('config.yaml', 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    # 3. IMPORTANT: Update file paths in the loaded config to point to the new data.\n",
        "    config['auxiliary_data_artifacts']['BORDER_CROSSING_TABLE']['path_or_handle'] = \"./study_data/border_crossings.csv\"\n",
        "    # ... (update other paths as needed) ...\n",
        "    \n",
        "    # 4. Define the path to the main raw data file.\n",
        "    raw_data_file_path = \"./study_data/consolidated_df_raw.parquet\"\n",
        "    \n",
        "    # 5. Execute the entire replication study.\n",
        "    final_results = execute_decomposition_toolkit_pipeline(\n",
        "        raw_data_path=raw_data_file_path,\n",
        "        master_config=config,\n",
        "        force_rerun_prep=True # Force re-run on first execution\n",
        "    )\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline returns a comprehensive dictionary containing all analytical artifacts, structured as follows:\n",
        "-   **`main_tables`**: Contains nested dictionaries for each result year, holding the full estimation outputs for all main decomposition and heterogeneity analyses.\n",
        "-   **`event_studies`**: Contains the full time-series results for analyses run over all years (e.g., apprenticeship uptake).\n",
        "-   **`robustness_checks`**: Contains results from all sensitivity and validation analyses.\n",
        "-   **`structural_parameters`**: Contains the final recovered economic parameters.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "effects_of_immigration_on_places_people/\n",
        "│\n",
        "├── effects_of_immigration_on_places_people_draft.ipynb  # Main implementation notebook\n",
        "├── config.yaml                                          # Master configuration file\n",
        "├── requirements.txt                                     # Python package dependencies\n",
        "│\n",
        "├── study_data/                                          # Directory for synthetic/real data\n",
        "│   ├── consolidated_df_raw.parquet\n",
        "│   └── ...\n",
        "│\n",
        "├── .pipeline_cache/                                     # Directory for cached artifacts\n",
        "│   ├── task3.pkl\n",
        "│   └── ...\n",
        "│\n",
        "├── LICENSE                                              # MIT Project License File\n",
        "└── README.md                                            # This file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can modify all study parameters, including file paths, sample selection criteria, and algorithm settings, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Generalization:** Abstracting the core decomposition logic to create a generic toolkit that can be applied to other local shocks (e.g., trade, automation) with minimal modification.\n",
        "-   **Alternative Estimation:** Incorporating alternative estimation techniques, such as difference-in-differences with staggered adoption or synthetic control methods.\n",
        "-   **Dynamic Panel Models:** Extending the analysis to account for dynamic adjustments and feedback effects over time.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{dustmann2025effects,\n",
        "  title={The Effects of Immigration on Places and People--Identification and Interpretation},\n",
        "  author={Dustmann, Christian and Otten, Sebastian and Sch{\\\"o}nberg, Uta and Stuhler, Jan},\n",
        "  journal={Journal of Labor Economics},\n",
        "  year={2025},\n",
        "  note={arXiv:2510.24225}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A General Decomposability Toolkit for Auditing the True Human Impact of Localized Economic Shocks: An Implementation of Dustmann et al. (2025).\n",
        "GitHub repository: https://github.com/chirindaopensource/effects_of_immigration_on_places_people\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Christian Dustmann, Sebastian Otten, Uta Schönberg, and Jan Stuhler** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, Matplotlib, Statsmodels, linearmodels, and Pydantic**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `effects_of_immigration_on_places_people_draft.ipynb` notebook and follows best practices for research software documentation.*\n"
      ],
      "metadata": {
        "id": "IzD89vPZr_1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*The Effects of Immigration on Places and People - Identification and Interpretation*\"\n",
        "\n",
        "Authors: Christian Dustmann, Sebastian Otten, Uta Schönberg, Jan Stuhler\n",
        "\n",
        "E-Journal Submission Date: 28 October 2025\n",
        "\n",
        "Journal Reference: Accepted at the Journal of Labor Economics\n",
        "\n",
        "Paper Link: https://arxiv.org/abs/2510.24225\n",
        "\n",
        "Replication Dataset Link: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SCIOLX\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Most studies on the labor market effects of immigration use repeated cross-sectional data to estimate the effects of immigration on regions. This paper shows that such regional effects are composites of effects that address fundamental questions in the immigration debate but remain unidentified with repeated cross-sectional data. We provide a unifying empirical framework that decomposes the regional effects of immigration into their underlying components and show how these are identifiable from data that track workers over time. Our empirical application illustrates that such analysis yields a far more informative picture of immigration's effects on wages, employment, and occupational upgrading.\n"
      ],
      "metadata": {
        "id": "Txu467wvwlH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **The Core Research Problem and Methodological Contribution**\n",
        "\n",
        "The paper's central thesis is that the canonical method for estimating the labor market effects of immigration—the \"spatial correlations\" approach using repeated cross-sectional data—is fundamentally flawed. This standard approach regresses changes in regional outcomes (like average wages or employment levels in a city) on changes in the local immigrant share.\n",
        "\n",
        "Dustmann et al. argue that the resulting coefficients are **composite effects**. They are amalgams of several distinct, policy-relevant mechanisms that cannot be separately identified with cross-sectional data. The paper's primary contribution is to develop a unifying framework that:\n",
        "1.  **Decomposes** these regional \"place-based\" effects into their underlying \"people-based\" components.\n",
        "2.  **Demonstrates** how longitudinal data, which tracks individual workers over time, is essential for identifying these separate components.\n",
        "3.  **Shows** that the distinction is not merely academic, as the magnitudes and interpretations of \"place\" versus \"people\" effects can differ dramatically.\n",
        "\n",
        "### **The Theoretical Framework and Key Decompositions**\n",
        "\n",
        "The authors extend the canonical supply-and-demand model of a local labor market. Their key innovation is to formalize how a regional effect is constructed from individual-level responses.\n",
        "\n",
        "**A. Decomposition of Employment Effects:**\n",
        "\n",
        "The standard approach estimates the **regional employment effect ($\\beta^R$)**, which is the total percentage change in native employment in a region following an immigration shock. The authors show through a simple identity (Equation 3) that this regional effect can be exactly decomposed into three components:\n",
        "\n",
        "1.  **Displacement Effect:** The effect on incumbent workers—natives employed in the region *before* the shock—who lose their jobs. This directly answers the public-policy question: \"Do immigrants take jobs from existing workers?\"\n",
        "2.  **Crowding-Out Effect:** The reduction in inflows of native workers into the region's labor market. This captures natives who would have taken jobs in the region (either from non-employment or other regions) but are now \"crowded out\" by immigrants.\n",
        "3.  **Relocation Effect:** The effect on incumbent workers who respond to the shock by moving to jobs in other regions.\n",
        "\n",
        "Critically, a large negative regional effect ($\\beta^R$) could be driven by a large displacement effect (incumbents are fired) or a large crowding-out effect (fewer new workers enter). These two scenarios have vastly different implications for the welfare of native workers.\n",
        "\n",
        "**B. Decomposition of Wage Effects:**\n",
        "\n",
        "Similarly, the standard approach estimates the **regional wage effect ($\\gamma^R$)**, which is the change in the average wage in a region. The authors demonstrate (Equations 6 and 7) that this effect is a composite of:\n",
        "\n",
        "1.  **The \"Pure\" Wage Effect ($\\gamma^W$):** The change in the price of labor for a worker of constant productivity. This is the theoretically \"pure\" effect of a labor supply shift along a fixed labor demand curve. It can be identified by observing the wage growth of natives who remain employed in the same region throughout the period (\"stayers\").\n",
        "2.  **Compositional Effects:** Changes in the average wage driven by changes in the *composition* of the workforce. Immigration can alter who is employed. If low-wage natives are more likely to exit employment (or high-wage natives are more likely to enter), the average wage in the region can increase, even if the pure wage effect for every individual worker is negative. This compositional change introduces a \"selectivity bias\" in traditional estimates.\n",
        "\n",
        "### **Empirical Strategy and Identification**\n",
        "\n",
        "To empirically test their framework, the authors leverage a powerful quasi-experimental setting:\n",
        "\n",
        "*   **The Shock:** A 1991 policy that allowed Czech workers to commute to jobs in the German border region without granting them residence rights. This represents a clean labor supply shock, as the commuters earned money in Germany but spent most of it in the Czech Republic, minimizing local demand effects.\n",
        "*   **Identification:** The immigrant inflow was geographically concentrated, declining sharply with distance from the border. This allows the authors to use an **instrumental variable (IV)** strategy, instrumenting the actual immigrant inflow in a municipality with its distance to the border (and its square). This addresses the endogeneity concern that immigrants might be drawn to areas with better economic prospects.\n",
        "*   **Data:** The analysis relies on German Social Security Records (IEB), a high-quality longitudinal administrative dataset covering the universe of workers in the social security system. This data is crucial as it allows them to track individual workers over time and across different employers, regions, and employment statuses.\n",
        "\n",
        "### **Main Empirical Findings on Employment**\n",
        "\n",
        "The results provide striking evidence for the paper's core thesis. For the period 1990-1993:\n",
        "\n",
        "*   **Regional Effect:** A 1 percentage point increase in the Czech immigrant share *decreased* total native employment in a municipality by **0.87%**. This is a large, significant effect, consistent with some of the more pessimistic findings in the literature.\n",
        "*   **Displacement Effect:** The same shock increased the probability that an *incumbent* native worker lost their job by only **0.14%**. This effect is small and fades to zero after five years.\n",
        "*   **The Source of the Gap:** The massive difference is almost entirely explained by the **crowding-out effect**. The immigrant inflow reduced the rate of new native hiring by **0.77%**.\n",
        "\n",
        "**Interpretation:** Immigration did not cause widespread job losses for existing native workers. Instead, it primarily reduced job-finding opportunities for natives who were not already employed in those specific local labor markets.\n",
        "\n",
        "### **Main Empirical Findings on Wages and Elasticities**\n",
        "\n",
        "The wage results tell a parallel story. For the period 1990-1993:\n",
        "\n",
        "*   **Regional Effect:** The estimated effect on regional average wages was close to zero (-0.008) and statistically insignificant. A researcher using cross-sectional data would conclude that immigration had no wage impact.\n",
        "*   **\"Pure\" Wage Effect:** By tracking incumbent workers, the authors find a statistically significant negative effect. A 1 percentage point increase in the immigrant share *decreased* the wages of continuously employed natives by **0.19%**.\n",
        "*   **The Source of the Gap:** The regional wage effect is biased toward zero due to **positive compositional selection**. The workers entering the affected labor markets were, on average, of higher \"quality\" (in terms of productivity) than the incumbents, while those leaving were also positively selected. This change in workforce composition almost perfectly offset the negative pure wage effect.\n",
        "\n",
        "**Implication for Economic Parameters:** Using the \"pure\" wage effect and the decomposed employment effects allows for a credible estimation of the inverse labor demand elasticity ($\\phi$), which they find to be -1.95 (implying a demand elasticity of -0.51). This is a standard value in the literature. In contrast, using the regional wage effect would lead to a near-infinite and implausible labor demand elasticity.\n",
        "\n",
        "### **Heterogeneity and Occupational Upgrading**\n",
        "\n",
        "The authors extend the analysis to explore heterogeneous effects and other adjustment margins:\n",
        "\n",
        "*   **Vulnerable Groups:** The negative effects are concentrated on specific groups. **Non-employed natives** and **older workers (50+)** experience significantly larger displacement and wage loss effects than the average incumbent.\n",
        "*   **Occupational \"Upgrading\":** The Czech commuters primarily entered routine-task occupations. While this led to a decline in native routine employment, the authors use their longitudinal data to show this was **not** because incumbent natives were \"upgrading\" to abstract jobs. Instead, the adjustment came from fewer new native workers entering routine jobs. They find evidence for a different kind of upgrading: young natives in affected regions were more likely to enter apprenticeship training rather than low-skilled employment, an investment in human capital.\n",
        "\n",
        "### **Conclusion and Broader Implications**\n",
        "\n",
        "The paper's conclusion is a powerful methodological warning. The effects of economic shocks **on places are not the same as the effects on people.** Regional-level estimates can be deeply misleading because they mask crucial dynamics of worker flows and compositional changes.\n",
        "\n",
        "The framework is highly portable. The same critique applies to other fields that rely on spatial correlation designs to study the impacts of local shocks, such as import competition (trade), automation (robotics), or fiscal policy. Without longitudinal data, it is difficult to know whether a negative regional employment effect reflects the firing of incumbent workers or a reduction in new hires, two phenomena with vastly different welfare implications."
      ],
      "metadata": {
        "id": "sqUVH5W74jCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "tKgCK_AllFif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  A General Decomposability Toolkit for Auditing the True Human Impact of\n",
        "#  Localized Economic Shocks\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"The Effects of Immigration on Places\n",
        "#  and People - Identification and Interpretation\" by Dustmann, Otten,\n",
        "#  Schönberg, and Stuhler (2025). It delivers a robust toolkit for decomposing\n",
        "#  aggregate, \"places-based\" economic impacts into their underlying, policy-\n",
        "#  relevant \"people-based\" components, such as displacement, crowding-out,\n",
        "#  and selection effects.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Causal effect estimation of local shocks via Instrumental Variables (2SLS).\n",
        "#  • Decomposition of regional employment effects into displacement, crowding-out,\n",
        "#    and relocation flows using longitudinal worker data.\n",
        "#  • Decomposition of regional wage effects into a \"pure\" price effect (purged\n",
        "#    of selection) and composition effects from worker inflows and outflows.\n",
        "#  • First-Difference IV (FD-IV) models on individual panel data to identify\n",
        "#    pure wage effects and test for occupational upgrading.\n",
        "#  • Tobit-style imputation for right-censored wage data.\n",
        "#  • Recovery of structural economic parameters (labor demand/supply elasticities)\n",
        "#    from reduced-form estimates.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • A complete, end-to-end pipeline from raw data validation to final output generation.\n",
        "#  • Robust inference via from-scratch Wild Cluster Bootstrap implementation.\n",
        "#  • A modular system of orchestrated, self-contained analysis functions.\n",
        "#  • Geospatial instrument construction (distance to border).\n",
        "#  • A comprehensive suite of robustness and sensitivity checks, including\n",
        "#    pre-trend analysis and pseudo-panel estimation.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Dustmann, C., Otten, S., Schönberg, U., & Stuhler, J. (2025). The Effects of\n",
        "#  Immigration on Places and People - Identification and Interpretation.\n",
        "#  Journal of Labor Economics.\n",
        "#  arXiv: https://arxiv.org/abs/2510.24225\n",
        "#  DOI: https://doi.org/10.1086/739196\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "# ==============================================================================\n",
        "# Consolidated Imports for the Entire Analysis Pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Core Data Handling and Numerical Computation ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- File System, Caching, and Utilities ---\n",
        "import time\n",
        "import pickle\n",
        "import copy\n",
        "import shutil\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Tuple, Set\n",
        "\n",
        "# --- Econometrics and Statistical Modeling ---\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from linearmodels.iv import IV2SLS\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# --- Configuration Validation ---\n",
        "from pydantic import BaseModel, Field, validator, ValidationError, conint, conlist, confloat\n",
        "from typing import Literal\n",
        "\n",
        "# --- Geospatial Analysis ---\n",
        "from pyproj import CRS, Transformer\n",
        "\n",
        "# --- Visualization ---\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n"
      ],
      "metadata": {
        "id": "-hnKUVESlLZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "EgWzWdC8lMhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### Documentation of All Orchestrator Callables\n",
        "\n",
        "#### **Task 1: `validate_consolidated_df_raw`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `consolidated_df_raw` (pd.DataFrame): The raw, unprocessed spell-level data.\n",
        "    *   `master_config` (Dict): The main configuration dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Schema Validation:** Verifies that the input DataFrame has the correct MultiIndex (`Worker_ID`, `Start_Date`), that all required columns are present, and that each column has a compatible data type (e.g., integer, float, datetime).\n",
        "    2.  **Integrity Validation:** Checks for internal consistency. It asserts that the MultiIndex is unique, that `Spell_Sequence_ID` is contiguous and monotonic for each worker, that `Start_Date <= End_Date` for all spells, and that the `Is_TopCoded_Wage` flag is logically consistent with the wage and cap values.\n",
        "    3.  **Geospatial/Policy Validation:** Confirms that coordinate columns are valid numbers and that geographic identifiers (`Municipality_ID`) are consistently mapped to higher-level geographies (`District_ID`). It also cross-validates the `Is_Border_Region` flag in the data against the district lists in the `master_config`.\n",
        "*   **Outputs:**\n",
        "    *   `bool`: Returns `True` if all checks pass. Raises a detailed `ValueError` if any check fails.\n",
        "*   **Role in Research Pipeline:** This function serves as the primary gateway to the entire pipeline. It is the first line of defense against corrupt or malformed input data, ensuring that all subsequent processing steps can rely on a dataset with a known, valid structure and internal consistency. It enforces the foundational assumptions about the raw data's quality.\n",
        "\n",
        "#### **Task 2: `validate_artifacts_and_config`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `master_config` (Dict): The main configuration dictionary.\n",
        "    *   `consolidated_df_raw` (pd.DataFrame): The raw data, used for cross-validation.\n",
        "*   **Processes:**\n",
        "    1.  **Artifact Validation:** Loads auxiliary data files specified in the config (e.g., `BORDER_CROSSING_TABLE`, `ROUTINE_ABSTRACT_MAPPING`). It validates their file paths, schemas, and content. A critical check ensures that the task mapping provides complete coverage for all `Occupation_Code`s found in the main dataset.\n",
        "    2.  **Configuration Validation:** Uses a `Pydantic`-based schema to rigorously validate the structure, types, and specific values of critical parameters within the `master_config` dictionary (e.g., `BASE_YEAR`, `BOOTSTRAP_REPLICATIONS`).\n",
        "*   **Outputs:**\n",
        "    *   `Dict[str, pd.DataFrame]`: A dictionary containing the loaded and validated auxiliary DataFrames (e.g., `{'border_crossings': df, 'task_mapping': df}`).\n",
        "*   **Role in Research Pipeline:** This function ensures the reproducibility and correctness of the analysis by validating all external dependencies. It guarantees that the parameters governing the analysis are exactly as specified by the replication instructions and that all necessary side-files (like geographic coordinates or occupational mappings) are present and correct.\n",
        "\n",
        "#### **Task 3: `cleanse_and_canonicalize_spells`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `consolidated_df_raw` (pd.DataFrame): The validated raw spell data.\n",
        "    *   `master_config` (Dict): The main configuration dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Normalization:** Coerces all columns to their canonical data types (e.g., strings to `datetime`) and standardizes string formats (e.g., zero-padding `Municipality_ID`). This step includes immediate self-validation to fail fast if coercion introduces errors.\n",
        "    2.  **Spell Resolution:** Transforms the spell-level data into a worker-year panel. It uses a highly efficient vectorized algorithm to identify the single \"main job\" for each worker at each annual snapshot date (June 30th) by applying a deterministic tie-breaking rule (highest wage, then longest duration, then earliest start date).\n",
        "    3.  **Flagging and Filtering:** Computes worker `age` and creates a comprehensive set of boolean flags (`is_full_time`, `is_older_worker`, `is_apprentice`) on the full panel. It then creates a second, filtered version of the panel that constitutes the main analysis sample by applying age and employment-type restrictions.\n",
        "*   **Outputs:**\n",
        "    *   `Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]`: A tuple containing three key DataFrames: the initial normalized spell data, the full unfiltered worker-year panel with all flags, and the filtered main analysis panel.\n",
        "*   **Role in Research Pipeline:** This is the core data transformation step. It converts the raw, event-based spell data into the clean, structured panel format that is the foundation for nearly all subsequent analyses.\n",
        "\n",
        "#### **Task 4: `impute_censored_wages`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `worker_year_panel` (pd.DataFrame): The cleansed worker-year panel from Task 3.\n",
        "    *   `master_config` (Dict): The main configuration dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Preparation:** Subsets the data to full-time workers and creates log-transformed wage and censoring cap columns.\n",
        "    2.  **Tobit Estimation:** For each cell defined by the interaction of `Gender_Code` and `District_ID`, it fits a censored-normal (Tobit) model via Maximum Likelihood Estimation to estimate the mean (`μ`) and standard deviation (`σ`) of the underlying latent log-wage distribution. It includes a robust fallback mechanism for sparse cells.\n",
        "    3.  **Imputation:** For each censored wage observation, it replaces the log-cap value with the conditional expectation of the wage given that it is above the cap. This is calculated using the estimated `μ` and `σ` for that observation's cell.\n",
        "        \\[ E[\\log w \\mid \\log w \\geq c] = \\mu + \\sigma \\frac{\\phi\\left(\\frac{c - \\mu}{\\sigma}\\right)}{1 - \\Phi\\left(\\frac{c - \\mu}{\\sigma}\\right)} \\]\n",
        "*   **Outputs:**\n",
        "    *   `pd.DataFrame`: The input `worker_year_panel` with a new column, `log_wage_imputed`, containing the final, corrected log wages.\n",
        "*   **Role in Research Pipeline:** This function corrects for the measurement error introduced by top-coding in administrative data. It provides an unbiased measure of wages, which is essential for accurately estimating the wage effects of immigration (`γ^R` and `γ^W`).\n",
        "\n",
        "#### **Task 5: `build_analysis_panel`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `analysis_panel_employed` (pd.DataFrame): The panel of employed workers with imputed wages.\n",
        "    *   `all_spells_cleansed` (pd.DataFrame): The full, normalized spell data.\n",
        "    *   `validated_artifacts` (Dict): The dictionary of auxiliary data.\n",
        "    *   `master_config` (Dict): The main configuration dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Grid Construction:** Creates a complete, balanced panel grid of all unique workers across all study years. It merges the employed panel onto this grid to identify non-employed worker-years.\n",
        "    2.  **Lookback Imputation:** For workers non-employed in 1990, it searches their 1986-1989 spell history to find their last known job characteristics, which are then attached to their 1990 observation.\n",
        "    3.  **Variable Enrichment:** Adds all remaining analysis variables to the full panel: `age_sq`, task labels (`Routine_or_Abstract_Label`, `Abstract_Intensity`) from the auxiliary mapping, `fte_weight`, nationality flags (`is_native`, `is_czech`), and categorical groups for the pseudo-panel analysis.\n",
        "*   **Outputs:**\n",
        "    *   `pd.DataFrame`: The definitive, fully specified `analysis_panel` where each row is a worker-year and all columns needed for any subsequent analysis are present.\n",
        "*   **Role in Research Pipeline:** This function produces the single, master micro-dataset for the project. All subsequent aggregation and estimation tasks draw from this single source of truth, ensuring consistency.\n",
        "\n",
        "#### **Task 6: `aggregate_to_regional_panel`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `analysis_panel` (pd.DataFrame): The final worker-year panel from Task 5.\n",
        "    *   `master_config` (Dict): The main configuration dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Employment Aggregation:** Groups the `analysis_panel` by `Municipality_ID` and `snapshot_year` to compute total FTE employment (`Total_rt`) and native FTE employment (`E_rt`).\n",
        "    2.  **Wage Aggregation:** For the subset of native, full-time workers, it computes the mean of `log_wage_imputed` for each municipality-year (`log_w̄_rt`).\n",
        "    3.  **National Wage Series:** It also computes the national average of `log_wage_imputed` for each year, which is needed for the non-employed imputation in Task 15.\n",
        "*   **Outputs:**\n",
        "    *   `Tuple[pd.DataFrame, pd.Series]`: A tuple containing (1) the `regional_panel` DataFrame with all aggregated municipality-year variables, and (2) the `national_wage_series`.\n",
        "*   **Role in Research Pipeline:** This function transforms the micro-level data into the macro-level (regional) data needed for the \"places-based\" analyses.\n",
        "\n",
        "#### **Task 7: `construct_immigration_shock`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `analysis_panel` (pd.DataFrame): The worker-year panel.\n",
        "    *   `regional_panel` (pd.DataFrame): The municipality-year panel.\n",
        "    *   `master_config` (Dict): The main configuration dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Numerator Calculation:** Computes the change in the FTE of Czech workers in each municipality between the specified start and end years (1990-1992 for the main shock, 1990-1991 for the 1991 event-year shock).\n",
        "    2.  **Denominator Calculation:** Extracts the total FTE employment of all workers in each municipality in the baseline year (1990).\n",
        "    3.  **Shock Calculation:** Divides the numerator by the denominator for each municipality.\n",
        "        \\[ \\Delta I_r = \\frac{\\text{Czech}^{92}_r - \\text{Czech}^{90}_r}{\\text{Total}^{90}_r} \\]\n",
        "*   **Outputs:**\n",
        "    *   `pd.DataFrame`: A DataFrame indexed by `Municipality_ID` containing the two shock variables (`shock_main` and `shock_1991`).\n",
        "*   **Role in Research Pipeline:** This function constructs the key independent variable (the \"treatment\") for all causal estimations.\n",
        "\n",
        "#### **Task 8: `construct_instrumental_variables`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `analysis_panel` (pd.DataFrame): The worker-year panel (to get municipality coordinates).\n",
        "    *   `validated_artifacts` (Dict): Contains the validated `border_crossings` DataFrame.\n",
        "    *   `master_config` (Dict): The main configuration dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Coordinate Harmonization:** Transforms the municipality and border crossing coordinates into a consistent system (e.g., WGS84 for great-circle distance) using `pyproj`.\n",
        "    2.  **Distance Calculation:** For each municipality, it calculates the minimum distance to any border crossing using either the Haversine formula or Euclidean distance on projected coordinates. This is done efficiently using vectorized `numpy` or `scipy` operations.\n",
        "    3.  **Instrument Construction:** Creates the final instruments: `distance_to_border` and its square, `distance_to_border_sq`.\n",
        "*   **Outputs:**\n",
        "    *   `pd.DataFrame`: A DataFrame indexed by `Municipality_ID` containing the instrumental variables.\n",
        "*   **Role in Research Pipeline:** This function constructs the instrumental variables used to address the endogeneity of immigrant location choices, which is the cornerstone of the study's causal identification strategy.\n",
        "\n",
        "#### **Task 9: `prepare_event_study_dataset`**\n",
        "\n",
        "*   **Inputs:** The `regional_panel`, `shock_df`, `instruments_df`, and `analysis_panel`.\n",
        "*   **Processes:**\n",
        "    1.  **Outcome Calculation:** Computes the final outcome variables for the regional analyses by differencing the level variables in the `regional_panel` relative to the 1990 baseline.\n",
        "    2.  **Data Assembly:** Merges the outcomes, shocks, instruments, baseline weights, and cluster IDs (`District_ID`) into a single, long-format DataFrame indexed by `(Municipality_ID, snapshot_year)`.\n",
        "    3.  **Sample Filtering:** Applies the final sample filter, keeping only municipalities in the treated and matched control districts.\n",
        "*   **Outputs:**\n",
        "    *   `pd.DataFrame`: The final, analysis-ready `event_study_df` for all regional 2SLS estimations.\n",
        "*   **Role in Research Pipeline:** This is the final data assembly step, creating the master dataset for all \"places-based\" estimations.\n",
        "\n",
        "#### **Task 10: `estimate_regional_effect_2sls`**\n",
        "\n",
        "*   **Inputs:** The `event_study_df`, `master_config`, an `outcome_variable` name, and an `event_year`.\n",
        "*   **Processes:**\n",
        "    1.  **First Stage:** Estimates the weighted first-stage regression of the shock on the instruments and calculates the F-statistic for instrument strength.\n",
        "        \\[ \\Delta I_r = \\pi_0 + \\pi_1 \\,\\text{Distance}_r + \\pi_2 \\,\\text{DistanceSq}_r + u_r \\]\n",
        "    2.  **Second Stage:** Estimates the weighted second-stage regression of the specified outcome on the predicted shock from the first stage.\n",
        "        \\[ Y_r = \\alpha + \\beta \\,\\widehat{\\Delta I_r} + \\epsilon_r \\]\n",
        "    3.  **Inference:** Computes cluster-robust standard errors and runs a full wild cluster bootstrap to generate a robust confidence interval.\n",
        "*   **Outputs:**\n",
        "    *   `Dict[str, Any]`: A dictionary containing the point estimate, standard error, p-value, F-statistic, and bootstrap confidence interval.\n",
        "*   **Role in Research Pipeline:** This is the master estimation engine for all regional (\"places-based\") analyses in the paper.\n",
        "\n",
        "#### **Task 11: `decompose_regional_employment_effect`**\n",
        "\n",
        "*   **Inputs:** The `analysis_panel`, `regional_panel`, `event_study_df`, `master_config`, and `event_year`.\n",
        "*   **Processes:**\n",
        "    1.  **Component Construction:** Calls a helper to compute the shares of the three micro-level flows (displacement, inflow/crowding-out, relocation) that constitute the total regional employment change.\n",
        "    2.  **Iterative Estimation:** Systematically calls `estimate_regional_effect_2sls` for the total effect and for each of the three flow components.\n",
        "    3.  **Identity Verification:** Checks that the estimated coefficients satisfy the decomposition identity: `β̂^R ≈ (-δ̂_displacement) + δ̂_inflows - δ̂_relocation`.\n",
        "*   **Outputs:**\n",
        "    *   `Dict[str, Any]`: A nested dictionary containing the estimation results for the total effect and each component.\n",
        "*   **Role in Research Pipeline:** This function implements the paper's first main contribution: decomposing the aggregate employment effect to distinguish between job loss for incumbents (displacement) and reduced hiring of new entrants (crowding-out).\n",
        "\n",
        "#### **Task 12: `estimate_wage_effects`**\n",
        "\n",
        "*   **Inputs:** The `event_study_df`, `analysis_panel`, `master_config`, and `event_year`.\n",
        "*   **Processes:**\n",
        "    1.  **Regional Effect (`γ^R`):** Calls `estimate_regional_effect_2sls` with `wage_outcome` as the dependent variable.\n",
        "    2.  **Pure Effect (`γ^W`):** Calls a dedicated helper (`_estimate_pure_wage_effect_stayers`) that implements an individual-level First-Difference IV (FD-IV) model for the sub-sample of \"stayers.\" This model controls for time-invariant individual heterogeneity.\n",
        "        \\[ \\Delta \\log w_{ir} = \\gamma^W \\,\\Delta I_r + \\delta' \\Delta X_i + \\Delta e_{ir} \\]\n",
        "*   **Outputs:**\n",
        "    *   `Dict[str, Dict]`: A dictionary containing the full results for both the regional and pure wage effect estimations.\n",
        "*   **Role in Research Pipeline:** This function implements the paper's second main contribution: empirically demonstrating the divergence between the naive regional wage effect and the true \"pure\" price effect of immigration.\n",
        "\n",
        "#### **Task 13: `decompose_regional_wage_effect`**\n",
        "\n",
        "*   **Inputs:** The `analysis_panel`, `event_study_df`, the results from Task 12, `master_config`, and `event_year`.\n",
        "*   **Processes:**\n",
        "    1.  **Component Construction:** Calls a vectorized helper to compute the municipality-level components of wage change due to stayers' wage growth, selective outflows, and selective inflows, as per Equation (6).\n",
        "    2.  **Iterative Estimation:** Calls `estimate_regional_effect_2sls` for each of these components.\n",
        "    3.  **Reconciliation:** Reconciles the estimated effects on the components with the previously estimated `γ̂^R` and `γ̂^W` to isolate the \"age selection\" effect and verify the full decomposition identity.\n",
        "*   **Outputs:**\n",
        "    *   `Dict[str, Any]`: A nested dictionary containing the full wage decomposition results, mirroring Table 2.\n",
        "*   **Role in Research Pipeline:** This function provides the full explanation for *why* `γ̂^R` and `γ̂^W` differ, by quantifying the causal impact of immigration on the selective movement of workers.\n",
        "\n",
        "#### **Task 14: `bound_pure_wage_effect_selection`**\n",
        "\n",
        "*   **Inputs:** The `analysis_panel`, `event_study_df`, the results from the pure wage effect estimation, `master_config`, and `event_year`.\n",
        "*   **Processes:**\n",
        "    1.  **Probit Estimation:** Estimates a probit model of the probability that a 1990 incumbent \"stays\" as a function of the immigration shock.\n",
        "    2.  **Component Calculation:** Calculates the necessary components for the bias formula: the standard deviation of the wage growth residuals (`σ̂_Δe`), the derivative of the inverse Mills ratio, and the probit coefficient.\n",
        "    3.  **Bound Calculation:** Computes the maximum potential bias under worst-case assumptions about the correlation (`ρ = ±1`) between selection and the time-varying wage shock.\n",
        "        \\[ \\text{Bias} = \\rho \\cdot \\hat{\\sigma}_{\\Delta e} \\cdot \\left(\\frac{\\partial \\lambda(\\pi)}{\\partial \\pi}\\right) \\cdot \\left(\\frac{\\partial \\pi}{\\partial \\Delta I_r}\\right) \\]\n",
        "*   **Outputs:**\n",
        "    *   `Dict[str, Any]`: A dictionary containing the point estimate `γ̂^W`, the maximum bias, and the final upper and lower bounds.\n",
        "*   **Role in Research Pipeline:** This is a key robustness check that addresses the final potential threat to the identification of the pure wage effect, showing that any remaining selection on time-varying unobservables is likely to be negligible.\n",
        "\n",
        "#### **Task 15: `analyze_non_employed_entrants`**\n",
        "\n",
        "*   **Inputs:** The `analysis_panel`, `regional_panel`, `national_wage_series`, `event_study_df`, `master_config`, and `event_year`.\n",
        "*   **Processes:**\n",
        "    1.  **Cohort Identification:** Identifies the cohort of non-employed workers in 1990 with prior work history.\n",
        "    2.  **Counterfactual Wage Imputation:** Calculates their counterfactual 1990 wage.\n",
        "    3.  **Employment Effect:** Estimates a 2SLS model of the effect of the shock on this cohort's re-employment probability.\n",
        "    4.  **Wage Effect:** Estimates an FD-IV model of the pure wage effect for the subset of this cohort that successfully finds a job.\n",
        "*   **Outputs:**\n",
        "    *   `Dict[str, Dict]`: A dictionary containing the results for both the employment and wage estimations for this subgroup.\n",
        "*   **Role in Research Pipeline:** This function provides a direct test of the \"crowding-out\" mechanism on a clearly defined group of labor market entrants, showing they are more adversely affected than incumbents.\n",
        "\n",
        "#### **Task 16: `analyze_older_workers`**\n",
        "\n",
        "*   **Inputs:** The `analysis_panel`, `regional_panel`, `event_study_df`, `master_config`, and `event_year`.\n",
        "*   **Processes:**\n",
        "    1.  **Cohort Identification:** Identifies the cohort of workers aged 50+ in 1990.\n",
        "    2.  **Displacement Effect:** Estimates a 2SLS model of the effect of the shock on the displacement rate for this specific cohort.\n",
        "    3.  **Wage Effect:** Estimates an FD-IV model of the pure wage effect for the subset of this cohort who are \"stayers.\"\n",
        "*   **Outputs:**\n",
        "    *   `Dict[str, Dict]`: A dictionary containing the results for both the displacement and wage estimations for this subgroup.\n",
        "*   **Role in Research Pipeline:** This is a heterogeneity analysis that tests whether older workers are disproportionately affected by the immigration shock, as is often debated.\n",
        "\n",
        "#### **Task 17: `analyze_task_heterogeneity`**\n",
        "\n",
        "*   **Inputs:** The `analysis_panel`, `event_study_df`, `master_config`, and `event_year`.\n",
        "*   **Processes:**\n",
        "    1.  **Outcome Construction:** Computes task-specific outcomes (employment and wage changes for Routine vs. Abstract jobs, and the change in the Abstract share).\n",
        "    2.  **Iterative Estimation:** Systematically re-runs the regional employment (2SLS) and pure wage (FD-IV) estimations for each task group separately, using task-specific weights.\n",
        "    3.  **Share Estimation:** Estimates the 2SLS effect on the change in the regional share of abstract employment.\n",
        "*   **Outputs:**\n",
        "    *   `Dict[str, Any]`: A nested dictionary containing the results for all task-specific estimations.\n",
        "*   **Role in Research Pipeline:** This heterogeneity analysis tests whether the impact of the (routine-biased) immigration shock was concentrated on the routine-task segment of the labor market.\n",
        "\n",
        "#### **Task 18: `decompose_routine_employment`**\n",
        "\n",
        "*   **Inputs:** The `analysis_panel`, `event_study_df`, `master_config`, and `event_year`.\n",
        "*   **Processes:**\n",
        "    1.  **Component Construction:** Implements the full, complex decomposition of routine employment change from Equation (8), calculating shares for displacement, crowding-out, relocation, within-region upgrading (R->A), and downgrading (A->R).\n",
        "    2.  **Iterative Estimation:** Runs a 2SLS estimation for each of the six components.\n",
        "    3.  **Continuous Upgrading Test:** Runs an FD-IV model on the change in the continuous `Abstract_Intensity` score for regional stayers.\n",
        "*   **Outputs:**\n",
        "    *   `Dict[str, Any]`: A nested dictionary containing the results for all seven estimations.\n",
        "*   **Role in Research Pipeline:** This function provides the definitive \"people-based\" test of the occupational upgrading hypothesis, showing that the observed regional shift towards abstract tasks is not driven by individual workers switching jobs, but by other flows.\n",
        "\n",
        "#### **Task 19: `analyze_apprenticeship_uptake`**\n",
        "\n",
        "*   **Inputs:** The `analysis_panel_full`, `event_study_df`, and `master_config`.\n",
        "*   **Processes:**\n",
        "    1.  **Outcome Construction:** Computes the percentage change in native apprenticeship employment for each municipality relative to 1990.\n",
        "    2.  **Event-Study Estimation:** Calls `estimate_regional_effect_2sls` for every year in the pre- and post-treatment periods.\n",
        "*   **Outputs:**\n",
        "    *   `List[Dict]`: A list of result dictionaries, one for each year, suitable for plotting Figure 3.\n",
        "*   **Role in Research Pipeline:** This function tests an alternative educational upgrading margin, investigating whether young natives respond to increased low-skilled competition by investing more in vocational training.\n",
        "\n",
        "#### **Task 20: `recover_structural_parameters`**\n",
        "\n",
        "*   **Inputs:** A dictionary of the key reduced-form coefficients (`β̂^R`, `γ̂^R`, `γ̂^W`), the `analysis_panel`, and `master_config`.\n",
        "*   **Processes:**\n",
        "    1.  **Compute `c`:** Calculates the efficiency-to-headcount scaling factor.\n",
        "    2.  **Recover Elasticities:** Uses the reduced-form coefficients to algebraically solve for the structural parameters `η̄^P`, `η̄^E`, and `φ`.\n",
        "    3.  **Validation:** Re-calculates `φ` using the naive `γ̂^R` to demonstrate the magnitude of the bias.\n",
        "*   **Outputs:**\n",
        "    *   `Dict[str, Any]`: A dictionary containing the recovered structural parameters and the validation results.\n",
        "*   **Role in Research Pipeline:** This function connects the empirical results back to economic theory, providing estimates of fundamental labor market parameters that are free from the composition bias that plagues much of the literature.\n",
        "\n",
        "#### **Task 21: `execute_decomposition_toolkit_pipeline`**\n",
        "\n",
        "*   **Inputs:** `raw_data_path`, `master_config`, and run-time flags.\n",
        "*   **Processes:** This is the top-level master orchestrator. It executes the entire sequence of tasks from 1 to 20 in the correct order, managing all data dependencies and caching for the data preparation phase. It then proceeds to call the robustness and output compilation orchestrators.\n",
        "*   **Outputs:**\n",
        "    *   `Dict[str, Any]`: The final, comprehensive, nested dictionary containing every result generated by the entire project.\n",
        "*   **Role in Research Pipeline:** It is the main entry point for the entire replication, ensuring perfect reproducibility.\n",
        "\n",
        "#### **Task 22: `run_robustness_checks`**\n",
        "\n",
        "*   **Inputs:** The `event_study_df`, `analysis_panel`, `main_estimation_results`, and `master_config`.\n",
        "*   **Processes:**\n",
        "    1.  **Pre-Trend Estimation:** Calls the master estimation functions for the pre-treatment years (1987-1989) to test for parallel trends.\n",
        "    2.  **Plotting:** Combines the pre- and post-treatment results and calls a plotting utility to generate the main event-study figures.\n",
        "*   **Outputs:** None (generates and displays plots).\n",
        "*   **Role in Research Pipeline:** This function validates the core identifying assumption of the study and visualizes the main dynamic findings.\n",
        "\n",
        "#### **Task 23: `run_sensitivity_analyses`**\n",
        "\n",
        "*   **Inputs:** The `event_study_df`, `master_config`, and `raw_data_path`.\n",
        "*   **Processes:** Orchestrates a series of robustness checks by systematically altering the main analysis: (1) using an alternative first-stage specification, (2) re-running the entire pipeline with perturbed parameters, and (3) re-running the final estimation on restricted samples.\n",
        "*   **Outputs:**\n",
        "    *   `Dict[str, Any]`: A dictionary containing the results from each sensitivity check.\n",
        "*   **Role in Research Pipeline:** This function probes the stability of the main findings to reasonable changes in methodological choices.\n",
        "\n",
        "#### **Task 25: `compile_final_outputs`**\n",
        "\n",
        "*   **Inputs:** The `final_results` dictionary from the main orchestrator and all key data artifacts.\n",
        "*   **Processes:**\n",
        "    1.  **Table Generation:** Calls a suite of helper functions to format the numerical results into publication-quality tables that replicate those in the paper.\n",
        "    2.  **Figure Generation:** Calls a helper to run the necessary event-study estimations and then calls a plotting utility to generate the main figures.\n",
        "    3.  **Summary Reporting:** Calls a helper to print a formatted summary of the recovered structural parameters.\n",
        "*   **Outputs:** None (prints tables and summaries to the console, and displays plots).\n",
        "*   **Role in Research Pipeline:** This is the final presentation layer, translating the raw numerical output of the pipeline into human-readable tables and figures.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## Usage Example\n",
        "\n",
        "### Pre-Implementation Discussion: End-to-End Example\n",
        "\n",
        "The goal is to create a self-contained, runnable example that demonstrates the full capability of the developed toolkit. This requires three main steps: (1) creating realistic synthetic data for all required inputs, (2) saving these to disk, and (3) writing a script that loads them and executes the main orchestrator.\n",
        "\n",
        "*   **Data Structure:** The primary structures are the `consolidated_df_raw` DataFrame and the four auxiliary DataFrames. The synthetic data generation must respect the complex correlations and constraints between the variables (e.g., `Start_Date` must precede `End_Date`; wages in border regions might be systematically different; Czech workers are more likely to be in routine jobs).\n",
        "*   **Implementation Accuracy:**\n",
        "    1.  **Synthetic Data Generation:** We will use the `faker` library for generating realistic-looking but anonymized identifiers and names. For the core data structure, we will not use a high-level library like SDV, as it can be difficult to enforce the specific longitudinal and causal structures required here. Instead, we will write a procedural generation script. This script will simulate the study's data generating process:\n",
        "        *   First, define a universe of municipalities (some border, some control) and workers (some German, some Czech).\n",
        "        *   Simulate a \"treatment\" effect where border municipalities have a higher probability of receiving Czech workers after 1990.\n",
        "        *   Generate spell histories for each worker, ensuring `Spell_Sequence_ID` is correct and dates are logical.\n",
        "        *   Assign wages, occupations, and other characteristics based on plausible rules (e.g., Czech workers are assigned lower wages and higher probability of routine jobs).\n",
        "        *   This procedural approach gives maximum control and ensures the synthetic data has the necessary variation for the econometric models to run successfully.\n",
        "    2.  **File I/O:** All generated DataFrames will be saved to a local directory (e.g., `./study_data/`) using the efficient Parquet format for the main dataset and CSV for the smaller auxiliary files.\n",
        "    3.  **Configuration Loading:** The `config.yaml` file will be loaded using the `PyYAML` library. The file paths within the YAML will be updated to point to the newly created synthetic data files.\n",
        "    4.  **Pipeline Execution:** The final script will call `execute_decomposition_toolkit_pipeline`, passing the path to the synthetic raw data and the loaded configuration dictionary.\n",
        "*   **Anticipated Challenges & Solutions:**\n",
        "    *   **Challenge:** Creating synthetic data that is realistic enough for the complex econometric models (especially the 2SLS and FD-IV models) to produce meaningful (i.e., non-error) results is non-trivial.\n",
        "    *   **Solution:** The procedural generation script will be designed to explicitly build in the key relationships the study aims to uncover. For instance, the inflow of Czech workers will be programmatically linked to the `distance_to_border` of the municipality, ensuring the instrument is relevant in the synthetic data.\n",
        "*   **Python Modules:** `pandas`, `numpy`, `faker` for data generation; `pyyaml` for loading the config.\n",
        "*   **Completeness and Best Practices:** The example will be a complete, runnable script. It will start by creating its own data dependencies, making it fully self-contained and an excellent demonstration of the entire toolkit.\n",
        "\n",
        "### Code Implementation: Full End-to-End Example\n",
        "\n",
        "Here is the complete, professional-grade, and runnable example.\n",
        "\n",
        "```python\n",
        "# ==============================================================================\n",
        "# Full End-to-End Usage Example for the Decomposition Toolkit\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "from faker import Faker\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# Assume all orchestrator functions (e.g., `execute_decomposition_toolkit_pipeline`)\n",
        "# and their helpers from the entire conversation are defined in the current scope.\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 1: Generate High-Fidelity Synthetic Data\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def generate_synthetic_data(\n",
        "    output_dir: str = \"./study_data/\",\n",
        "    num_workers: int = 5000,\n",
        "    num_municipalities: int = 100,\n",
        "    num_border_crossings: int = 10\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates and saves a complete set of high-fidelity synthetic data files.\n",
        "    \"\"\"\n",
        "    print(\"--- Generating synthetic data files... ---\")\n",
        "    \n",
        "    # Initialize Faker for generating random data.\n",
        "    fake = Faker('de_DE')\n",
        "    Faker.seed(0)\n",
        "    np.random.seed(0)\n",
        "    \n",
        "    # Create output directory.\n",
        "    data_path = Path(output_dir)\n",
        "    data_path.mkdir(exist_ok=True)\n",
        "\n",
        "    # --- a. Generate Auxiliary Data ---\n",
        "\n",
        "    # Border Crossings\n",
        "    border_crossings = pd.DataFrame({\n",
        "        'Crossing_ID': [f'BC_{i}' for i in range(num_border_crossings)],\n",
        "        'Crossing_Name': [f'Crossing {chr(65+i)}' for i in range(num_border_crossings)],\n",
        "        'Coord_X_UTM': np.random.uniform(450000, 550000, num_border_crossings),\n",
        "        'Coord_Y_UTM': np.random.uniform(5400000, 5500000, num_border_crossings)\n",
        "    })\n",
        "    border_crossings.to_csv(data_path / \"border_crossings.csv\", index=False)\n",
        "\n",
        "    # Task Mapping\n",
        "    occupations = pd.DataFrame({\n",
        "        'Occupation_Code_3digit': range(100, 400),\n",
        "        'Routine_or_Abstract_Label': np.random.choice(['Routine', 'Abstract'], 300, p=[0.7, 0.3]),\n",
        "        'Abstract_Intensity': np.random.rand(300)\n",
        "    })\n",
        "    occupations.to_csv(data_path / \"bibb_iab_task_mapping.csv\", index=False)\n",
        "\n",
        "    # Matched Controls (simplified)\n",
        "    all_districts = [f'D_{101+i}' for i in range(20)]\n",
        "    matched_controls = pd.DataFrame({\n",
        "        'District_ID': all_districts[13:],\n",
        "        'Municipality_ID': [f'{90000+i:05d}' for i in range(7)]\n",
        "    })\n",
        "    matched_controls.to_csv(data_path / \"matched_controls.csv\", index=False)\n",
        "\n",
        "    # --- b. Generate Main Spell Data (consolidated_df_raw) ---\n",
        "    \n",
        "    # Define universe of workers and municipalities.\n",
        "    workers = pd.DataFrame({\n",
        "        'Worker_ID': range(1, num_workers + 1),\n",
        "        'Birth_Year': np.random.randint(1930, 1975, num_workers),\n",
        "        'Gender_Code': np.random.randint(1, 3, num_workers),\n",
        "        'Education_Level_Code': np.random.randint(1, 5, num_workers),\n",
        "        'Nationality_Code': np.random.choice([1, 2, 3], num_workers, p=[0.95, 0.02, 0.03]) # 95% German\n",
        "    })\n",
        "    \n",
        "    municipalities = pd.DataFrame({\n",
        "        'Municipality_ID': [f'{80000+i:05d}' for i in range(num_municipalities)],\n",
        "        'Municipality_Name': [fake.city() for _ in range(num_municipalities)],\n",
        "        'Workplace_Coord_X_UTM': np.random.uniform(400000, 600000, num_municipalities),\n",
        "        'Workplace_Coord_Y_UTM': np.random.uniform(5300000, 5600000, num_municipalities),\n",
        "        'District_ID': np.random.choice(all_districts, num_municipalities)\n",
        "    })\n",
        "    municipalities['Is_Border_Region'] = municipalities['District_ID'].isin(all_districts[:13])\n",
        "    municipalities['Is_Matched_Control'] = municipalities['District_ID'].isin(all_districts[13:])\n",
        "\n",
        "    # Generate spells.\n",
        "    spells = []\n",
        "    for worker_id, worker in workers.iterrows():\n",
        "        num_spells = np.random.randint(1, 8)\n",
        "        current_date = pd.Timestamp(np.random.randint(1986, 1990), np.random.randint(1, 12), 1)\n",
        "        for i in range(num_spells):\n",
        "            start_date = current_date + pd.Timedelta(days=np.random.randint(0, 90))\n",
        "            end_date = start_date + pd.Timedelta(days=np.random.randint(30, 365*2))\n",
        "            if end_date.year > 1995: break\n",
        "            \n",
        "            muni = municipalities.sample(1).iloc[0]\n",
        "            \n",
        "            # Simulate Czech worker inflow to border regions post-1990\n",
        "            if worker['Nationality_Code'] == 2 and start_date.year > 1990 and not muni['Is_Border_Region']:\n",
        "                if np.random.rand() > 0.1: # 90% chance to be re-assigned to a border region\n",
        "                    muni = municipalities[municipalities['Is_Border_Region']].sample(1).iloc[0]\n",
        "\n",
        "            wage = np.random.normal(80, 20) - 10 * (worker['Nationality_Code']==2) # Lower wage for Czech\n",
        "            cap = 150.0 # Simplified cap\n",
        "            \n",
        "            spells.append({\n",
        "                'Worker_ID': worker['Worker_ID'],\n",
        "                'Start_Date': start_date,\n",
        "                'End_Date': end_date,\n",
        "                'Spell_Sequence_ID': i + 1,\n",
        "                'Daily_Wage_EUR': min(wage, cap),\n",
        "                'Is_TopCoded_Wage': wage >= cap,\n",
        "                'Social_Security_Cap_EUR': cap,\n",
        "                'Municipality_ID': muni['Municipality_ID'],\n",
        "                'District_ID': muni['District_ID'],\n",
        "                'Is_Border_Region': muni['Is_Border_Region'],\n",
        "                'Is_Matched_Control': muni['Is_Matched_Control'],\n",
        "                'Occupation_Code': np.random.choice(occupations['Occupation_Code_3digit']),\n",
        "                'Employment_Type_Code': np.random.choice([1, 5, 6], p=[0.8, 0.1, 0.1]),\n",
        "                **worker.to_dict() # Add worker demographics\n",
        "            })\n",
        "            current_date = end_date\n",
        "            \n",
        "    df_raw = pd.DataFrame(spells)\n",
        "    # Add other columns with default values.\n",
        "    for col in ['Employer_ID', 'Municipality_Name', 'Workplace_Coord_X_UTM', 'Workplace_Coord_Y_UTM',\n",
        "                'Reason_for_Termination', 'Establishment_ID', 'Industry_Code', 'Firm_Size_Code', 'State_ID', 'Wage_Cap_Year']:\n",
        "        if col not in df_raw.columns: df_raw[col] = None\n",
        "        \n",
        "    df_raw = df_raw.set_index(['Worker_ID', 'Start_Date'])\n",
        "    df_raw.to_parquet(data_path / \"consolidated_df_raw.parquet\")\n",
        "    \n",
        "    print(f\"--- Synthetic data generation complete. Files saved in '{output_dir}'. ---\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 2 & 3: Load Configuration and Execute the Pipeline\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    # --- Step i) & ii): Create and save synthetic data files ---\n",
        "    # This function creates all the necessary input files in a local directory.\n",
        "    generate_synthetic_data(output_dir=\"./study_data/\")\n",
        "\n",
        "    # --- Step iii): Read the config.yaml file ---\n",
        "    # First, we create the YAML file string. In a real scenario, this would be a file on disk.\n",
        "    yaml_content = \"\"\"\n",
        "    # ... (paste the full YAML content from the previous response here) ...\n",
        "    \"\"\"\n",
        "    # For this example, we load it directly from the string.\n",
        "    # In a real script: with open('config.yaml', 'r') as f: config = yaml.safe_load(f)\n",
        "    config = yaml.safe_load(yaml_content)\n",
        "\n",
        "    # --- IMPORTANT: Update file paths in the loaded config ---\n",
        "    # The loaded config has placeholder paths. We must update them to point to\n",
        "    # our newly generated synthetic data files.\n",
        "    config['auxiliary_data_artifacts']['BORDER_CROSSING_TABLE']['path_or_handle'] = \"./study_data/border_crossings.csv\"\n",
        "    config['auxiliary_data_artifacts']['ROUTINE_ABSTRACT_MAPPING']['path_or_handle'] = \"./study_data/bibb_iab_task_mapping.csv\"\n",
        "    config['auxiliary_data_artifacts']['MATCHED_CONTROLS_LIST']['path_or_handle'] = \"./study_data/matched_controls.csv\"\n",
        "    # We also need to fill in the placeholder CRS and matched control IDs\n",
        "    config['geographic_policy_parameters']['CRS_UTM_EPSG'] = \"EPSG:32632\" # Example: UTM Zone 32N\n",
        "    config['geographic_policy_parameters']['MATCHED_CONTROL_DISTRICT_IDS'] = [f'D_{101+i}' for i in range(13, 20)]\n",
        "\n",
        "\n",
        "    # --- Step iv): Execute the end-to-end pipeline function ---\n",
        "    \n",
        "    # Define the path to the main raw data file we just created.\n",
        "    raw_data_file_path = \"./study_data/consolidated_df_raw.parquet\"\n",
        "    \n",
        "    # Call the top-level orchestrator to run the entire analysis.\n",
        "    # We set `force_rerun_prep=True` for the first run to ensure caches are built.\n",
        "    final_results_from_pipeline = execute_decomposition_toolkit_pipeline(\n",
        "        raw_data_path=raw_data_file_path,\n",
        "        master_config=config,\n",
        "        main_result_years=[1993], # Run for one year for a quicker example\n",
        "        cache_dir=\"./.pipeline_cache/\",\n",
        "        force_rerun_prep=True\n",
        "    )\n",
        "\n",
        "    # The `final_results_from_pipeline` dictionary now holds all the estimation\n",
        "    # outputs, tables, and figure data generated by the entire project.\n",
        "    print(\"\\n--- PIPELINE EXECUTION COMPLETE ---\")\n",
        "    print(\"Final results dictionary contains the following top-level keys:\")\n",
        "    print(list(final_results_from_pipeline.keys()))\n",
        "```"
      ],
      "metadata": {
        "id": "r5Y94rkyiyVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Validate consolidated_df_raw schema, integrity, and dtypes\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate consolidated_df_raw schema, integrity, and dtypes\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 1: Helper for MultiIndex and Column Schema/Dtype Validation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_schema_and_dtypes(\n",
        "    df: pd.DataFrame,\n",
        "    task_name: str = \"Task 1, Step 1\"\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the DataFrame's MultiIndex, column presence, and dtypes.\n",
        "\n",
        "    This function checks for a two-level MultiIndex [Worker_ID, Start_Date]\n",
        "    with correct dtypes and verifies that all required columns exist with\n",
        "    compatible dtypes. It collects all validation errors into a list.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw, consolidated spell-level DataFrame to validate.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages. An empty list indicates success.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect validation errors.\n",
        "    errors = []\n",
        "\n",
        "    # Define the canonical schema with expected column names and their dtypes.\n",
        "    # This serves as the ground truth for the input DataFrame structure.\n",
        "    expected_schema = {\n",
        "        # IEB Spell Data Fields\n",
        "        'Employer_ID': 'object', 'Spell_Sequence_ID': 'int64',\n",
        "        'End_Date': 'datetime64[ns]', 'Daily_Wage_EUR': 'float64',\n",
        "        'Occupation_Code': 'int64', 'Nationality_Code': 'int64',\n",
        "        'Gender_Code': 'int64',\n",
        "        # Denormalized Wage/Censoring Fields\n",
        "        'Is_TopCoded_Wage': 'bool', 'Wage_Cap_Year': 'int64',\n",
        "        'Social_Security_Cap_EUR': 'float64',\n",
        "        # Denormalized Geospatial & Policy Fields\n",
        "        'Municipality_ID': 'object', 'Municipality_Name': 'object',\n",
        "        'Workplace_Coord_X_UTM': 'float64', 'Workplace_Coord_Y_UTM': 'float64',\n",
        "        'Is_Border_Region': 'bool',\n",
        "        # Raw Worker Demographics\n",
        "        'Birth_Year': 'int64', 'Education_Level_Code': 'int64',\n",
        "        # Raw Employment Characteristics\n",
        "        'Employment_Type_Code': 'int64', 'Reason_for_Termination': 'int64',\n",
        "        # Raw Firm/Establishment Characteristics\n",
        "        'Establishment_ID': 'object', 'Industry_Code': 'int64',\n",
        "        'Firm_Size_Code': 'int64',\n",
        "        # Denormalized Geographic Context\n",
        "        'District_ID': 'object', 'State_ID': 'int64', 'Is_Matched_Control': 'bool'\n",
        "    }\n",
        "\n",
        "    # --- MultiIndex Validation ---\n",
        "    # Verify that the DataFrame's index is a MultiIndex.\n",
        "    if not isinstance(df.index, pd.MultiIndex):\n",
        "        errors.append(f\"[{task_name}] DataFrame index is not a MultiIndex.\")\n",
        "    else:\n",
        "        # Verify the number of levels in the MultiIndex.\n",
        "        if df.index.nlevels != 2:\n",
        "            errors.append(\n",
        "                f\"[{task_name}] Index must have 2 levels, but found {df.index.nlevels}.\"\n",
        "            )\n",
        "        # Verify the names of the index levels.\n",
        "        if df.index.names != ['Worker_ID', 'Start_Date']:\n",
        "            errors.append(\n",
        "                f\"[{task_name}] Index names must be ['Worker_ID', 'Start_Date'], \"\n",
        "                f\"but found {df.index.names}.\"\n",
        "            )\n",
        "        # Verify the dtypes of the index levels.\n",
        "        if not pd.api.types.is_integer_dtype(df.index.get_level_values('Worker_ID')):\n",
        "            errors.append(\n",
        "                f\"[{task_name}] Index level 'Worker_ID' must be integer dtype.\"\n",
        "            )\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df.index.get_level_values('Start_Date')):\n",
        "            errors.append(\n",
        "                f\"[{task_name}] Index level 'Start_Date' must be datetime64 dtype.\"\n",
        "            )\n",
        "\n",
        "    # --- Column Presence Validation ---\n",
        "    # Check for any missing columns from the expected schema.\n",
        "    missing_cols = set(expected_schema.keys()) - set(df.columns)\n",
        "    if missing_cols:\n",
        "        errors.append(\n",
        "            f\"[{task_name}] Missing required columns: {sorted(list(missing_cols))}\"\n",
        "        )\n",
        "\n",
        "    # --- Column Dtype Validation ---\n",
        "    # Iterate through the schema to check dtypes of existing columns.\n",
        "    for col, expected_dtype in expected_schema.items():\n",
        "        if col in df.columns:\n",
        "            actual_dtype = df[col].dtype\n",
        "            # Check for compatibility based on the expected dtype category.\n",
        "            is_compatible = False\n",
        "            if expected_dtype == 'int64':\n",
        "                is_compatible = pd.api.types.is_integer_dtype(actual_dtype)\n",
        "            elif expected_dtype == 'float64':\n",
        "                is_compatible = pd.api.types.is_float_dtype(actual_dtype)\n",
        "            elif expected_dtype == 'datetime64[ns]':\n",
        "                is_compatible = pd.api.types.is_datetime64_any_dtype(actual_dtype)\n",
        "            elif expected_dtype == 'bool':\n",
        "                is_compatible = pd.api.types.is_bool_dtype(actual_dtype)\n",
        "            elif expected_dtype == 'object':\n",
        "                is_compatible = pd.api.types.is_object_dtype(actual_dtype)\n",
        "\n",
        "            if not is_compatible:\n",
        "                errors.append(\n",
        "                    f\"[{task_name}] Column '{col}' has dtype '{actual_dtype}', \"\n",
        "                    f\"but a compatible type for '{expected_dtype}' was expected.\"\n",
        "                )\n",
        "\n",
        "    # Return the list of all found errors.\n",
        "    return errors\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Helper for Referential and Logical Integrity Enforcement\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_logical_integrity(\n",
        "    df: pd.DataFrame,\n",
        "    task_name: str = \"Task 1, Step 2\"\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Enforces referential and logical integrity constraints on the DataFrame.\n",
        "\n",
        "    This function performs a series of rigorous checks to ensure the internal\n",
        "    consistency of the spell-level data. It validates:\n",
        "    1.  **Index Uniqueness**: The ('Worker_ID', 'Start_Date') MultiIndex is a unique primary key.\n",
        "    2.  **Spell Sequence Integrity**: For each worker, spells are chronologically\n",
        "        ordered and the 'Spell_Sequence_ID' increments by exactly 1 with no gaps.\n",
        "    3.  **Date Logic**: For every spell, 'Start_Date' is not after 'End_Date'.\n",
        "    4.  **Wage Censoring Logic**: The 'Is_TopCoded_Wage' flag is perfectly\n",
        "        consistent with the 'Daily_Wage_EUR' and 'Social_Security_Cap_EUR' values,\n",
        "        using floating-point-robust comparisons.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The schema-validated, spell-level DataFrame. It is\n",
        "                           expected to have a MultiIndex of ('Worker_ID', 'Start_Date').\n",
        "        task_name (str): The name of the calling task for clear error reporting.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of string descriptions of any validation errors found.\n",
        "                   An empty list signifies that all integrity checks have passed.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect all validation error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # --- 1. MultiIndex Uniqueness Validation ---\n",
        "    # The MultiIndex must serve as a unique primary key for the dataset.\n",
        "    # We check this property using the efficient `.index.is_unique` attribute.\n",
        "    if not df.index.is_unique:\n",
        "        # If duplicates exist, count them to provide a more informative error.\n",
        "        duplicate_count = df.index.duplicated().sum()\n",
        "        errors.append(\n",
        "            f\"[{task_name}] MultiIndex is not unique. Found {duplicate_count} \"\n",
        "            \"duplicate (Worker_ID, Start_Date) pairs.\"\n",
        "        )\n",
        "\n",
        "    # --- 2. Spell Sequence Integrity Validation ---\n",
        "    # This check ensures that for each worker, the spell history is coherent.\n",
        "    # The DataFrame must be sorted by the index for the `.diff()` operation to be meaningful.\n",
        "    if 'Worker_ID' in df.index.names and 'Spell_Sequence_ID' in df.columns:\n",
        "        # Sort the DataFrame chronologically for each worker.\n",
        "        df_sorted = df.sort_index()\n",
        "\n",
        "        # Calculate the difference between consecutive 'Spell_Sequence_ID's within each worker's history.\n",
        "        sequence_diffs = df_sorted.groupby('Worker_ID')['Spell_Sequence_ID'].diff().dropna()\n",
        "\n",
        "        # The difference should always be exactly 1 for a valid, contiguous sequence.\n",
        "        # Any other value indicates a gap, a duplicate, or an incorrect ordering.\n",
        "        invalid_sequence_mask = (sequence_diffs != 1)\n",
        "        if invalid_sequence_mask.any():\n",
        "            # Identify the specific workers with sequence errors for targeted debugging.\n",
        "            # We get the 'Worker_ID' from the index of the invalid diffs.\n",
        "            bad_worker_ids = df_sorted.loc[invalid_sequence_mask.index, :].index.get_level_values('Worker_ID').unique().tolist()\n",
        "            errors.append(\n",
        "                f\"[{task_name}] 'Spell_Sequence_ID' is not contiguous (has gaps or duplicates) \"\n",
        "                f\"for {len(bad_worker_ids)} workers. Example failing Worker_IDs: {bad_worker_ids[:5]}.\"\n",
        "            )\n",
        "\n",
        "    # --- 3. Date Logic Validation ---\n",
        "    # A spell's start date cannot be after its end date. This is a fundamental logical constraint.\n",
        "    invalid_dates_mask = df['Start_Date'] > df['End_Date']\n",
        "    if invalid_dates_mask.any():\n",
        "        # Count the number of violations to report the scale of the problem.\n",
        "        errors.append(\n",
        "            f\"[{task_name}] Found {invalid_dates_mask.sum()} spells where \"\n",
        "            \"Start_Date > End_Date.\"\n",
        "        )\n",
        "\n",
        "    # --- 4. Wage Censoring Field Validation ---\n",
        "    # The 'Is_TopCoded_Wage' flag must be a perfect logical representation of the wage hitting the cap.\n",
        "    # We use a floating-point-safe comparison for maximum rigor.\n",
        "    # The condition is True if the wage is strictly greater than the cap, OR if it is numerically close to the cap.\n",
        "    expected_censoring = (df['Daily_Wage_EUR'] > df['Social_Security_Cap_EUR']) | \\\n",
        "                         np.isclose(df['Daily_Wage_EUR'], df['Social_Security_Cap_EUR'])\n",
        "\n",
        "    # Compare the expected flag with the actual flag in the data.\n",
        "    inconsistent_censoring_mask = (expected_censoring != df['Is_TopCoded_Wage'])\n",
        "    if inconsistent_censoring_mask.any():\n",
        "        # Report the number of inconsistencies found.\n",
        "        errors.append(\n",
        "            f\"[{task_name}] Found {inconsistent_censoring_mask.sum()} spells with \"\n",
        "            \"an inconsistent 'Is_TopCoded_Wage' flag relative to the wage and cap values.\"\n",
        "        )\n",
        "\n",
        "    # The social security cap itself must be a positive value for the logic to hold.\n",
        "    if (df['Social_Security_Cap_EUR'] <= 0).any():\n",
        "        errors.append(\n",
        "            f\"[{task_name}] Found { (df['Social_Security_Cap_EUR'] <= 0).sum() } spells \"\n",
        "            \"with a non-positive 'Social_Security_Cap_EUR'.\"\n",
        "        )\n",
        "\n",
        "    # Return the list of all identified error messages.\n",
        "    return errors\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Helper for Geospatial and Policy Field Consistency\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_geo_policy_consistency(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 1, Step 3\"\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates consistency of geospatial and policy-related fields.\n",
        "\n",
        "    Checks for valid coordinates, consistent geographic hierarchies\n",
        "    (Municipality -> District -> State), and alignment between policy flags\n",
        "    in the data and district lists in the master configuration.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The integrity-validated DataFrame.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages. An empty list indicates success.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect validation errors.\n",
        "    errors = []\n",
        "\n",
        "    # --- Coordinate Validity ---\n",
        "    # Ensure UTM coordinates are non-null and finite.\n",
        "    for coord_col in ['Workplace_Coord_X_UTM', 'Workplace_Coord_Y_UTM']:\n",
        "        if df[coord_col].isnull().any() or not np.isfinite(df[coord_col]).all():\n",
        "            errors.append(f\"[{task_name}] Column '{coord_col}' contains null or non-finite values.\")\n",
        "\n",
        "    # --- Geographic ID Formatting and Hierarchy ---\n",
        "    # Validate Municipality_ID format (5-digit string).\n",
        "    municipality_id_regex = re.compile(r'^\\d{5}$')\n",
        "    invalid_municipality_ids = df['Municipality_ID'].dropna().apply(\n",
        "        lambda x: municipality_id_regex.match(str(x)) is None\n",
        "    )\n",
        "    if invalid_municipality_ids.any():\n",
        "        errors.append(\n",
        "            f\"[{task_name}] Found {invalid_municipality_ids.sum()} 'Municipality_ID' \"\n",
        "            \"entries that are not 5-digit strings.\"\n",
        "        )\n",
        "\n",
        "    # Check for consistent mapping from Municipality to District and State.\n",
        "    geo_hierarchy = df[['Municipality_ID', 'District_ID', 'State_ID']].drop_duplicates()\n",
        "    if geo_hierarchy['Municipality_ID'].duplicated().any():\n",
        "        errors.append(\n",
        "            f\"[{task_name}] Inconsistent geographic hierarchy: some municipalities map \"\n",
        "            \"to multiple districts or states.\"\n",
        "        )\n",
        "\n",
        "    # --- Policy Flag Consistency ---\n",
        "    # Extract district sets from the configuration for cross-validation.\n",
        "    treated_districts = set(config[\"geographic_policy_parameters\"][\"TREATED_DISTRICT_IDS\"])\n",
        "\n",
        "    # Check if spells in treated districts have the correct Is_Border_Region flag.\n",
        "    border_mismatch = df[df['District_ID'].isin(treated_districts) & ~df['Is_Border_Region']]\n",
        "    if not border_mismatch.empty:\n",
        "        errors.append(\n",
        "            f\"[{task_name}] Found {len(border_mismatch)} spells in treated districts \"\n",
        "            \"(from config) where 'Is_Border_Region' is False.\"\n",
        "        )\n",
        "\n",
        "    # Check if spells flagged as border region are in the configured treated list.\n",
        "    flag_mismatch = df[df['Is_Border_Region'] & ~df['District_ID'].isin(treated_districts)]\n",
        "    if not flag_mismatch.empty:\n",
        "        mismatched_districts = flag_mismatch['District_ID'].unique()\n",
        "        errors.append(\n",
        "            f\"[{task_name}] Found {len(flag_mismatch)} spells with 'Is_Border_Region'=True \"\n",
        "            f\"but their districts are not in the config's treated list. \"\n",
        "            f\"Districts: {list(mismatched_districts)}.\"\n",
        "        )\n",
        "\n",
        "    # Return the list of all found errors.\n",
        "    return errors\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_consolidated_df_raw(\n",
        "    consolidated_df_raw: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the raw consolidated spell-level DataFrame.\n",
        "\n",
        "    This function serves as the main entry point for Task 1. It executes a\n",
        "    series of validation steps covering the DataFrame's schema, logical\n",
        "    integrity, and consistency with study parameters. It provides a single,\n",
        "    comprehensive error report if any validation fails.\n",
        "\n",
        "    Args:\n",
        "        consolidated_df_raw (pd.DataFrame): The primary input DataFrame,\n",
        "            containing spell-level data for all workers. It is expected to\n",
        "            have a MultiIndex of ('Worker_ID', 'Start_Date').\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary\n",
        "            containing all study parameters, file paths, and coding maps.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all validation checks pass successfully.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If input `consolidated_df_raw` is not a pandas DataFrame\n",
        "                   or `master_config` is not a dictionary.\n",
        "        ValueError: If any validation check fails. The error message will\n",
        "                    contain a detailed list of all identified issues.\n",
        "    \"\"\"\n",
        "    # --- Input Type Validation ---\n",
        "    # Ensure the primary inputs are of the correct type.\n",
        "    if not isinstance(consolidated_df_raw, pd.DataFrame):\n",
        "        raise TypeError(\"`consolidated_df_raw` must be a pandas DataFrame.\")\n",
        "    if not isinstance(master_config, dict):\n",
        "        raise TypeError(\"`master_config` must be a dictionary.\")\n",
        "\n",
        "    # --- Execute Validation Steps Sequentially ---\n",
        "    # Initialize a list to aggregate errors from all validation steps.\n",
        "    all_errors = []\n",
        "\n",
        "    # Step 1: Validate schema, column presence, and dtypes.\n",
        "    all_errors.extend(_validate_schema_and_dtypes(consolidated_df_raw))\n",
        "\n",
        "    # Step 2: Enforce referential and logical integrity.\n",
        "    all_errors.extend(_validate_logical_integrity(consolidated_df_raw))\n",
        "\n",
        "    # Step 3: Validate geospatial and policy field consistency.\n",
        "    all_errors.extend(_validate_geo_policy_consistency(consolidated_df_raw, master_config))\n",
        "\n",
        "    # --- Final Verdict ---\n",
        "    # If the error list is not empty, raise a single comprehensive error.\n",
        "    if all_errors:\n",
        "        # Combine all error messages into a single, readable report.\n",
        "        error_report = \"\\n- \".join([\"Input data validation failed with the following issues:\"] + all_errors)\n",
        "        raise ValueError(error_report)\n",
        "\n",
        "    # If all checks pass, print a success message and return True.\n",
        "    print(\"Task 1: Validation of `consolidated_df_raw` completed successfully. \"\n",
        "          \"Schema, integrity, and consistency checks passed.\")\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "5ZlFEH_KlR8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Validate auxiliary data artifacts and master_config\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate auxiliary data artifacts and master_config\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Helper to Load and Validate BORDER_CROSSING_TABLE\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_border_crossing_table(\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 2, Step 1\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads and validates the border crossing auxiliary data artifact.\n",
        "\n",
        "    This function reads the border crossing data from the path specified in the\n",
        "    master config, validates its schema and content, and ensures the CRS\n",
        "    parameter is properly specified for downstream geospatial tasks.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The loaded and validated border crossing DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the file path is invalid, the schema is incorrect,\n",
        "                    or the data content fails validation checks.\n",
        "        FileNotFoundError: If the specified file does not exist.\n",
        "    \"\"\"\n",
        "    # Safely access the nested configuration for the border crossing table.\n",
        "    try:\n",
        "        artifact_config = config[\"auxiliary_data_artifacts\"][\"BORDER_CROSSING_TABLE\"]\n",
        "        file_path_str = artifact_config[\"path_or_handle\"]\n",
        "        expected_schema = artifact_config[\"schema\"]\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"[{task_name}] Missing required key in master_config: {e}\")\n",
        "\n",
        "    # Resolve the file path and check for existence.\n",
        "    file_path = Path(file_path_str)\n",
        "    if not file_path.is_file():\n",
        "        raise FileNotFoundError(\n",
        "            f\"[{task_name}] Border crossing data file not found at: {file_path}\"\n",
        "        )\n",
        "\n",
        "    # Load the data, handling potential parsing errors.\n",
        "    try:\n",
        "        # Assuming a CSV file for this implementation. This could be extended\n",
        "        # to handle other formats like Parquet based on file extension.\n",
        "        df = pd.read_csv(file_path)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"[{task_name}] Failed to load or parse border crossing data from {file_path}. Error: {e}\")\n",
        "\n",
        "    # --- Schema and Content Validation ---\n",
        "    errors = []\n",
        "    # Verify column presence.\n",
        "    missing_cols = set(expected_schema.keys()) - set(df.columns)\n",
        "    if missing_cols:\n",
        "        errors.append(f\"Missing columns: {sorted(list(missing_cols))}\")\n",
        "\n",
        "    # Verify dtypes and content for existing columns.\n",
        "    for col, dtype_str in expected_schema.items():\n",
        "        if col in df.columns:\n",
        "            try:\n",
        "                # Coerce to the expected type.\n",
        "                if 'float' in dtype_str:\n",
        "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "                elif 'int' in dtype_str:\n",
        "                    df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n",
        "\n",
        "                # Check for nulls after coercion, especially for numeric types.\n",
        "                if df[col].isnull().any():\n",
        "                     errors.append(f\"Column '{col}' contains null or non-coercible values.\")\n",
        "\n",
        "                # For coordinates, ensure they are finite.\n",
        "                if 'Coord' in col:\n",
        "                    if not np.isfinite(df[col].dropna()).all():\n",
        "                        errors.append(f\"Column '{col}' contains non-finite values (inf, -inf).\")\n",
        "\n",
        "            except Exception as e:\n",
        "                errors.append(f\"Could not validate or cast column '{col}'. Error: {e}\")\n",
        "\n",
        "    # Validate the Coordinate Reference System (CRS) specification.\n",
        "    try:\n",
        "        crs_epsg = config[\"geographic_policy_parameters\"][\"CRS_UTM_EPSG\"]\n",
        "        if not crs_epsg or \"XXXX\" in crs_epsg:\n",
        "            errors.append(\"`CRS_UTM_EPSG` in config is not specified or is a placeholder.\")\n",
        "    except KeyError:\n",
        "        errors.append(\"Missing `CRS_UTM_EPSG` in `geographic_policy_parameters`.\")\n",
        "\n",
        "    # If any errors were found, raise a comprehensive exception.\n",
        "    if errors:\n",
        "        error_report = \"\\n- \".join([f\"[{task_name}] Validation of border crossing table failed:\"] + errors)\n",
        "        raise ValueError(error_report)\n",
        "\n",
        "    return df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Helper to Validate Task Mapping and Matched Controls\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_task_mapping_artifact(\n",
        "    config: Dict[str, Any],\n",
        "    main_df: pd.DataFrame,\n",
        "    task_name: str = \"Task 2, Step 2 (Task Mapping)\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads and validates the occupation-to-task mapping artifact.\n",
        "\n",
        "    Ensures the artifact has the correct schema and, critically, provides\n",
        "    complete coverage for all occupation codes present in the main dataset.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        main_df (pd.DataFrame): The main consolidated DataFrame to check against.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The loaded and validated task mapping DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If validation fails.\n",
        "    \"\"\"\n",
        "    # Load and perform basic schema validation on the task mapping artifact.\n",
        "    try:\n",
        "        artifact_config = config[\"auxiliary_data_artifacts\"][\"ROUTINE_ABSTRACT_MAPPING\"]\n",
        "        file_path = Path(artifact_config[\"path_or_handle\"])\n",
        "        df_map = pd.read_csv(file_path) # Assuming CSV\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"[{task_name}] Failed to load task mapping artifact. Error: {e}\")\n",
        "\n",
        "    expected_cols = {\"Occupation_Code_3digit\", \"Routine_or_Abstract_Label\", \"Abstract_Intensity\"}\n",
        "    if not expected_cols.issubset(df_map.columns):\n",
        "        raise ValueError(f\"[{task_name}] Task mapping artifact missing required columns. \"\n",
        "                         f\"Expected: {expected_cols}, Found: {set(df_map.columns)}\")\n",
        "\n",
        "    # --- Coverage Validation ---\n",
        "    # Ensure every occupation code in the main data has a corresponding mapping.\n",
        "    # This is critical to prevent data loss or errors in downstream analysis.\n",
        "    occupation_codes_in_data: Set[int] = set(main_df['Occupation_Code'].unique())\n",
        "    occupation_codes_in_map: Set[int] = set(df_map['Occupation_Code_3digit'].unique())\n",
        "\n",
        "    unmapped_codes = occupation_codes_in_data - occupation_codes_in_map\n",
        "    if unmapped_codes:\n",
        "        raise ValueError(\n",
        "            f\"[{task_name}] The task mapping is incomplete. \"\n",
        "            f\"{len(unmapped_codes)} occupation codes found in the main data \"\n",
        "            f\"are missing from the mapping file. Examples: {list(unmapped_codes)[:10]}\"\n",
        "        )\n",
        "\n",
        "    return df_map\n",
        "\n",
        "def _validate_matched_controls_artifact(\n",
        "    config: Dict[str, Any],\n",
        "    main_df: pd.DataFrame,\n",
        "    task_name: str = \"Task 2, Step 2 (Matched Controls)\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads and validates the matched controls list artifact.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        main_df (pd.DataFrame): The main consolidated DataFrame to check against.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The loaded and validated matched controls DataFrame.\n",
        "    \"\"\"\n",
        "    # Load and perform basic schema validation on the matched controls artifact.\n",
        "    try:\n",
        "        artifact_config = config[\"auxiliary_data_artifacts\"][\"MATCHED_CONTROLS_LIST\"]\n",
        "        file_path = Path(artifact_config[\"path_or_handle\"])\n",
        "        df_controls = pd.read_csv(file_path) # Assuming CSV\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"[{task_name}] Failed to load matched controls artifact. Error: {e}\")\n",
        "\n",
        "    expected_cols = {\"District_ID\", \"Municipality_ID\"}\n",
        "    if not expected_cols.issubset(df_controls.columns):\n",
        "        raise ValueError(f\"[{task_name}] Matched controls artifact missing required columns. \"\n",
        "                         f\"Expected: {expected_cols}, Found: {set(df_controls.columns)}\")\n",
        "\n",
        "    # --- Consistency Validation ---\n",
        "    # Ensure the list of matched control districts in the config is populated and consistent.\n",
        "    control_districts_from_config = set(config[\"geographic_policy_parameters\"][\"MATCHED_CONTROL_DISTRICT_IDS\"])\n",
        "    control_districts_from_artifact = set(df_controls['District_ID'].astype(str).unique())\n",
        "\n",
        "    if not control_districts_from_config:\n",
        "        raise ValueError(f\"[{task_name}] `MATCHED_CONTROL_DISTRICT_IDS` in config is empty.\")\n",
        "\n",
        "    if control_districts_from_config != control_districts_from_artifact:\n",
        "        raise ValueError(\n",
        "            f\"[{task_name}] Mismatch between `MATCHED_CONTROL_DISTRICT_IDS` in config \"\n",
        "            f\"and districts found in the matched controls artifact.\"\n",
        "        )\n",
        "\n",
        "    return df_controls\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Helper to Validate master_config Parameters\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class EventStudyShockRule(BaseModel):\n",
        "    \"\"\"\n",
        "    Defines the structure for a single event study shock rule, specifying the\n",
        "    start and end years for calculating the immigration inflow.\n",
        "    \"\"\"\n",
        "    # The start year of the period for shock calculation.\n",
        "    start_year: int\n",
        "\n",
        "    # The end year of the period for shock calculation.\n",
        "    end_year: int\n",
        "\n",
        "class TemporalParams(BaseModel):\n",
        "    \"\"\"\n",
        "    Validates the 'temporal_parameters' section of the config, ensuring all\n",
        "    critical dates and periods for the study are correctly specified.\n",
        "    \"\"\"\n",
        "    # The first year of data acquisition, fixed for replication.\n",
        "    DATA_ACQUISITION_START_YEAR: Literal[1986]\n",
        "\n",
        "    # The last year of data acquisition, fixed for replication.\n",
        "    DATA_ACQUISITION_END_YEAR: Literal[1995]\n",
        "\n",
        "    # The month for the annual employment snapshot.\n",
        "    ANNUAL_SNAPSHOT_MONTH: Literal[6]\n",
        "\n",
        "    # The day for the annual employment snapshot.\n",
        "    ANNUAL_SNAPSHOT_DAY: Literal[30]\n",
        "\n",
        "    # The pre-treatment baseline year, fixed for replication.\n",
        "    BASE_YEAR: Literal[1990]\n",
        "\n",
        "    # The first year the treatment (commuting policy) is in effect.\n",
        "    TREATMENT_START_YEAR: Literal[1991]\n",
        "\n",
        "    # A dictionary defining the specific shock calculation windows for the event study.\n",
        "    EVENT_STUDY_SHOCK_RULES: Dict[Literal[\"1991\", \"1992_to_1995\"], EventStudyShockRule]\n",
        "\n",
        "    @validator('EVENT_STUDY_SHOCK_RULES')\n",
        "    def check_shock_rules(cls, v: Dict) -> Dict:\n",
        "        \"\"\"Validator to check the precise content of the event study shock rules.\"\"\"\n",
        "        # Define the expected rule for the 1991 event year.\n",
        "        expected_1991_rule = EventStudyShockRule(start_year=1990, end_year=1991)\n",
        "        # Define the expected rule for all subsequent event years.\n",
        "        expected_post_1991_rule = EventStudyShockRule(start_year=1990, end_year=1992)\n",
        "\n",
        "        # Assert that the configured rules match the expected rules exactly.\n",
        "        if v.get(\"1991\") != expected_1991_rule:\n",
        "            raise ValueError(\"EVENT_STUDY_SHOCK_RULES for '1991' is incorrect.\")\n",
        "        if v.get(\"1992_to_1995\") != expected_post_1991_rule:\n",
        "            raise ValueError(\"EVENT_STUDY_SHOCK_RULES for '1992_to_1995' is incorrect.\")\n",
        "\n",
        "        # Return the validated dictionary.\n",
        "        return v\n",
        "\n",
        "class AlgorithmConfig(BaseModel):\n",
        "    \"\"\"\n",
        "    Validates the 'algorithm_config_parameters' section of the config,\n",
        "    ensuring key econometric and computational parameters are correct.\n",
        "    \"\"\"\n",
        "    # The number of bootstrap replications, fixed for replication.\n",
        "    BOOTSTRAP_REPLICATIONS: Literal[500]\n",
        "\n",
        "    # The type of distribution for wild bootstrap weights, fixed to Rademacher.\n",
        "    WILD_BOOTSTRAP_TYPE: Literal[\"Rademacher\"]\n",
        "\n",
        "    # The geographic level for clustering standard errors, fixed to District_ID.\n",
        "    CLUSTER_LEVEL: Literal[\"District_ID\"]\n",
        "\n",
        "    # The mapping of employment type codes to Full-Time Equivalent weights.\n",
        "    PART_TIME_EQUIVALENCY_WEIGHTS: Dict[int, float]\n",
        "\n",
        "    @validator('PART_TIME_EQUIVALENCY_WEIGHTS')\n",
        "    def check_fte_weights(cls, v: Dict) -> Dict:\n",
        "        \"\"\"Validator to check the precise content of the FTE weight mapping.\"\"\"\n",
        "        # Define the exact dictionary of weights required by the study.\n",
        "        expected = {1: 1.00, 5: 0.67, 6: 0.50}\n",
        "\n",
        "        # Assert that the configured weights match the expected weights exactly.\n",
        "        if v != expected:\n",
        "            raise ValueError(f\"PART_TIME_EQUIVALENCY_WEIGHTS must be exactly {expected}.\")\n",
        "\n",
        "        # Return the validated dictionary.\n",
        "        return v\n",
        "\n",
        "class MasterConfigModel(BaseModel):\n",
        "    \"\"\"\n",
        "    The Pydantic model for the entire master_config structure.\n",
        "\n",
        "    This top-level model composes the nested models and validates the presence\n",
        "    of all major sections of the configuration dictionary.\n",
        "    \"\"\"\n",
        "    # Validate the nested 'temporal_parameters' dictionary against its specific model.\n",
        "    temporal_parameters: TemporalParams\n",
        "\n",
        "    # Validate the nested 'algorithm_config_parameters' dictionary against its specific model.\n",
        "    algorithm_config_parameters: AlgorithmConfig\n",
        "\n",
        "    # For other sections, we ensure they exist as dictionaries but do not apply\n",
        "    # detailed field-level validation in this remediation step for brevity.\n",
        "    # A full production implementation would define models for these as well.\n",
        "    study_metadata: Dict\n",
        "    sample_selection_parameters: Dict\n",
        "    geographic_policy_parameters: Dict\n",
        "    variable_coding_maps: Dict\n",
        "    shock_and_scaling_parameters: Dict\n",
        "    wage_imputation_parameters: Dict\n",
        "    selection_bounding_parameters: Dict\n",
        "    auxiliary_data_artifacts: Dict\n",
        "\n",
        "def _validate_config_parameters(\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 2, Step 3\"\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Performs rigorous validation of the master_config dictionary using Pydantic.\n",
        "\n",
        "    This function replaces manual, brittle checking with a formal, schema-based\n",
        "    validation approach. It defines the expected structure, types, and specific\n",
        "    values for critical parameters and uses Pydantic to parse and validate the\n",
        "    input configuration dictionary against this schema. This method is vastly\n",
        "    more robust, maintainable, and provides superior error reporting.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The master configuration dictionary to be validated.\n",
        "        task_name (str): The name of the calling task for clear error reporting.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of human-readable error messages if validation fails.\n",
        "                   An empty list signifies that the configuration is valid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # The core of the validation: attempt to parse the raw dictionary\n",
        "        # into the formal Pydantic model. Pydantic handles all type checks,\n",
        "        # value constraints (via Literal), and custom validator functions.\n",
        "        MasterConfigModel.parse_obj(config)\n",
        "\n",
        "        # If parsing succeeds without raising a ValidationError, the config is valid.\n",
        "        # Return an empty list to signal success to the orchestrator.\n",
        "        return []\n",
        "\n",
        "    except ValidationError as e:\n",
        "        # If parsing fails, Pydantic's ValidationError contains a detailed,\n",
        "        # structured list of all issues found throughout the nested structure.\n",
        "\n",
        "        # We format these structured errors into a simple list of strings\n",
        "        # as expected by the calling orchestrator function.\n",
        "        error_messages = []\n",
        "        for error in e.errors():\n",
        "            # 'loc' provides the path to the failing key, e.g., ('temporal_parameters', 'BASE_YEAR').\n",
        "            loc = \".\".join(map(str, error['loc']))\n",
        "            # 'msg' provides a human-readable description of the failure.\n",
        "            msg = error['msg']\n",
        "            # Append the formatted error message to our list.\n",
        "            error_messages.append(f\"[{task_name}] Config validation error at '{loc}': {msg}\")\n",
        "\n",
        "        # Return the comprehensive list of all validation failures.\n",
        "        return error_messages\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_artifacts_and_config(\n",
        "    master_config: Dict[str, Any],\n",
        "    consolidated_df_raw: pd.DataFrame\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates validation of auxiliary data artifacts and master_config.\n",
        "\n",
        "    This function is the main entry point for Task 2. It validates all\n",
        "    external data dependencies and the main configuration dictionary that\n",
        "    drives the entire analysis pipeline, ensuring reproducibility and correctness.\n",
        "\n",
        "    Args:\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        consolidated_df_raw (pd.DataFrame): The main spell-level DataFrame,\n",
        "            required for cross-validation checks (e.g., occupation code coverage).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary containing the loaded and\n",
        "            validated auxiliary DataFrames, keyed by their artifact name\n",
        "            (e.g., \"border_crossings\", \"task_mapping\").\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If inputs are of the wrong type.\n",
        "        ValueError: If any validation check fails, with a comprehensive report.\n",
        "        FileNotFoundError: If an artifact file is not found.\n",
        "    \"\"\"\n",
        "    # --- Input Type Validation ---\n",
        "    if not isinstance(master_config, dict):\n",
        "        raise TypeError(\"`master_config` must be a dictionary.\")\n",
        "    if not isinstance(consolidated_df_raw, pd.DataFrame):\n",
        "        raise TypeError(\"`consolidated_df_raw` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Execute Validation Steps Sequentially ---\n",
        "    validated_artifacts = {}\n",
        "    all_errors = []\n",
        "\n",
        "    # Step 1: Validate Border Crossing Table.\n",
        "    try:\n",
        "        validated_artifacts[\"border_crossings\"] = _validate_border_crossing_table(master_config)\n",
        "    except (ValueError, FileNotFoundError) as e:\n",
        "        all_errors.append(str(e))\n",
        "\n",
        "    # Step 2: Validate Task Mapping and Matched Controls artifacts.\n",
        "    try:\n",
        "        validated_artifacts[\"task_mapping\"] = _validate_task_mapping_artifact(master_config, consolidated_df_raw)\n",
        "    except ValueError as e:\n",
        "        all_errors.append(str(e))\n",
        "\n",
        "    try:\n",
        "        validated_artifacts[\"matched_controls\"] = _validate_matched_controls_artifact(master_config, consolidated_df_raw)\n",
        "    except ValueError as e:\n",
        "        all_errors.append(str(e))\n",
        "\n",
        "    # Step 3: Validate critical parameters within the master_config dictionary.\n",
        "    config_errors = _validate_config_parameters(master_config)\n",
        "    if config_errors:\n",
        "        all_errors.extend(config_errors)\n",
        "\n",
        "    # --- Final Verdict ---\n",
        "    if all_errors:\n",
        "        error_report = \"\\n- \".join([\"Artifact and configuration validation failed:\"] + all_errors)\n",
        "        raise ValueError(error_report)\n",
        "\n",
        "    print(\"Task 2: Validation of auxiliary artifacts and master_config completed successfully.\")\n",
        "\n",
        "    return validated_artifacts\n"
      ],
      "metadata": {
        "id": "8aAz1cUlncli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Cleanse and canonicalize spell-level data\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Cleanse and canonicalize spell-level data\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Helper to Normalize Dtypes and Enforce String Formatting\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _normalize_dtypes_and_formats(\n",
        "    df: pd.DataFrame,\n",
        "    task_name: str = \"Task 3, Step 1\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalizes dtypes and standardizes string formats with immediate validation.\n",
        "\n",
        "    This function creates a cleansed copy of the input DataFrame. It iterates\n",
        "    through columns, coercing them to their canonical data types (datetime,\n",
        "    numeric, string). Crucially, after each coercion on a mandatory field, it\n",
        "    immediately validates that no null values were introduced, ensuring data\n",
        "    integrity and failing fast on parsing errors. It also standardizes the\n",
        "    format of key string identifiers (e.g., zero-padding for Municipality_ID).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The validated, raw spell-level DataFrame from Task 1.\n",
        "        task_name (str): The name of the calling task for clear error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame with cleansed and validated dtypes and formats.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a mandatory column cannot be coerced to its target\n",
        "                    dtype without introducing null values (e.g., 'abc' in an\n",
        "                    integer column), indicating corrupt source data.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input `df` must be a pandas DataFrame.\")\n",
        "\n",
        "    # Work on a copy to prevent side effects on the original DataFrame.\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # --- Schema and Mandatory Fields Definition ---\n",
        "    # Define the target dtypes for all relevant columns.\n",
        "    schema = {\n",
        "        'date': ['End_Date'],\n",
        "        'int': ['Spell_Sequence_ID', 'Occupation_Code', 'Nationality_Code',\n",
        "                'Gender_Code', 'Wage_Cap_Year', 'Birth_Year',\n",
        "                'Education_Level_Code', 'Employment_Type_Code',\n",
        "                'Reason_for_Termination', 'Industry_Code', 'Firm_Size_Code',\n",
        "                'State_ID'],\n",
        "        'float': ['Daily_Wage_EUR', 'Social_Security_Cap_EUR',\n",
        "                  'Workplace_Coord_X_UTM', 'Workplace_Coord_Y_UTM'],\n",
        "        'str_pad': {'Municipality_ID': 5}, # Columns to be zero-padded\n",
        "        'str_strip': ['Employer_ID', 'Establishment_ID', 'District_ID'] # Columns to be stripped\n",
        "    }\n",
        "    # Define which columns cannot have nulls after coercion.\n",
        "    mandatory_fields = set(schema['date'] + schema['int'] + schema['float'])\n",
        "\n",
        "    # --- 1. Date Type Canonicalization and Validation ---\n",
        "\n",
        "    # Reset index to handle 'Start_Date' as a column temporarily.\n",
        "    df_clean = df_clean.reset_index()\n",
        "    date_cols = schema['date'] + ['Start_Date']\n",
        "\n",
        "    for col in date_cols:\n",
        "        # Coerce column to datetime, turning unparseable values into NaT.\n",
        "        df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
        "\n",
        "        # Immediately validate that no nulls were introduced in this mandatory field.\n",
        "        if df_clean[col].isnull().any():\n",
        "            raise ValueError(\n",
        "                f\"[{task_name}] Failed to parse all values in date column '{col}'. \"\n",
        "                \"Null values (NaT) were introduced, indicating corrupt data.\"\n",
        "            )\n",
        "\n",
        "    # --- 2. Numeric Type Canonicalization and Validation ---\n",
        "\n",
        "    # Process integer columns.\n",
        "    for col in schema['int']:\n",
        "        # Coerce column to numeric, turning non-numeric values into NaN.\n",
        "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "        # Immediately validate that no nulls were introduced.\n",
        "        if col in mandatory_fields and df_clean[col].isnull().any():\n",
        "            raise ValueError(\n",
        "                f\"[{task_name}] Failed to parse all values in integer column '{col}'. \"\n",
        "                \"Null values were introduced, indicating corrupt data.\"\n",
        "            )\n",
        "        # Cast to a nullable integer type to handle potential NaNs in non-mandatory fields.\n",
        "        df_clean[col] = df_clean[col].astype('Int64')\n",
        "\n",
        "    # Process float columns.\n",
        "    for col in schema['float']:\n",
        "        # Coerce column to numeric.\n",
        "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "        # Immediately validate that no nulls were introduced.\n",
        "        if col in mandatory_fields and df_clean[col].isnull().any():\n",
        "            raise ValueError(\n",
        "                f\"[{task_name}] Failed to parse all values in float column '{col}'. \"\n",
        "                \"Null values were introduced, indicating corrupt data.\"\n",
        "            )\n",
        "\n",
        "    # --- 3. String Formatting Canonicalization ---\n",
        "\n",
        "    # Process strings that require zero-padding.\n",
        "    for col, width in schema['str_pad'].items():\n",
        "        # Ensure column is string type, strip whitespace, then apply zfill.\n",
        "        df_clean[col] = df_clean[col].astype(str).str.strip().str.zfill(width)\n",
        "\n",
        "    # Process strings that require stripping of whitespace.\n",
        "    for col in schema['str_strip']:\n",
        "        # Ensure column is string type and strip whitespace.\n",
        "        df_clean[col] = df_clean[col].astype(str).str.strip()\n",
        "\n",
        "    # --- Finalization ---\n",
        "\n",
        "    # Set the canonical MultiIndex after all transformations are complete.\n",
        "    df_clean = df_clean.set_index(['Worker_ID', 'Start_Date'])\n",
        "\n",
        "    # Log success message.\n",
        "    print(f\"[{task_name}] Dtypes and string formats normalized and validated successfully.\")\n",
        "\n",
        "    # Return the fully cleansed and validated DataFrame.\n",
        "    return df_clean\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Helper to Resolve Overlapping Spells at Snapshots\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _resolve_main_job_at_snapshots(\n",
        "    df_spells: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 3, Step 2\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transforms spell-level data into a worker-year panel using a vectorized approach.\n",
        "\n",
        "    This function efficiently resolves concurrent employment spells to identify a\n",
        "    single \"main job\" for each worker at each annual snapshot date (June 30).\n",
        "    It avoids Python-level loops by expanding each spell into the years it covers,\n",
        "    filtering for activity on the snapshot date, and then applying a single,\n",
        "    globally sorted de-duplication based on the study's deterministic\n",
        "    tie-breaking rule.\n",
        "\n",
        "    The tie-breaking rule is:\n",
        "    1. Highest 'Daily_Wage_EUR'.\n",
        "    2. Longest spell duration.\n",
        "    3. Earliest 'Start_Date'.\n",
        "\n",
        "    Args:\n",
        "        df_spells (pd.DataFrame): The cleansed, spell-level DataFrame from\n",
        "                                  the previous normalization step.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for clear error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A worker-year panel DataFrame where each row represents\n",
        "                      the main job of a worker in a given year. The index is\n",
        "                      set to ['Worker_ID', 'snapshot_year']. Returns an empty\n",
        "                      DataFrame if no active spells are found.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df_spells, pd.DataFrame):\n",
        "        raise TypeError(\"Input `df_spells` must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input `config` must be a dictionary.\")\n",
        "\n",
        "    # --- 1. Prepare Data and Generate Snapshot Information ---\n",
        "\n",
        "    # Extract necessary temporal parameters from the configuration.\n",
        "    start_year = config[\"temporal_parameters\"][\"DATA_ACQUISITION_START_YEAR\"]\n",
        "    end_year = config[\"temporal_parameters\"][\"DATA_ACQUISITION_END_YEAR\"]\n",
        "    snap_month = config[\"temporal_parameters\"][\"ANNUAL_SNAPSHOT_MONTH\"]\n",
        "    snap_day = config[\"temporal_parameters\"][\"ANNUAL_SNAPSHOT_DAY\"]\n",
        "\n",
        "    # Create a DataFrame mapping each year in the study to its snapshot date.\n",
        "    all_years = range(start_year, end_year + 1)\n",
        "    snapshots = pd.DataFrame({\n",
        "        'snapshot_year': all_years,\n",
        "        'snapshot_date': [pd.Timestamp(year, snap_month, snap_day) for year in all_years]\n",
        "    })\n",
        "\n",
        "    # Work with a copy of the spell data, resetting the index to access date columns.\n",
        "    spells = df_spells.reset_index()\n",
        "\n",
        "    # --- 2. Vectorized Expansion of Spells to Spell-Year Level ---\n",
        "\n",
        "    # For each spell, create a list of all years it spans.\n",
        "    # This is the key step to enable vectorization and avoid loops.\n",
        "    spells['snapshot_year'] = spells.apply(\n",
        "        lambda row: list(range(row['Start_Date'].year, row['End_Date'].year + 1)),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Use .explode() to transform the DataFrame into a long format, creating one\n",
        "    # row for each year a spell is potentially active.\n",
        "    spell_year_panel = spells.explode('snapshot_year')\n",
        "\n",
        "    # Convert the exploded year to an integer type.\n",
        "    spell_year_panel['snapshot_year'] = pd.to_numeric(spell_year_panel['snapshot_year'], errors='coerce').astype('Int64')\n",
        "    spell_year_panel.dropna(subset=['snapshot_year'], inplace=True)\n",
        "\n",
        "    # --- 3. Filter to Spells Active on the Snapshot Date ---\n",
        "\n",
        "    # Merge the snapshot date for each year into the panel.\n",
        "    active_spells = spell_year_panel.merge(snapshots, on='snapshot_year', how='left')\n",
        "\n",
        "    # Apply the precise filter: a spell is active if the snapshot date is within its start and end dates.\n",
        "    active_spells = active_spells[\n",
        "        (active_spells['Start_Date'] <= active_spells['snapshot_date']) &\n",
        "        (active_spells['End_Date'] >= active_spells['snapshot_date'])\n",
        "    ].copy()\n",
        "\n",
        "    # If no spells are active on any snapshot date, return an empty DataFrame.\n",
        "    if active_spells.empty:\n",
        "        print(f\"[{task_name}] No active spells found on any snapshot date.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # --- 4. Apply Deterministic Tie-Breaking Rule ---\n",
        "\n",
        "    # a. Compute the spell duration, which is the second tie-breaker criterion.\n",
        "    active_spells['spell_duration'] = (active_spells['End_Date'] - active_spells['Start_Date']).dt.days\n",
        "\n",
        "    # b. Sort the entire DataFrame according to the tie-breaking hierarchy.\n",
        "    # This single sort operation is highly efficient.\n",
        "    sorted_spells = active_spells.sort_values(\n",
        "        by=[\n",
        "            'Worker_ID',\n",
        "            'snapshot_year',\n",
        "            'Daily_Wage_EUR',      # 1. Highest wage first\n",
        "            'spell_duration',      # 2. Longest duration first\n",
        "            'Start_Date'           # 3. Earliest start date first\n",
        "        ],\n",
        "        ascending=[\n",
        "            True,\n",
        "            True,\n",
        "            False,                 # Descending for wage\n",
        "            False,                 # Descending for duration\n",
        "            True                   # Ascending for start date\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # c. Select the first row for each worker-year group. This is the main job.\n",
        "    main_jobs = sorted_spells.drop_duplicates(subset=['Worker_ID', 'snapshot_year'], keep='first')\n",
        "\n",
        "    # --- 5. Finalize the Panel DataFrame ---\n",
        "\n",
        "    # Set the canonical index for the worker-year panel.\n",
        "    worker_year_panel = main_jobs.set_index(['Worker_ID', 'snapshot_year'])\n",
        "\n",
        "    # Remove temporary columns used for the tie-breaking process.\n",
        "    worker_year_panel = worker_year_panel.drop(columns=['snapshot_date', 'spell_duration'])\n",
        "\n",
        "    # Log success message.\n",
        "    print(f\"[{task_name}] Worker-year panel with main jobs created successfully using vectorized approach.\")\n",
        "\n",
        "    # Return the final, clean panel.\n",
        "    return worker_year_panel\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Helper to Apply Filters and Create Sample Indicators\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _apply_filters_and_create_flags(\n",
        "    worker_year_panel: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 3, Step 3\"\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Applies sample selection filters and creates analysis flags.\n",
        "\n",
        "    This function takes the raw worker-year panel of main jobs and performs two\n",
        "    critical operations:\n",
        "    1.  **Enrichment**: It computes worker age and creates a comprehensive set of\n",
        "        boolean flags for key subpopulations (e.g., full-time, older workers,\n",
        "        apprentices). These flags are created on the full, unfiltered dataset\n",
        "        to preserve information for all auxiliary analyses.\n",
        "    2.  **Filtering**: It creates a second, filtered DataFrame that represents the\n",
        "        core analysis sample by applying the study's main inclusion criteria\n",
        "        (age range and exclusion of irregular/marginal/seasonal employment).\n",
        "\n",
        "    Args:\n",
        "        worker_year_panel (pd.DataFrame): The worker-year panel of main jobs,\n",
        "            output from the spell resolution step.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for clear error reporting.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing:\n",
        "        - panel_with_flags (pd.DataFrame): The complete, unfiltered worker-year\n",
        "          panel enriched with age and all boolean indicator columns.\n",
        "        - panel_filtered (pd.DataFrame): A filtered view of the panel representing\n",
        "          the main analysis sample.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(worker_year_panel, pd.DataFrame):\n",
        "        raise TypeError(\"Input `worker_year_panel` must be a pandas DataFrame.\")\n",
        "    if worker_year_panel.empty:\n",
        "        print(f\"[{task_name}] Input worker-year panel is empty. Returning two empty DataFrames.\")\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # Work on a copy to prevent side effects.\n",
        "    panel_with_flags = worker_year_panel.copy()\n",
        "\n",
        "    # --- 1. Compute Age ---\n",
        "    # Age is defined as the calendar year of the snapshot minus the worker's birth year.\n",
        "    # This is a fundamental demographic variable for filtering and controls.\n",
        "    panel_with_flags['age'] = panel_with_flags.index.get_level_values('snapshot_year') - panel_with_flags['Birth_Year']\n",
        "\n",
        "    # --- 2. Create Sample Indicator Flags (on the full panel) ---\n",
        "    # It is critical to create flags before filtering, as some analyses (e.g., on\n",
        "    # apprentices) rely on data that will be excluded from the main sample.\n",
        "\n",
        "    # Flag for full-time workers, used in all wage analyses.\n",
        "    panel_with_flags['is_full_time'] = panel_with_flags['Employment_Type_Code'].isin(\n",
        "        config[\"sample_selection_parameters\"][\"WAGE_ANALYSIS_EMPLOYMENT_CODES\"]\n",
        "    )\n",
        "\n",
        "    # Flag for older workers, used in the heterogeneity analysis (Task 16).\n",
        "    panel_with_flags['is_older_worker'] = panel_with_flags['age'] >= config[\"sample_selection_parameters\"][\"OLDER_WORKER_AGE_THRESHOLD\"]\n",
        "\n",
        "    # Flag for apprentices, used in the training uptake analysis (Task 19).\n",
        "    # This flag is based on an employment code that is typically excluded from the main sample.\n",
        "    panel_with_flags['is_apprentice'] = panel_with_flags['Employment_Type_Code'] == config[\"variable_coding_maps\"][\"APPRENTICESHIP_CODE\"]\n",
        "\n",
        "    # --- 3. Apply Core Sample Selection Filters ---\n",
        "\n",
        "    # Record the initial size of the panel for auditing and logging purposes.\n",
        "    initial_size = len(panel_with_flags)\n",
        "\n",
        "    # a. Filter by employment type to exclude irregular, marginal, and seasonal work\n",
        "    # from the main analysis sample, as specified in the paper.\n",
        "    excluded_emp_types = config[\"sample_selection_parameters\"][\"EXCLUDED_EMPLOYMENT_TYPE_CODES\"]\n",
        "    main_sample_mask = ~panel_with_flags['Employment_Type_Code'].isin(excluded_emp_types)\n",
        "\n",
        "    # b. Filter by the valid age range for the study population.\n",
        "    min_age = config[\"sample_selection_parameters\"][\"WORKER_AGE_MIN\"]\n",
        "    max_age = config[\"sample_selection_parameters\"][\"WORKER_AGE_MAX\"]\n",
        "    age_mask = (panel_with_flags['age'] >= min_age) & (panel_with_flags['age'] <= max_age)\n",
        "\n",
        "    # Combine the masks to define the final filtered sample.\n",
        "    panel_filtered = panel_with_flags[main_sample_mask & age_mask].copy()\n",
        "\n",
        "    # Report on the filtering process to provide a clear audit trail.\n",
        "    final_size = len(panel_filtered)\n",
        "    print(\n",
        "        f\"[{task_name}] Sample filtering complete. \"\n",
        "        f\"Initial size: {initial_size}, Main analysis sample size: {final_size} \"\n",
        "        f\"({initial_size - final_size} observations excluded from main sample).\"\n",
        "    )\n",
        "\n",
        "    # Return both the fully enriched panel and the main filtered panel.\n",
        "    return panel_with_flags, panel_filtered\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_and_canonicalize_spells(\n",
        "    consolidated_df_raw: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the cleansing and canonicalization of spell-level data.\n",
        "\n",
        "    This function serves as the master orchestrator for Task 3. It executes a\n",
        "    three-step pipeline that transforms the raw, spell-level data into the\n",
        "    foundational datasets for the entire analysis. This revised version returns\n",
        "    three distinct DataFrames to correctly manage data dependencies for all\n",
        "    downstream tasks.\n",
        "\n",
        "    The pipeline is as follows:\n",
        "    1.  **Normalization**: Standardizes all data types and string formats of the\n",
        "        raw spell data (`_normalize_dtypes_and_formats`).\n",
        "    2.  **Spell Resolution**: Transforms the spell data into a worker-year panel\n",
        "        by identifying a unique \"main job\" for each worker at each annual\n",
        "        snapshot (`_resolve_main_job_at_snapshots`).\n",
        "    3.  **Flagging & Filtering**: Enriches the panel with computed age and all\n",
        "        necessary boolean flags, then creates a second, filtered version of\n",
        "        the panel for the main analysis sample (`_apply_filters_and_create_flags`).\n",
        "\n",
        "    Args:\n",
        "        consolidated_df_raw (pd.DataFrame): The validated, raw spell-level\n",
        "            DataFrame from Task 1.\n",
        "        master_config (Dict[str, Any]): The validated master configuration\n",
        "            dictionary from Task 2.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: A tuple containing:\n",
        "        - **df_normalized**: The full, spell-level DataFrame after dtype and\n",
        "          format normalization. This is required by Task 5 to know the\n",
        "          universe of all workers.\n",
        "        - **panel_full_with_flags**: The complete, unfiltered worker-year panel\n",
        "          containing all workers with main jobs, enriched with age and all\n",
        "          boolean indicator columns.\n",
        "        - **panel_main_analysis**: A filtered view of the panel representing the\n",
        "          core analysis sample.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the primary inputs are of the correct type before proceeding.\n",
        "    if not isinstance(consolidated_df_raw, pd.DataFrame):\n",
        "        raise TypeError(\"`consolidated_df_raw` must be a pandas DataFrame.\")\n",
        "    if not isinstance(master_config, dict):\n",
        "        raise TypeError(\"`master_config` must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Normalize dtypes and enforce string formatting ---\n",
        "    # This step produces the first critical output: the cleansed spell data.\n",
        "    df_normalized = _normalize_dtypes_and_formats(\n",
        "        df=consolidated_df_raw,\n",
        "        task_name=\"Task 3, Step 1\"\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Resolve overlapping spells and create the worker-year panel ---\n",
        "    # This step transforms the cleansed spell data into a raw panel of main jobs.\n",
        "    worker_year_panel = _resolve_main_job_at_snapshots(\n",
        "        df_spells=df_normalized,\n",
        "        config=master_config,\n",
        "        task_name=\"Task 3, Step 2\"\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Apply sample filters and create indicator flags ---\n",
        "    # This step takes the raw panel and produces the final two panel outputs.\n",
        "    panel_full_with_flags, panel_main_analysis = _apply_filters_and_create_flags(\n",
        "        worker_year_panel=worker_year_panel,\n",
        "        config=master_config,\n",
        "        task_name=\"Task 3, Step 3\"\n",
        "    )\n",
        "\n",
        "    # Log the successful completion of the entire task.\n",
        "    print(\"\\nTask 3: Cleansing and canonicalization of spell-level data completed successfully.\")\n",
        "\n",
        "    # Return all three essential DataFrames for the downstream pipeline.\n",
        "    return df_normalized, panel_full_with_flags, panel_main_analysis\n"
      ],
      "metadata": {
        "id": "HBR8SJsQoSeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Impute censored wages (Tobit-style, for full-time spells)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Impute censored wages (Tobit-style, for full-time spells)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Helper to Prepare Data for Wage Imputation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _prepare_wage_imputation_data(\n",
        "    worker_year_panel: pd.DataFrame,\n",
        "    task_name: str = \"Task 4, Step 1\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Subsets to full-time workers and prepares columns for Tobit imputation.\n",
        "\n",
        "    This function filters the panel to full-time observations, computes raw log\n",
        "    wages, and creates essential flags and log-transformed cap values needed\n",
        "    for the censored regression model.\n",
        "\n",
        "    Args:\n",
        "        worker_year_panel (pd.DataFrame): The cleansed worker-year panel from Task 3.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame of full-time workers with columns required\n",
        "                      for imputation ('log_wage_raw', 'is_censored', 'log_wage_cap').\n",
        "    \"\"\"\n",
        "    # Filter the panel to include only full-time workers, as wage analysis is\n",
        "    # restricted to this group.\n",
        "    full_time_df = worker_year_panel[worker_year_panel['is_full_time']].copy()\n",
        "\n",
        "    # Input validation: Ensure daily wages are strictly positive before logging.\n",
        "    if (full_time_df['Daily_Wage_EUR'] <= 0).any():\n",
        "        raise ValueError(f\"[{task_name}] Found non-positive 'Daily_Wage_EUR' values, \"\n",
        "                         \"which cannot be log-transformed.\")\n",
        "\n",
        "    # Create the raw log wage column from the daily wage.\n",
        "    full_time_df['log_wage_raw'] = np.log(full_time_df['Daily_Wage_EUR'])\n",
        "\n",
        "    # Create a definitive boolean flag for censored observations.\n",
        "    # This relies on the 'Is_TopCoded_Wage' flag validated in Task 1.\n",
        "    full_time_df['is_censored'] = full_time_df['Is_TopCoded_Wage']\n",
        "\n",
        "    # Create the log of the censoring cap.\n",
        "    full_time_df['log_wage_cap'] = np.log(full_time_df['Social_Security_Cap_EUR'])\n",
        "\n",
        "    print(f\"[{task_name}] Prepared {len(full_time_df)} full-time observations for wage imputation.\")\n",
        "    return full_time_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Helper to Fit Censored-Normal (Tobit) Models by Group\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _estimate_tobit_parameters_for_group(\n",
        "    group_df: pd.DataFrame\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Estimates parameters (mu, sigma) for a single group using MLE.\n",
        "\n",
        "    This function defines and minimizes the negative log-likelihood for a\n",
        "    right-censored normal distribution.\n",
        "\n",
        "    Args:\n",
        "        group_df (pd.DataFrame): A subset of the data for a single imputation group.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A series containing the estimated 'mu' and 'sigma'.\n",
        "    \"\"\"\n",
        "    # Separate censored and uncensored observations within the group.\n",
        "    uncensored_wages = group_df.loc[~group_df['is_censored'], 'log_wage_raw']\n",
        "    censored_caps = group_df.loc[group_df['is_censored'], 'log_wage_cap']\n",
        "\n",
        "    # Define the negative log-likelihood function for the Tobit model.\n",
        "    def neg_log_likelihood(params: np.ndarray) -> float:\n",
        "        mu, log_sigma = params[0], params[1]\n",
        "        sigma = np.exp(log_sigma) # Ensure sigma is positive via reparameterization.\n",
        "\n",
        "        # Log-likelihood for uncensored observations.\n",
        "        ll_uncensored = norm.logpdf(uncensored_wages, loc=mu, scale=sigma).sum()\n",
        "\n",
        "        # Log-likelihood for censored observations (using log of survival function for stability).\n",
        "        ll_censored = norm.logsf(censored_caps, loc=mu, scale=sigma).sum()\n",
        "\n",
        "        # Total negative log-likelihood.\n",
        "        return -(ll_uncensored + ll_censored)\n",
        "\n",
        "    # Provide initial guesses for the optimization.\n",
        "    # Use moments from the uncensored part of the data as a starting point.\n",
        "    mu_initial = uncensored_wages.mean() if not uncensored_wages.empty else 5.0\n",
        "    sigma_initial = uncensored_wages.std() if len(uncensored_wages) > 1 else 1.0\n",
        "    # Handle cases where std is zero or NaN.\n",
        "    if not np.isfinite(sigma_initial) or sigma_initial <= 0:\n",
        "        sigma_initial = 1.0\n",
        "\n",
        "    initial_params = np.array([mu_initial, np.log(sigma_initial)])\n",
        "\n",
        "    # Perform the optimization to find the MLE parameters.\n",
        "    result = minimize(\n",
        "        neg_log_likelihood,\n",
        "        initial_params,\n",
        "        method='L-BFGS-B' # A robust quasi-Newton method.\n",
        "    )\n",
        "\n",
        "    # If optimization fails, return NaNs to be handled by the fallback mechanism.\n",
        "    if not result.success:\n",
        "        return pd.Series({'mu': np.nan, 'sigma': np.nan})\n",
        "\n",
        "    # Extract and return the estimated parameters.\n",
        "    mu_hat, log_sigma_hat = result.x\n",
        "    sigma_hat = np.exp(log_sigma_hat)\n",
        "    return pd.Series({'mu': mu_hat, 'sigma': sigma_hat})\n",
        "\n",
        "def _estimate_tobit_parameters_by_group(\n",
        "    full_time_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 4, Step 2\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Estimates Tobit model parameters for each imputation group.\n",
        "\n",
        "    Groups data as specified in the config, applies MLE to each group, and\n",
        "    handles small groups or convergence failures with a fallback mechanism.\n",
        "\n",
        "    Args:\n",
        "        full_time_df (pd.DataFrame): The prepared full-time wage data.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The input DataFrame with 'mu' and 'sigma' columns merged in.\n",
        "    \"\"\"\n",
        "    # Define the primary grouping keys from the configuration.\n",
        "    grouping_keys = config[\"wage_imputation_parameters\"][\"IMPUTATION_GROUPING\"]\n",
        "\n",
        "    # --- Primary Estimation ---\n",
        "    # Estimate parameters for each group defined by the grouping keys.\n",
        "    print(f\"[{task_name}] Estimating Tobit parameters for {grouping_keys} groups...\")\n",
        "    tobit_params = full_time_df.groupby(grouping_keys).apply(_estimate_tobit_parameters_for_group)\n",
        "\n",
        "    # Merge the estimated parameters back to the main DataFrame.\n",
        "    df_with_params = full_time_df.merge(\n",
        "        tobit_params.rename(columns={'mu': 'mu_est', 'sigma': 'sigma_est'}),\n",
        "        on=grouping_keys,\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # --- Fallback Mechanism ---\n",
        "    # Identify observations where estimation failed (e.g., small groups, non-convergence).\n",
        "    failed_estimation_mask = df_with_params['mu_est'].isnull()\n",
        "    if failed_estimation_mask.any():\n",
        "        print(f\"[{task_name}] Primary estimation failed for {failed_estimation_mask.sum()} observations. \"\n",
        "              \"Applying fallback estimation (pooling by gender).\")\n",
        "\n",
        "        # Fallback 1: Pool by the first grouping key (e.g., 'Gender_Code').\n",
        "        fallback_grouping = [grouping_keys[0]]\n",
        "        fallback_params = df_with_params[failed_estimation_mask].groupby(fallback_grouping).apply(_estimate_tobit_parameters_for_group)\n",
        "\n",
        "        # Merge fallback parameters.\n",
        "        fallback_merged = df_with_params[failed_estimation_mask].drop(columns=['mu_est', 'sigma_est']).merge(\n",
        "            fallback_params.rename(columns={'mu': 'mu_fb', 'sigma': 'sigma_fb'}),\n",
        "            on=fallback_grouping,\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Update the main DataFrame with the fallback estimates.\n",
        "        df_with_params.loc[failed_estimation_mask, 'mu_est'] = fallback_merged['mu_fb']\n",
        "        df_with_params.loc[failed_estimation_mask, 'sigma_est'] = fallback_merged['sigma_fb']\n",
        "\n",
        "    # Final check for any remaining nulls (e.g., if fallback also failed).\n",
        "    if df_with_params['mu_est'].isnull().any():\n",
        "        raise RuntimeError(f\"[{task_name}] Wage imputation failed. Could not estimate \"\n",
        "                         \"Tobit parameters even after fallback.\")\n",
        "\n",
        "    print(f\"[{task_name}] Tobit parameter estimation complete.\")\n",
        "    return df_with_params\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Helper to Replace Censored Wages with Conditional Expectation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _impute_censored_wages(\n",
        "    df_with_params: pd.DataFrame,\n",
        "    task_name: str = \"Task 4, Step 3\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Imputes censored log wages using the conditional expectation formula.\n",
        "\n",
        "    Calculates E[log_w | log_w >= cap] for censored observations using the\n",
        "    estimated Tobit parameters and creates the final imputed wage column.\n",
        "\n",
        "    Args:\n",
        "        df_with_params (pd.DataFrame): DataFrame with raw log wages and estimated\n",
        "                                       Tobit parameters ('mu_est', 'sigma_est').\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with the new 'log_wage_imputed' column.\n",
        "    \"\"\"\n",
        "    # Isolate the data for censored observations.\n",
        "    censored_df = df_with_params[df_with_params['is_censored']].copy()\n",
        "\n",
        "    # --- Calculate Conditional Expectation ---\n",
        "    # E[log_w | log_w >= c] = mu + sigma * (phi(z) / (1 - Phi(z)))\n",
        "    # where z = (c - mu) / sigma\n",
        "    mu = censored_df['mu_est']\n",
        "    sigma = censored_df['sigma_est']\n",
        "    log_cap = censored_df['log_wage_cap']\n",
        "\n",
        "    # Standardized value z for the inverse Mills ratio calculation.\n",
        "    z = (log_cap - mu) / sigma\n",
        "\n",
        "    # Calculate the inverse Mills ratio (phi(z) / (1 - Phi(z))).\n",
        "    # Using norm.pdf and norm.sf is numerically stable.\n",
        "    inverse_mills_ratio = norm.pdf(z) / norm.sf(z)\n",
        "\n",
        "    # Calculate the imputed value.\n",
        "    imputed_values = mu + sigma * inverse_mills_ratio\n",
        "\n",
        "    # --- Create the Final Imputed Wage Column ---\n",
        "    # Initialize the new column with the raw log wages.\n",
        "    df_with_params['log_wage_imputed'] = df_with_params['log_wage_raw']\n",
        "\n",
        "    # Update the values for censored observations with the imputed values.\n",
        "    df_with_params.loc[df_with_params['is_censored'], 'log_wage_imputed'] = imputed_values\n",
        "\n",
        "    num_imputed = df_with_params['is_censored'].sum()\n",
        "    print(f\"[{task_name}] Imputed {num_imputed} censored wage observations.\")\n",
        "\n",
        "    return df_with_params\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def impute_censored_wages(\n",
        "    worker_year_panel: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the Tobit-style imputation of right-censored wages.\n",
        "\n",
        "    This function executes a three-step pipeline for full-time workers:\n",
        "    1. Prepares the data by creating log wages and censoring flags.\n",
        "    2. Estimates parameters (mu, sigma) of a censored normal distribution for\n",
        "       each group specified in the configuration (e.g., by gender and district).\n",
        "    3. Replaces censored wage values with the conditional expectation based on\n",
        "       the estimated model parameters.\n",
        "\n",
        "    The final imputed log wage is added as a new column 'log_wage_imputed' to\n",
        "    the input worker-year panel.\n",
        "\n",
        "    Args:\n",
        "        worker_year_panel (pd.DataFrame): The cleansed worker-year panel from Task 3.\n",
        "        master_config (Dict[str, Any]): The validated master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The worker-year panel with the 'log_wage_imputed' column added.\n",
        "                      Uncensored and non-full-time observations will have this\n",
        "                      column populated with their original (log) wage or NaN.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(worker_year_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`worker_year_panel` must be a pandas DataFrame.\")\n",
        "    if not isinstance(master_config, dict):\n",
        "        raise TypeError(\"`master_config` must be a dictionary.\")\n",
        "\n",
        "    # Step 1: Subset to full-time workers and prepare data.\n",
        "    full_time_df = _prepare_wage_imputation_data(worker_year_panel)\n",
        "\n",
        "    # If there are no full-time workers, there's nothing to impute.\n",
        "    if full_time_df.empty:\n",
        "        print(\"Task 4: No full-time workers found. Skipping wage imputation.\")\n",
        "        worker_year_panel['log_wage_imputed'] = np.nan\n",
        "        return worker_year_panel\n",
        "\n",
        "    # Step 2: Estimate Tobit parameters for each group with fallbacks.\n",
        "    df_with_params = _estimate_tobit_parameters_by_group(full_time_df, master_config)\n",
        "\n",
        "    # Step 3: Calculate conditional expectation and create the imputed column.\n",
        "    df_imputed = _impute_censored_wages(df_with_params)\n",
        "\n",
        "    # --- Merge Imputed Wages Back into the Main Panel ---\n",
        "    # We only need the final imputed wage column.\n",
        "    final_panel = worker_year_panel.merge(\n",
        "        df_imputed[['log_wage_imputed']],\n",
        "        left_index=True,\n",
        "        right_index=True,\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    print(\"Task 4: Censored wage imputation completed successfully.\")\n",
        "    return final_panel\n"
      ],
      "metadata": {
        "id": "hmPSoOP5o2sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Build annual worker-year panel at June 30 snapshots\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Build annual worker-year panel at June 30 snapshots (Enrichment)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Helper to Construct Full Panel Grid and Handle Non-Employment\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _construct_full_panel_grid(\n",
        "    employed_panel: pd.DataFrame,\n",
        "    all_spells_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 5, Step 1\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs a complete worker-year panel grid and handles non-employment.\n",
        "\n",
        "    This function expands the panel of employed workers to a full, balanced grid\n",
        "    covering all unique workers and all study years. This process correctly\n",
        "    identifies non-employed periods as rows with missing job information.\n",
        "\n",
        "    Crucially, it also performs a lookback search for workers who were non-employed\n",
        "    in the baseline year (1990). For this cohort, it finds their last known job\n",
        "    characteristics from the 1986-1989 period and assigns this information\n",
        "    directly and efficiently to their 1990 observation in the panel.\n",
        "\n",
        "    Args:\n",
        "        employed_panel (pd.DataFrame): The worker-year panel of main jobs,\n",
        "            enriched with flags (output from Task 3).\n",
        "        all_spells_df (pd.DataFrame): The fully cleansed spell-level data\n",
        "            (from Task 3, Step 1), used to identify the universe of all workers\n",
        "            and for the non-employed lookback.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for clear error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A comprehensive worker-year panel indexed by\n",
        "                      ['Worker_ID', 'snapshot_year'], including both employed\n",
        "                      and non-employed observations, with lookback information\n",
        "                      attached for the relevant 1990 non-employed cohort.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(employed_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`employed_panel` must be a pandas DataFrame.\")\n",
        "    if not isinstance(all_spells_df, pd.DataFrame):\n",
        "        raise TypeError(\"`all_spells_df` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- 1. Create Full Panel Grid ---\n",
        "\n",
        "    # Define the universe of all workers (from the original spell data) and all years.\n",
        "    all_workers = all_spells_df.index.get_level_values('Worker_ID').unique()\n",
        "    start_year = config[\"temporal_parameters\"][\"DATA_ACQUISITION_START_YEAR\"]\n",
        "    end_year = config[\"temporal_parameters\"][\"DATA_ACQUISITION_END_YEAR\"]\n",
        "    all_years = range(start_year, end_year + 1)\n",
        "\n",
        "    # Create a MultiIndex representing every possible worker-year observation.\n",
        "    full_grid_index = pd.MultiIndex.from_product(\n",
        "        [all_workers, all_years], names=['Worker_ID', 'snapshot_year']\n",
        "    )\n",
        "\n",
        "    # Create the full panel DataFrame from this grid.\n",
        "    full_panel = pd.DataFrame(index=full_grid_index)\n",
        "\n",
        "    # --- 2. Merge Employment Data ---\n",
        "\n",
        "    # Join the employed panel onto the full grid. A left join ensures all\n",
        "    # worker-year observations are kept. Non-matches will have NaNs in job-\n",
        "    # related columns, correctly identifying them as non-employed periods.\n",
        "    full_panel = full_panel.join(employed_panel, how='left')\n",
        "\n",
        "    # Create an explicit boolean flag for employment status.\n",
        "    full_panel['is_employed'] = full_panel['Municipality_ID'].notna()\n",
        "\n",
        "    print(f\"[{task_name}] Full worker-year grid created with {len(full_panel)} observations.\")\n",
        "\n",
        "    # --- 3. Handle Non-Employed in Baseline Year (1990) ---\n",
        "\n",
        "    # Extract parameters for the lookback operation.\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "    lookback_start = config[\"sample_selection_parameters\"][\"NON_EMPLOYED_LOOKBACK_START_YEAR\"]\n",
        "    lookback_end = config[\"sample_selection_parameters\"][\"NON_EMPLOYED_LOOKBACK_END_YEAR\"]\n",
        "\n",
        "    # Identify the Worker_IDs of the target cohort: non-employed in 1990.\n",
        "    non_employed_1990_workers = full_panel.loc[\n",
        "        (full_panel.index.get_level_values('snapshot_year') == base_year) &\n",
        "        (~full_panel['is_employed'])\n",
        "    ].index.get_level_values('Worker_ID')\n",
        "\n",
        "    # Proceed only if such workers exist.\n",
        "    if not non_employed_1990_workers.empty:\n",
        "        # a. Perform the lookback search on a separate, small DataFrame.\n",
        "        # Filter all historical spells to the lookback window for this specific cohort.\n",
        "        lookback_spells = all_spells_df.loc[\n",
        "            all_spells_df.index.get_level_values('Worker_ID').isin(non_employed_1990_workers)\n",
        "        ].reset_index()\n",
        "\n",
        "        # Add a year column for filtering.\n",
        "        lookback_spells['spell_year'] = lookback_spells['Start_Date'].dt.year\n",
        "\n",
        "        # Apply the lookback window filter.\n",
        "        lookback_spells = lookback_spells[\n",
        "            (lookback_spells['spell_year'] >= lookback_start) &\n",
        "            (lookback_spells['spell_year'] <= lookback_end)\n",
        "        ]\n",
        "\n",
        "        if not lookback_spells.empty:\n",
        "            # b. Find the last job for each worker in the lookback period.\n",
        "            # Sort by date to ensure 'last' is the most recent spell.\n",
        "            lookback_spells = lookback_spells.sort_values(by=['Worker_ID', 'Start_Date'])\n",
        "\n",
        "            # Use drop_duplicates to efficiently get the last spell per worker.\n",
        "            last_jobs = lookback_spells.drop_duplicates(subset=['Worker_ID'], keep='last')\n",
        "\n",
        "            # Prepare the lookback data for assignment.\n",
        "            last_jobs = last_jobs.set_index('Worker_ID')[[\n",
        "                'Municipality_ID', 'spell_year', 'Daily_Wage_EUR'\n",
        "            ]].rename(columns={\n",
        "                'Municipality_ID': 'last_pre1990_municipality_id',\n",
        "                'spell_year': 'last_pre1990_spell_year',\n",
        "                'Daily_Wage_EUR': 'last_pre1990_daily_wage'\n",
        "            })\n",
        "\n",
        "            # c. Use direct assignment with .loc for a precise and efficient update.\n",
        "            # Create the MultiIndex that targets the exact rows to be updated.\n",
        "            target_idx = pd.MultiIndex.from_product(\n",
        "                [last_jobs.index, [base_year]], names=['Worker_ID', 'snapshot_year']\n",
        "            )\n",
        "\n",
        "            # Align the data to be assigned with the target index order.\n",
        "            values_to_assign = last_jobs.reindex(target_idx.get_level_values('Worker_ID'))\n",
        "\n",
        "            # Assign the values from the lookback directly to the target rows.\n",
        "            for col in values_to_assign.columns:\n",
        "                full_panel.loc[target_idx, col] = values_to_assign[col].values\n",
        "\n",
        "            print(f\"[{task_name}] Lookback job history attached for {len(last_jobs)} non-employed 1990 workers.\")\n",
        "\n",
        "    # Return the fully constructed and enriched panel.\n",
        "    return full_panel\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Steps 2 & 3: Helper to Add Econometric Variables and Group Indicators\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _add_econometric_variables(\n",
        "    full_panel: pd.DataFrame,\n",
        "    task_mapping_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 5, Steps 2 & 3\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds all remaining analysis variables to the full worker-year panel.\n",
        "\n",
        "    This includes FTE weights, age controls, task assignments, nationality labels,\n",
        "    and categorical groups for pseudo-panel analysis.\n",
        "\n",
        "    Args:\n",
        "        full_panel (pd.DataFrame): The comprehensive panel from the previous step.\n",
        "        task_mapping_df (pd.DataFrame): The validated task mapping artifact.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The fully enriched, analysis-ready panel.\n",
        "    \"\"\"\n",
        "    panel = full_panel.copy()\n",
        "\n",
        "    # --- Step 2: Construct Age Controls and Task Assignments ---\n",
        "    # Compute squared age for wage regressions.\n",
        "    panel['age_sq'] = panel['age'] ** 2\n",
        "\n",
        "    # Merge task information (Routine/Abstract label and intensity).\n",
        "    # This will add NaNs for non-employed observations, which is correct.\n",
        "    panel = panel.merge(\n",
        "        task_mapping_df,\n",
        "        left_on='Occupation_Code',\n",
        "        right_on='Occupation_Code_3digit',\n",
        "        how='left'\n",
        "    ).drop(columns=['Occupation_Code_3digit'])\n",
        "\n",
        "    # --- Step 3: Construct FTE Weights, Nationality, and Group Indicators ---\n",
        "    # Map employment type to Full-Time Equivalent (FTE) weights.\n",
        "    fte_map = config[\"algorithm_config_parameters\"][\"PART_TIME_EQUIVALENCY_WEIGHTS\"]\n",
        "    panel['fte_weight'] = panel['Employment_Type_Code'].map(fte_map).fillna(0) # Non-employed get 0.\n",
        "\n",
        "    # Map nationality code to labels and create boolean flags.\n",
        "    nat_map = config[\"variable_coding_maps\"][\"NATIONALITY_MAP\"]\n",
        "    panel['nationality'] = panel['Nationality_Code'].map(nat_map)\n",
        "    panel['is_native'] = (panel['nationality'] == 'German')\n",
        "    panel['is_czech'] = (panel['nationality'] == 'Czech')\n",
        "\n",
        "    # Create categorical groups for pseudo-panel analysis.\n",
        "    # Age groups.\n",
        "    age_bins = [-np.inf, 29, 49, np.inf]\n",
        "    age_labels = ['<30', '30-50', '>50']\n",
        "    panel['age_group'] = pd.cut(panel['age'], bins=age_bins, labels=age_labels, right=True)\n",
        "\n",
        "    # Education groups.\n",
        "    edu_map = {1: \"No vocational/apprenticeship\", 2: \"Vocational/apprenticeship\",\n",
        "               3: \"University\", 4: \"University\"} # Grouping 3 and 4\n",
        "    panel['education_group'] = panel['Education_Level_Code'].map(edu_map)\n",
        "\n",
        "    # Gender groups.\n",
        "    gender_map = config[\"variable_coding_maps\"][\"GENDER_MAP\"]\n",
        "    panel['gender_group'] = panel['Gender_Code'].map(gender_map)\n",
        "\n",
        "    print(f\"[{task_name}] All econometric variables and group indicators added.\")\n",
        "    return panel\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def build_analysis_panel(\n",
        "    analysis_panel_employed: pd.DataFrame,\n",
        "    all_spells_cleansed: pd.DataFrame,\n",
        "    validated_artifacts: Dict[str, pd.DataFrame],\n",
        "    master_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of the final, fully enriched analysis panel.\n",
        "\n",
        "    This function takes the panel of employed workers (from Task 3) and expands\n",
        "    it to include non-employed observations. It then enriches this complete\n",
        "    panel with all variables required for the study's econometric analyses,\n",
        "    including FTE weights, age controls, task assignments, nationality flags,\n",
        "    and pseudo-panel group identifiers.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel_employed (pd.DataFrame): The filtered worker-year panel\n",
        "            of main jobs for employed individuals, output from Task 3.\n",
        "        all_spells_cleansed (pd.DataFrame): The full, cleansed spell-level data\n",
        "            from Task 3, Step 1, used for identifying all workers and for the\n",
        "            non-employed lookback.\n",
        "        validated_artifacts (Dict[str, pd.DataFrame]): A dictionary containing\n",
        "            the validated auxiliary data, including the 'task_mapping' DataFrame.\n",
        "        master_config (Dict[str, Any]): The validated master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The definitive, fully specified worker-year panel dataset,\n",
        "                      ready for all subsequent aggregation and estimation tasks.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(analysis_panel_employed, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel_employed` must be a pandas DataFrame.\")\n",
        "    if not isinstance(all_spells_cleansed, pd.DataFrame):\n",
        "        raise TypeError(\"`all_spells_cleansed` must be a pandas DataFrame.\")\n",
        "    if 'task_mapping' not in validated_artifacts:\n",
        "        raise KeyError(\"Validated 'task_mapping' artifact not found in `validated_artifacts`.\")\n",
        "\n",
        "    # Step 1: Construct the full panel grid, including non-employed observations.\n",
        "    full_panel = _construct_full_panel_grid(\n",
        "        employed_panel=analysis_panel_employed,\n",
        "        all_spells_df=all_spells_cleansed,\n",
        "        config=master_config\n",
        "    )\n",
        "\n",
        "    # Step 2 & 3: Add all remaining econometric variables and group indicators.\n",
        "    final_panel = _add_econometric_variables(\n",
        "        full_panel=full_panel,\n",
        "        task_mapping_df=validated_artifacts['task_mapping'],\n",
        "        config=master_config\n",
        "    )\n",
        "\n",
        "    # Merge the imputed log wage from the employed panel.\n",
        "    # The imputed wage only exists for employed, full-time workers.\n",
        "    if 'log_wage_imputed' in analysis_panel_employed.columns:\n",
        "        final_panel = final_panel.merge(\n",
        "            analysis_panel_employed[['log_wage_imputed']],\n",
        "            left_index=True,\n",
        "            right_index=True,\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "    print(\"Task 5: Final analysis panel built successfully.\")\n",
        "    return final_panel\n"
      ],
      "metadata": {
        "id": "Vlrl8xpHp1Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Aggregate to municipality-year level for regional analyses\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Aggregate to municipality-year level for regional analyses\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 1: Helper to Compute Native and Total FTE Employment Stocks\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _aggregate_employment_stocks(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 6, Step 1\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates worker-level data to compute municipality-year employment stocks.\n",
        "\n",
        "    This function calculates Full-Time Equivalent (FTE) employment for native\n",
        "    workers and all workers combined for each municipality and year. It also\n",
        "    extracts the baseline native employment to be used as regression weights.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel from Task 5.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A municipality-year panel with employment stock columns.\n",
        "    \"\"\"\n",
        "    # Filter to only employed observations for aggregation.\n",
        "    employed_df = analysis_panel[analysis_panel['is_employed']].copy()\n",
        "\n",
        "    # Define aggregation functions.\n",
        "    # E_native_rt: Sum of FTE weights for native workers.\n",
        "    # Total_rt: Sum of FTE weights for all workers.\n",
        "    # N_native_fulltime: Count of full-time native workers for wage averaging.\n",
        "    aggregations = {\n",
        "        'fte_weight': [\n",
        "            ('E_native_rt', lambda w: w[employed_df.loc[w.index, 'is_native']].sum()),\n",
        "            ('Total_rt', 'sum')\n",
        "        ],\n",
        "        'is_full_time': [\n",
        "            ('N_native_fulltime', lambda ft: ft[employed_df.loc[ft.index, 'is_native']].sum())\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Perform the aggregation at the municipality-year level.\n",
        "    regional_panel = employed_df.groupby(['Municipality_ID', 'snapshot_year']).agg(aggregations)\n",
        "\n",
        "    # Flatten the MultiIndex columns created by .agg().\n",
        "    regional_panel.columns = ['_'.join(col).strip() for col in regional_panel.columns.values]\n",
        "    regional_panel = regional_panel.rename(columns={\n",
        "        'fte_weight_E_native_rt': 'native_fte_employment',\n",
        "        'fte_weight_Total_rt': 'total_fte_employment',\n",
        "        'is_full_time_N_native_fulltime': 'native_fulltime_headcount'\n",
        "    })\n",
        "\n",
        "    # --- Ensure a balanced panel structure ---\n",
        "    # Create a full grid of all municipalities and years.\n",
        "    all_municipalities = analysis_panel['Municipality_ID'].dropna().unique()\n",
        "    start_year = config[\"temporal_parameters\"][\"DATA_ACQUISITION_START_YEAR\"]\n",
        "    end_year = config[\"temporal_parameters\"][\"DATA_ACQUISITION_END_YEAR\"]\n",
        "    all_years = range(start_year, end_year + 1)\n",
        "    full_grid_index = pd.MultiIndex.from_product(\n",
        "        [all_municipalities, all_years], names=['Municipality_ID', 'snapshot_year']\n",
        "    )\n",
        "\n",
        "    # Reindex the panel to the full grid, filling missing employment with 0.\n",
        "    regional_panel = regional_panel.reindex(full_grid_index, fill_value=0)\n",
        "\n",
        "    # --- Extract Baseline Weights ---\n",
        "    # The weight for all municipality-level regressions is the native FTE\n",
        "    # employment in the baseline year.\n",
        "    baseline_year = config[\"temporal_parameters\"][\"BASELINE_WEIGHT_YEAR\"]\n",
        "    weight_col_name = config[\"algorithm_config_parameters\"][\"WEIGHT_COLUMN_MUNICIPALITY\"]\n",
        "\n",
        "    baseline_weights = regional_panel.loc[\n",
        "        (slice(None), baseline_year), 'native_fte_employment'\n",
        "    ].rename(weight_col_name).reset_index(level='snapshot_year', drop=True)\n",
        "\n",
        "    # Merge the baseline weights into the panel so each municipality-year has it.\n",
        "    regional_panel = regional_panel.merge(\n",
        "        baseline_weights, on='Municipality_ID', how='left'\n",
        "    )\n",
        "    regional_panel[weight_col_name] = regional_panel[weight_col_name].fillna(0)\n",
        "\n",
        "    print(f\"[{task_name}] Aggregated employment stocks for {len(all_municipalities)} municipalities.\")\n",
        "    return regional_panel\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 2: Helper to Compute Mean Log Wages for Full-Time Natives\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _aggregate_regional_wages(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    regional_panel: pd.DataFrame,\n",
        "    task_name: str = \"Task 6, Step 2\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates worker-level wages to compute municipality-year mean log wages.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        regional_panel (pd.DataFrame): The municipality-year panel with employment stocks.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The regional panel augmented with mean log wage columns.\n",
        "    \"\"\"\n",
        "    # Filter to the sample relevant for wage analysis.\n",
        "    wage_sample = analysis_panel[\n",
        "        analysis_panel['is_native'] & analysis_panel['is_full_time']\n",
        "    ].copy()\n",
        "\n",
        "    # Calculate the mean log wage for each municipality-year.\n",
        "    mean_log_wages = wage_sample.groupby(['Municipality_ID', 'snapshot_year'])['log_wage_imputed'].mean()\n",
        "    mean_log_wages.name = 'mean_log_wage'\n",
        "\n",
        "    # Merge the mean log wages into the main regional panel.\n",
        "    # Municipalities with no full-time natives in a year will have NaN, which is correct.\n",
        "    regional_panel_out = regional_panel.merge(\n",
        "        mean_log_wages, on=['Municipality_ID', 'snapshot_year'], how='left'\n",
        "    )\n",
        "\n",
        "    print(f\"[{task_name}] Aggregated mean log wages for full-time natives.\")\n",
        "    return regional_panel_out\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Helper to Compute National Mean Log Wages by Year\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_national_mean_wages(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    task_name: str = \"Task 6, Step 3\"\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the national mean log wage for full-time natives for each year.\n",
        "\n",
        "    This is a required input for the counterfactual wage imputation for\n",
        "    non-employed workers in Task 15.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series of national mean log wages, indexed by year.\n",
        "    \"\"\"\n",
        "    # Filter to the sample relevant for wage analysis.\n",
        "    wage_sample = analysis_panel[\n",
        "        analysis_panel['is_native'] & analysis_panel['is_full_time']\n",
        "    ].copy()\n",
        "\n",
        "    # Group by year only and calculate the mean log wage.\n",
        "    national_wages = wage_sample.groupby('snapshot_year')['log_wage_imputed'].mean()\n",
        "    national_wages.name = 'national_mean_log_wage'\n",
        "\n",
        "    print(f\"[{task_name}] Computed national mean log wages for years {national_wages.index.min()}-{national_wages.index.max()}.\")\n",
        "    return national_wages\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_to_regional_panel(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Orchestrates the aggregation of the worker panel to the regional (municipality-year) level.\n",
        "\n",
        "    This function performs three key aggregations:\n",
        "    1. Computes native and total Full-Time Equivalent (FTE) employment stocks.\n",
        "    2. Computes the mean imputed log wage for full-time native workers.\n",
        "    3. Computes the national average log wage for each year, needed for later tasks.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel from Task 5.\n",
        "        master_config (Dict[str, Any]): The validated master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.Series]:\n",
        "        - A DataFrame indexed by ('Municipality_ID', 'snapshot_year') containing\n",
        "          all aggregated regional variables.\n",
        "        - A Series indexed by 'snapshot_year' containing the national mean log wage.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(analysis_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel` must be a pandas DataFrame.\")\n",
        "    if not isinstance(master_config, dict):\n",
        "        raise TypeError(\"`master_config` must be a dictionary.\")\n",
        "\n",
        "    # Step 1: Compute native and total FTE employment stocks and baseline weights.\n",
        "    regional_panel = _aggregate_employment_stocks(analysis_panel, master_config)\n",
        "\n",
        "    # Step 2: Compute and merge mean log wages for full-time natives.\n",
        "    regional_panel_with_wages = _aggregate_regional_wages(analysis_panel, regional_panel)\n",
        "\n",
        "    # Step 3: Compute the national mean log wage series as a separate artifact.\n",
        "    national_wage_series = _compute_national_mean_wages(analysis_panel)\n",
        "\n",
        "    print(\"Task 6: Aggregation to municipality-year panel completed successfully.\")\n",
        "    return regional_panel_with_wages, national_wage_series\n"
      ],
      "metadata": {
        "id": "fPT95gixqlFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Construct the immigration shock ΔI_r per municipality\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Construct the immigration shock ΔI_r per municipality\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 1: Helper to Compute Czech FTE Counts by Year\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_czech_fte_by_year(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    task_name: str = \"Task 7, Step 1\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the Full-Time Equivalent (FTE) count of Czech workers by municipality and year.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel from Task 5.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by 'Municipality_ID' with columns\n",
        "                      for Czech FTE counts in each relevant year (e.g., 'fte_1990').\n",
        "    \"\"\"\n",
        "    # Filter to Czech workers who are employed.\n",
        "    czech_workers = analysis_panel[analysis_panel['is_czech'] & analysis_panel['is_employed']].copy()\n",
        "\n",
        "    # If no Czech workers are found, return an empty DataFrame with the expected structure.\n",
        "    if czech_workers.empty:\n",
        "        print(f\"[{task_name}] No Czech workers found in the panel. Returning empty FTE counts.\")\n",
        "        all_municipalities = analysis_panel['Municipality_ID'].dropna().unique()\n",
        "        return pd.DataFrame(index=all_municipalities)\n",
        "\n",
        "    # Aggregate FTE weights by municipality and year.\n",
        "    czech_fte = czech_workers.groupby(['Municipality_ID', 'snapshot_year'])['fte_weight'].sum()\n",
        "\n",
        "    # Pivot the data to a wide format: municipalities as rows, years as columns.\n",
        "    czech_fte_wide = czech_fte.unstack(level='snapshot_year').add_prefix('fte_')\n",
        "\n",
        "    # Ensure all municipalities from the original panel are present, filling missing with 0.\n",
        "    all_municipalities = analysis_panel['Municipality_ID'].dropna().unique()\n",
        "    czech_fte_wide = czech_fte_wide.reindex(all_municipalities, fill_value=0)\n",
        "\n",
        "    print(f\"[{task_name}] Computed Czech FTE counts for {len(czech_fte_wide)} municipalities.\")\n",
        "    return czech_fte_wide\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Steps 2 & 3: Helper to Compute the Immigration Shock Variables\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_immigration_shocks(\n",
        "    czech_fte_df: pd.DataFrame,\n",
        "    regional_panel: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 7, Steps 2 & 3\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the main and event-year specific immigration shock variables.\n",
        "\n",
        "    Args:\n",
        "        czech_fte_df (pd.DataFrame): Wide-format DataFrame of Czech FTEs by year.\n",
        "        regional_panel (pd.DataFrame): The municipality-year panel from Task 6.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by 'Municipality_ID' with columns\n",
        "                      'shock_main' and 'shock_1991'.\n",
        "    \"\"\"\n",
        "    # --- Step 2: Extract Baseline Denominator ---\n",
        "    # The denominator is the total FTE employment (all nationalities) in the baseline year.\n",
        "    denom_year = config[\"shock_and_scaling_parameters\"][\"SHOCK_DENOMINATOR_YEAR\"]\n",
        "\n",
        "    # Extract the denominator series from the regional panel.\n",
        "    denominator = regional_panel.loc[\n",
        "        (slice(None), denom_year), 'total_fte_employment'\n",
        "    ].droplevel('snapshot_year')\n",
        "\n",
        "    # Validate the denominator to prevent division by zero.\n",
        "    if (denominator <= 0).any():\n",
        "        zero_denom_munis = denominator[denominator <= 0].index.tolist()\n",
        "        raise ValueError(\n",
        "            f\"[{task_name}] Found {len(zero_denom_munis)} municipalities with zero or \"\n",
        "            f\"negative total employment in baseline year {denom_year}. \"\n",
        "            f\"Cannot compute shock. Example: {zero_denom_munis[:5]}\"\n",
        "        )\n",
        "\n",
        "    # --- Step 3: Compute Shock Regressors ---\n",
        "    # Extract start and end years for shock definitions from config.\n",
        "    main_shock_years = config[\"temporal_parameters\"][\"EVENT_STUDY_SHOCK_RULES\"][\"1992_to_1995\"]\n",
        "    y_start_main, y_end_main = main_shock_years['start_year'], main_shock_years['end_year']\n",
        "\n",
        "    event_1991_years = config[\"temporal_parameters\"][\"EVENT_STUDY_SHOCK_RULES\"][\"1991\"]\n",
        "    y_start_1991, y_end_1991 = event_1991_years['start_year'], event_1991_years['end_year']\n",
        "\n",
        "    # Ensure required year columns exist in the Czech FTE data.\n",
        "    required_cols = {f'fte_{y}' for y in [y_start_main, y_end_main, y_start_1991, y_end_1991]}\n",
        "    if not required_cols.issubset(czech_fte_df.columns):\n",
        "        missing = required_cols - set(czech_fte_df.columns)\n",
        "        raise KeyError(f\"[{task_name}] Missing required Czech FTE columns: {missing}\")\n",
        "\n",
        "    # Compute the numerator for the main shock (1990-1992).\n",
        "    # ΔCzech_r = Czech^92_r - Czech^90_r\n",
        "    numerator_main = czech_fte_df[f'fte_{y_end_main}'] - czech_fte_df[f'fte_{y_start_main}']\n",
        "\n",
        "    # Compute the main shock variable.\n",
        "    # ΔI_r = (Czech^92_r - Czech^90_r) / Total^90_r\n",
        "    shock_main = numerator_main / denominator\n",
        "\n",
        "    # Compute the numerator for the 1991 event-year shock (1990-1991).\n",
        "    # ΔCzech_r^1991 = Czech^91_r - Czech^90_r\n",
        "    numerator_1991 = czech_fte_df[f'fte_{y_end_1991}'] - czech_fte_df[f'fte_{y_start_1991}']\n",
        "\n",
        "    # Compute the 1991 event-year shock variable.\n",
        "    # ΔI^1991_r = (Czech^91_r - Czech^90_r) / Total^90_r\n",
        "    shock_1991 = numerator_1991 / denominator\n",
        "\n",
        "    # Combine into a single DataFrame.\n",
        "    shock_df = pd.DataFrame({\n",
        "        'shock_main': shock_main,\n",
        "        'shock_1991': shock_1991\n",
        "    })\n",
        "\n",
        "    # Fill any NaNs that might result from division (though validated against) with 0.\n",
        "    shock_df = shock_df.fillna(0)\n",
        "\n",
        "    print(f\"[{task_name}] Immigration shock variables computed successfully.\")\n",
        "    return shock_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def construct_immigration_shock(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    regional_panel: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of the immigration shock variables.\n",
        "\n",
        "    This function calculates the change in Czech Full-Time Equivalent (FTE)\n",
        "    employment as a share of total baseline employment for each municipality.\n",
        "    It produces two versions of the shock variable as specified in the paper's\n",
        "    event-study design:\n",
        "    1. `shock_main`: Based on the 1990-1992 change, used for post-1991 years.\n",
        "    2. `shock_1991`: Based on the 1990-1991 change, used for the 1991 event year.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel from Task 5.\n",
        "        regional_panel (pd.DataFrame): The municipality-year panel from Task 6.\n",
        "        master_config (Dict[str, Any]): The validated master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by 'Municipality_ID' containing the\n",
        "                      'shock_main' and 'shock_1991' variables.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(analysis_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel` must be a pandas DataFrame.\")\n",
        "    if not isinstance(regional_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`regional_panel` must be a pandas DataFrame.\")\n",
        "    if not isinstance(master_config, dict):\n",
        "        raise TypeError(\"`master_config` must be a dictionary.\")\n",
        "\n",
        "    # Step 1: Compute Czech FTE counts for all relevant years in a wide format.\n",
        "    czech_fte_df = _compute_czech_fte_by_year(analysis_panel)\n",
        "\n",
        "    # Steps 2 & 3: Extract the denominator and compute the final shock variables.\n",
        "    shock_df = _compute_immigration_shocks(\n",
        "        czech_fte_df=czech_fte_df,\n",
        "        regional_panel=regional_panel,\n",
        "        config=master_config\n",
        "    )\n",
        "\n",
        "    print(\"Task 7: Construction of immigration shock variables completed successfully.\")\n",
        "    return shock_df\n"
      ],
      "metadata": {
        "id": "wTule8x7rLNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Construct instrumental variables (distance to border)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Construct instrumental variables (distance to border)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 1: Helper to Harmonize Coordinate Systems and Units\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _prepare_coordinates(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    border_crossings_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 8, Step 1\"\n",
        ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray, float]:\n",
        "    \"\"\"\n",
        "    Prepares and harmonizes municipality and border crossing coordinates.\n",
        "\n",
        "    Extracts unique municipality coordinates, validates CRS, and transforms\n",
        "    coordinates to the required system (e.g., WGS84 for great-circle) based\n",
        "    on the configuration.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        border_crossings_df (pd.DataFrame): Validated border crossing data.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "        - pd.DataFrame: Unique municipalities with their original coordinates.\n",
        "        - np.ndarray: N x 2 array of harmonized municipality coordinates.\n",
        "        - np.ndarray: M x 2 array of harmonized border crossing coordinates.\n",
        "        - float: Scaling factor to convert distance results to kilometers.\n",
        "    \"\"\"\n",
        "    # Extract unique municipality coordinates to avoid redundant calculations.\n",
        "    muni_coords = analysis_panel[['Municipality_ID', 'Workplace_Coord_X_UTM', 'Workplace_Coord_Y_UTM']]\n",
        "    muni_coords = muni_coords.drop_duplicates(subset=['Municipality_ID']).set_index('Municipality_ID')\n",
        "\n",
        "    # Extract coordinate arrays.\n",
        "    muni_coords_arr = muni_coords[['Workplace_Coord_X_UTM', 'Workplace_Coord_Y_UTM']].to_numpy()\n",
        "    border_coords_arr = border_crossings_df[['Coord_X_UTM', 'Coord_Y_UTM']].to_numpy()\n",
        "\n",
        "    # Get geospatial parameters from config.\n",
        "    calc_method = config[\"algorithm_config_parameters\"][\"DISTANCE_CALCULATION_METHOD\"]\n",
        "    units = config[\"algorithm_config_parameters\"][\"DISTANCE_UNITS\"]\n",
        "    source_crs_str = config[\"geographic_policy_parameters\"][\"CRS_UTM_EPSG\"]\n",
        "\n",
        "    # Initialize scaling factor (default is no scaling).\n",
        "    scaling_factor = 1.0\n",
        "\n",
        "    if calc_method == \"great_circle\":\n",
        "        # For Haversine distance, coordinates must be in latitude/longitude (WGS84).\n",
        "        print(f\"[{task_name}] Transforming coordinates from {source_crs_str} to WGS84 (EPSG:4326) for great-circle distance.\")\n",
        "        try:\n",
        "            source_crs = CRS(source_crs_str)\n",
        "            target_crs = CRS(\"EPSG:4326\") # WGS84\n",
        "            transformer = Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
        "\n",
        "            # Transform coordinates. Note: pyproj expects (x, y) order.\n",
        "            muni_lon, muni_lat = transformer.transform(muni_coords_arr[:, 0], muni_coords_arr[:, 1])\n",
        "            border_lon, border_lat = transformer.transform(border_coords_arr[:, 0], border_coords_arr[:, 1])\n",
        "\n",
        "            # Output arrays should be (lat, lon) for many Haversine implementations, but we will be consistent.\n",
        "            # Our Haversine implementation will expect (lon, lat) in radians.\n",
        "            muni_coords_harmonized = np.radians(np.column_stack([muni_lon, muni_lat]))\n",
        "            border_coords_harmonized = np.radians(np.column_stack([border_lon, border_lat]))\n",
        "\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"[{task_name}] Failed to transform coordinates. Error: {e}\")\n",
        "\n",
        "    elif calc_method == \"euclidean\":\n",
        "        # For Euclidean distance, assume coordinates are in a projected CRS (e.g., UTM).\n",
        "        print(f\"[{task_name}] Using Euclidean distance on projected coordinates from {source_crs_str}.\")\n",
        "        muni_coords_harmonized = muni_coords_arr\n",
        "        border_coords_harmonized = border_coords_arr\n",
        "        # If the source CRS is in meters, we need to scale the result to km.\n",
        "        if units == \"km\":\n",
        "            scaling_factor = 0.001\n",
        "    else:\n",
        "        raise ValueError(f\"[{task_name}] Invalid DISTANCE_CALCULATION_METHOD: '{calc_method}'\")\n",
        "\n",
        "    return muni_coords.reset_index(), muni_coords_harmonized, border_coords_harmonized, scaling_factor\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 2: Helper to Compute Minimum Distance to Border\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_min_distances(\n",
        "    muni_coords: np.ndarray,\n",
        "    border_coords: np.ndarray,\n",
        "    method: str,\n",
        "    scaling_factor: float,\n",
        "    task_name: str = \"Task 8, Step 2\"\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the minimum distance from each municipality to any border crossing.\n",
        "\n",
        "    Args:\n",
        "        muni_coords (np.ndarray): Harmonized municipality coordinates.\n",
        "        border_coords (np.ndarray): Harmonized border crossing coordinates.\n",
        "        method (str): The calculation method ('euclidean' or 'great_circle').\n",
        "        scaling_factor (float): Factor to convert distance to desired units (km).\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 1D array of minimum distances for each municipality.\n",
        "    \"\"\"\n",
        "    if method == \"euclidean\":\n",
        "        # Use scipy's highly optimized cdist for Euclidean distance.\n",
        "        dist_matrix = cdist(muni_coords, border_coords, metric='euclidean')\n",
        "\n",
        "    elif method == \"great_circle\":\n",
        "        # Implement vectorized Haversine formula for great-circle distance.\n",
        "        # Coords are expected in radians: (lon, lat).\n",
        "        lon1, lat1 = muni_coords[:, 0][:, np.newaxis], muni_coords[:, 1][:, np.newaxis]\n",
        "        lon2, lat2 = border_coords[:, 0], border_coords[:, 1]\n",
        "\n",
        "        dlon = lon2 - lon1\n",
        "        dlat = lat2 - lat1\n",
        "\n",
        "        a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
        "        c = 2 * np.arcsin(np.sqrt(a))\n",
        "\n",
        "        # Earth radius in kilometers.\n",
        "        R = 6371.0\n",
        "        dist_matrix = R * c\n",
        "\n",
        "    # Find the minimum distance for each municipality (row-wise minimum).\n",
        "    min_distances = np.min(dist_matrix, axis=1)\n",
        "\n",
        "    # Apply the scaling factor (e.g., for meters to km).\n",
        "    min_distances_scaled = min_distances * scaling_factor\n",
        "\n",
        "    print(f\"[{task_name}] Minimum distances computed for {len(min_distances_scaled)} municipalities.\")\n",
        "    return min_distances_scaled\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def construct_instrumental_variables(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    validated_artifacts: Dict[str, Any],\n",
        "    master_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of the instrumental variables.\n",
        "\n",
        "    This function calculates the distance from each municipality to the nearest\n",
        "    border crossing and its square. These variables serve as instruments for\n",
        "    the immigration shock in the 2SLS regressions.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel from Task 5.\n",
        "        validated_artifacts (Dict[str, Any]): Dictionary of validated auxiliary data,\n",
        "                                              including the 'border_crossings' DataFrame.\n",
        "        master_config (Dict[str, Any]): The validated master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by 'Municipality_ID' containing the\n",
        "                      instrumental variables 'distance_to_border' and\n",
        "                      'distance_to_border_sq', and optional interactions.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if 'border_crossings' not in validated_artifacts:\n",
        "        raise KeyError(\"Validated 'border_crossings' artifact not found in `validated_artifacts`.\")\n",
        "\n",
        "    # Step 1: Harmonize coordinate systems.\n",
        "    unique_munis, muni_coords_h, border_coords_h, scale = _prepare_coordinates(\n",
        "        analysis_panel=analysis_panel,\n",
        "        border_crossings_df=validated_artifacts['border_crossings'],\n",
        "        config=master_config\n",
        "    )\n",
        "\n",
        "    # Step 2: Compute the minimum distance for each municipality.\n",
        "    min_distances = _compute_min_distances(\n",
        "        muni_coords=muni_coords_h,\n",
        "        border_coords=border_coords_h,\n",
        "        method=master_config[\"algorithm_config_parameters\"][\"DISTANCE_CALCULATION_METHOD\"],\n",
        "        scaling_factor=scale\n",
        "    )\n",
        "\n",
        "    # Step 3: Assemble the final instruments DataFrame.\n",
        "    instruments_df = pd.DataFrame({\n",
        "        'Municipality_ID': unique_munis['Municipality_ID'],\n",
        "        'distance_to_border': min_distances\n",
        "    }).set_index('Municipality_ID')\n",
        "\n",
        "    # Construct the squared distance term.\n",
        "    instruments_df['distance_to_border_sq'] = instruments_df['distance_to_border'] ** 2\n",
        "\n",
        "    # Construct interaction terms if specified in the config.\n",
        "    if master_config[\"algorithm_config_parameters\"][\"IV_INTERACTION_WITH_BORDER_DUMMY\"]:\n",
        "        # Get the time-invariant 'Is_Border_Region' flag for each municipality.\n",
        "        border_region_flag = analysis_panel.groupby('Municipality_ID')['Is_Border_Region'].first()\n",
        "        instruments_df = instruments_df.merge(border_region_flag, on='Municipality_ID', how='left')\n",
        "\n",
        "        # Create interaction terms.\n",
        "        instruments_df['dist_x_border'] = instruments_df['distance_to_border'] * instruments_df['Is_Border_Region']\n",
        "        instruments_df['dist_sq_x_border'] = instruments_df['distance_to_border_sq'] * instruments_df['Is_Border_Region']\n",
        "        print(\"Task 8, Step 3: Interaction-term instruments constructed.\")\n",
        "\n",
        "    print(\"Task 8: Construction of instrumental variables completed successfully.\")\n",
        "    return instruments_df\n"
      ],
      "metadata": {
        "id": "f-1atiYBrx5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Prepare event-study datasets (outcomes, shock, instruments, weights, clusters)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Prepare event-study datasets\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 1: Helper to Define Outcome Variables Relative to Base Year\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_event_study_outcomes(\n",
        "    regional_panel: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 9, Step 1\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes event-study outcome variables relative to the base year (1990).\n",
        "\n",
        "    This function calculates the percentage change in native employment and the\n",
        "    absolute change in mean log wages for each municipality-year relative to\n",
        "    its 1990 baseline value.\n",
        "\n",
        "    Args:\n",
        "        regional_panel (pd.DataFrame): The aggregated municipality-year panel from Task 6.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The regional panel augmented with 'emp_outcome' and 'wage_outcome'.\n",
        "    \"\"\"\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "\n",
        "    # Create a DataFrame containing only the baseline year values.\n",
        "    baseline_values = regional_panel.loc[(slice(None), base_year), :].copy()\n",
        "    baseline_values = baseline_values.reset_index(level='snapshot_year', drop=True)\n",
        "    baseline_values = baseline_values.rename(columns={\n",
        "        'native_fte_employment': 'native_fte_employment_base',\n",
        "        'mean_log_wage': 'mean_log_wage_base'\n",
        "    })\n",
        "\n",
        "    # Merge baseline values back onto the full regional panel.\n",
        "    panel_with_base = regional_panel.merge(\n",
        "        baseline_values[['native_fte_employment_base', 'mean_log_wage_base']],\n",
        "        on='Municipality_ID',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # --- Compute Employment Outcome ---\n",
        "    # Outcome: (E_rt - E_r,1990) / E_r,1990\n",
        "    # Use np.divide for safe division, which handles division by zero.\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        panel_with_base['emp_outcome'] = np.divide(\n",
        "            panel_with_base['native_fte_employment'] - panel_with_base['native_fte_employment_base'],\n",
        "            panel_with_base['native_fte_employment_base']\n",
        "        )\n",
        "    # Replace inf/-inf resulting from 0/0 or x/0 with NaN.\n",
        "    panel_with_base['emp_outcome'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # --- Compute Wage Outcome ---\n",
        "    # Outcome: log_w_bar_rt - log_w_bar_r,1990\n",
        "    panel_with_base['wage_outcome'] = panel_with_base['mean_log_wage'] - panel_with_base['mean_log_wage_base']\n",
        "\n",
        "    # Set outcomes for the base year to exactly 0.\n",
        "    panel_with_base.loc[panel_with_base.index.get_level_values('snapshot_year') == base_year, ['emp_outcome', 'wage_outcome']] = 0.0\n",
        "\n",
        "    print(f\"[{task_name}] Event-study outcome variables computed relative to {base_year}.\")\n",
        "    return panel_with_base\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 2: Helper to Assign Shock Regressors and Instruments\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _assign_shocks_and_instruments(\n",
        "    panel_with_outcomes: pd.DataFrame,\n",
        "    shock_df: pd.DataFrame,\n",
        "    instruments_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 9, Step 2\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Merges and assigns the correct shock and instrument variables for each event year.\n",
        "\n",
        "    Args:\n",
        "        panel_with_outcomes (pd.DataFrame): The regional panel with outcome variables.\n",
        "        shock_df (pd.DataFrame): DataFrame with 'shock_main' and 'shock_1991'.\n",
        "        instruments_df (pd.DataFrame): DataFrame with distance-based instruments.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The panel augmented with shock and instrument columns.\n",
        "    \"\"\"\n",
        "    # Merge shock and instrument variables onto the panel.\n",
        "    panel = panel_with_outcomes.merge(shock_df, on='Municipality_ID', how='left')\n",
        "    panel = panel.merge(instruments_df, on='Municipality_ID', how='left')\n",
        "\n",
        "    # --- Assign the Correct Event-Year Shock ---\n",
        "    # The 'shock' column will be the endogenous regressor in all 2SLS models.\n",
        "    # Its value depends on the year, per the paper's event-study design.\n",
        "    panel['shock'] = np.where(\n",
        "        panel.index.get_level_values('snapshot_year') == 1991,\n",
        "        panel['shock_1991'],  # Use 1990-1991 shock for 1991 event year.\n",
        "        panel['shock_main']   # Use 1990-1992 shock for all other years (pre and post).\n",
        "    )\n",
        "\n",
        "    print(f\"[{task_name}] Shock and instrument variables assigned.\")\n",
        "    return panel\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 3: Helper to Attach Identifiers and Apply Sample Filters\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _apply_final_sample_filters(\n",
        "    full_event_panel: pd.DataFrame,\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 9, Step 3\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Attaches cluster IDs and applies the final sample filter for the analysis.\n",
        "\n",
        "    Args:\n",
        "        full_event_panel (pd.DataFrame): The fully assembled event-study panel.\n",
        "        analysis_panel (pd.DataFrame): The worker-year panel, used to get District_ID.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The final, analysis-ready event-study DataFrame.\n",
        "    \"\"\"\n",
        "    # --- Attach Cluster Identifier ---\n",
        "    # District_ID is time-invariant for each municipality.\n",
        "    district_map = analysis_panel[['Municipality_ID', 'District_ID']].drop_duplicates().set_index('Municipality_ID')\n",
        "    panel = full_event_panel.merge(district_map, on='Municipality_ID', how='left')\n",
        "\n",
        "    # --- Apply Sample Filter ---\n",
        "    # The analysis sample includes only treated border districts and matched controls.\n",
        "    geo_params = config[\"geographic_policy_parameters\"]\n",
        "    treated_districts: Set[str] = set(geo_params[\"TREATED_DISTRICT_IDS\"])\n",
        "    control_districts: Set[str] = set(geo_params[\"MATCHED_CONTROL_DISTRICT_IDS\"])\n",
        "    analysis_districts: Set[str] = treated_districts.union(control_districts)\n",
        "\n",
        "    # Excluded districts should not be in the analysis sample.\n",
        "    excluded_districts: Set[str] = set(geo_params[\"EXCLUDED_BORDER_DISTRICT_IDS\"])\n",
        "    if not analysis_districts.isdisjoint(excluded_districts):\n",
        "        raise ValueError(f\"[{task_name}] Overlap found between analysis districts and excluded districts.\")\n",
        "\n",
        "    initial_munis = panel['Municipality_ID'].nunique()\n",
        "\n",
        "    # Filter the panel to the defined analysis sample.\n",
        "    final_panel = panel[panel['District_ID'].isin(analysis_districts)].copy()\n",
        "\n",
        "    final_munis = final_panel['Municipality_ID'].nunique()\n",
        "    print(f\"[{task_name}] Final sample filter applied. Kept {final_munis} of {initial_munis} municipalities.\")\n",
        "\n",
        "    return final_panel\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_event_study_dataset(\n",
        "    regional_panel: pd.DataFrame,\n",
        "    shock_df: pd.DataFrame,\n",
        "    instruments_df: pd.DataFrame,\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the assembly of the final event-study dataset.\n",
        "\n",
        "    This function combines regional outcomes, shock variables, instruments,\n",
        "    weights, and cluster identifiers into a single, analysis-ready DataFrame\n",
        "    in a long format (one row per municipality-year).\n",
        "\n",
        "    Args:\n",
        "        regional_panel (pd.DataFrame): Aggregated municipality-year panel from Task 6.\n",
        "        shock_df (pd.DataFrame): Immigration shock variables from Task 7.\n",
        "        instruments_df (pd.DataFrame): Instrumental variables from Task 8.\n",
        "        analysis_panel (pd.DataFrame): The main worker-year panel, used for District_ID mapping.\n",
        "        master_config (Dict[str, Any]): The validated master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The final, analysis-ready dataset for all 2SLS regressions.\n",
        "                      The DataFrame is indexed by ('Municipality_ID', 'snapshot_year').\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    required_inputs = {\n",
        "        'regional_panel': regional_panel, 'shock_df': shock_df,\n",
        "        'instruments_df': instruments_df, 'analysis_panel': analysis_panel\n",
        "    }\n",
        "    for name, df in required_inputs.items():\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            raise TypeError(f\"`{name}` must be a pandas DataFrame.\")\n",
        "    if not isinstance(master_config, dict):\n",
        "        raise TypeError(\"`master_config` must be a dictionary.\")\n",
        "\n",
        "    # Step 1: Compute outcome variables relative to the base year.\n",
        "    panel_with_outcomes = _compute_event_study_outcomes(regional_panel, master_config)\n",
        "\n",
        "    # Step 2: Assign the correct shock regressor for each year and merge instruments.\n",
        "    panel_with_ivs = _assign_shocks_and_instruments(\n",
        "        panel_with_outcomes, shock_df, instruments_df, master_config\n",
        "    )\n",
        "\n",
        "    # Step 3: Attach cluster IDs and apply the final sample filter.\n",
        "    final_event_study_df = _apply_final_sample_filters(\n",
        "        panel_with_ivs, analysis_panel, master_config\n",
        "    )\n",
        "\n",
        "    # Final check for required columns before returning.\n",
        "    required_cols = [\n",
        "        'emp_outcome', 'wage_outcome', 'shock', 'distance_to_border',\n",
        "        'distance_to_border_sq', master_config[\"algorithm_config_parameters\"][\"WEIGHT_COLUMN_MUNICIPALITY\"],\n",
        "        'District_ID'\n",
        "    ]\n",
        "    if not all(col in final_event_study_df.columns for col in required_cols):\n",
        "        raise RuntimeError(\"Final event study DataFrame is missing required columns.\")\n",
        "\n",
        "    print(\"Task 9: Preparation of event-study dataset completed successfully.\")\n",
        "    return final_event_study_df\n"
      ],
      "metadata": {
        "id": "7h-VpOd3sgqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Estimate regional employment effect (2SLS; Equation (2))\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Estimate regional employment effect (2SLS; Equation (2))\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Step 1: Helper to Estimate Weighted First Stage\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _estimate_first_stage(\n",
        "    data: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 10, Step 1\"\n",
        ") -> Tuple[Any, pd.Series]:\n",
        "    \"\"\"\n",
        "    Estimates the weighted first-stage regression of the 2SLS model.\n",
        "\n",
        "    Regresses the endogenous shock variable on the instrumental variables,\n",
        "    using baseline native employment as weights. It also performs a weak\n",
        "    instrument test (F-statistic).\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The analysis-ready data for a single event year.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Any, pd.Series]:\n",
        "        - The fitted statsmodels results object for the first stage.\n",
        "        - A Series of predicted values for the endogenous variable.\n",
        "    \"\"\"\n",
        "    # Define the formula for the first-stage regression.\n",
        "    # shock ~ 1 + distance_to_border + distance_to_border_sq\n",
        "    iv_names = list(config[\"algorithm_config_parameters\"][\"INSTRUMENT_SPECIFICATION\"].keys())\n",
        "    formula = f\"shock ~ 1 + {' + '.join(iv_names)}\"\n",
        "\n",
        "    # Get the column name for weights.\n",
        "    weight_col = config[\"algorithm_config_parameters\"][\"WEIGHT_COLUMN_MUNICIPALITY\"]\n",
        "\n",
        "    # Estimate the model using Weighted Least Squares (WLS).\n",
        "    first_stage_model = smf.wls(\n",
        "        formula=formula,\n",
        "        data=data,\n",
        "        weights=data[weight_col]\n",
        "    ).fit()\n",
        "\n",
        "    # --- Weak Instrument Test ---\n",
        "    # Perform a joint F-test on the instruments' coefficients.\n",
        "    f_test_result = first_stage_model.f_test(iv_names)\n",
        "    f_statistic = f_test_result.fvalue\n",
        "\n",
        "    # The paper reports F ~ 52. We assert against the standard threshold of 10.\n",
        "    if f_statistic < 10:\n",
        "        # This should be a warning, not an error, as it's a diagnostic.\n",
        "        print(f\"[{task_name}] WARNING: Weak instrument detected. \"\n",
        "              f\"First-stage F-statistic is {f_statistic:.2f}, which is below 10.\")\n",
        "    else:\n",
        "        print(f\"[{task_name}] First-stage F-statistic: {f_statistic:.2f} (Instruments are strong).\")\n",
        "\n",
        "    # Predict the values of the endogenous variable.\n",
        "    predicted_shock = first_stage_model.predict(data)\n",
        "    predicted_shock.name = 'shock_hat'\n",
        "\n",
        "    return first_stage_model, predicted_shock\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Step 2: Helper to Estimate Weighted Second Stage\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _estimate_second_stage(\n",
        "    data: pd.DataFrame,\n",
        "    outcome_var: str,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 10, Step 2\"\n",
        ") -> Any:\n",
        "    \"\"\"\n",
        "    Estimates the weighted second-stage regression with cluster-robust SEs.\n",
        "\n",
        "    Regresses the outcome variable on the predicted shock from the first stage.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The analysis data, including 'shock_hat'.\n",
        "        outcome_var (str): The name of the dependent variable column.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        Any: The fitted statsmodels results object with cluster-robust errors.\n",
        "    \"\"\"\n",
        "    # Define the formula for the second-stage regression.\n",
        "    # outcome ~ 1 + shock_hat\n",
        "    formula = f\"{outcome_var} ~ 1 + shock_hat\"\n",
        "\n",
        "    # Get column names for weights and clusters.\n",
        "    weight_col = config[\"algorithm_config_parameters\"][\"WEIGHT_COLUMN_MUNICIPALITY\"]\n",
        "    cluster_col = config[\"algorithm_config_parameters\"][\"CLUSTER_LEVEL\"]\n",
        "\n",
        "    # Estimate the model using Weighted Least Squares (WLS).\n",
        "    second_stage_model = smf.wls(\n",
        "        formula=formula,\n",
        "        data=data,\n",
        "        weights=data[weight_col]\n",
        "    ).fit()\n",
        "\n",
        "    # Compute cluster-robust standard errors.\n",
        "    robust_results = second_stage_model.get_robustcov_results(\n",
        "        cov_type='cluster',\n",
        "        groups=data[cluster_col]\n",
        "    )\n",
        "\n",
        "    print(f\"[{task_name}] Second-stage estimation for outcome '{outcome_var}' complete.\")\n",
        "    return robust_results\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Step 3: Helper to Implement the Wild Cluster Bootstrap\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_wild_cluster_bootstrap(\n",
        "    data: pd.DataFrame,\n",
        "    outcome_var: str,\n",
        "    second_stage_results: Any,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 10, Step 3\"\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Implements the wild cluster bootstrap for robust inference.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The analysis data for a single event year.\n",
        "        outcome_var (str): The name of the dependent variable column.\n",
        "        second_stage_results (Any): The initial second-stage results object.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: The lower and upper bounds of the bootstrap confidence interval.\n",
        "    \"\"\"\n",
        "    # Extract necessary parameters and data.\n",
        "    n_reps = config[\"algorithm_config_parameters\"][\"BOOTSTRAP_REPLICATIONS\"]\n",
        "    seed = config[\"algorithm_config_parameters\"][\"RANDOM_SEED\"]\n",
        "    cluster_col = config[\"algorithm_config_parameters\"][\"CLUSTER_LEVEL\"]\n",
        "    conf_level = config[\"algorithm_config_parameters\"][\"BOOTSTRAP_CONF_LEVEL\"]\n",
        "\n",
        "    # Set seed for reproducibility.\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Get unique cluster identifiers.\n",
        "    clusters = data[cluster_col].unique()\n",
        "\n",
        "    # Get fitted values and residuals from the original second-stage model.\n",
        "    fitted_values = second_stage_results.fittedvalues\n",
        "    residuals = second_stage_results.resid\n",
        "\n",
        "    # Store bootstrap coefficient estimates.\n",
        "    bootstrap_coeffs = []\n",
        "\n",
        "    print(f\"[{task_name}] Starting wild cluster bootstrap with {n_reps} replications...\")\n",
        "    for i in range(n_reps):\n",
        "        # 1. Generate Rademacher weights at the cluster level.\n",
        "        rademacher_weights = rng.choice([-1, 1], size=len(clusters))\n",
        "        cluster_weights = pd.DataFrame({\n",
        "            cluster_col: clusters,\n",
        "            'v': rademacher_weights\n",
        "        })\n",
        "\n",
        "        # 2. Merge weights and create bootstrap residuals.\n",
        "        # ε_r^(b) = v_g(r) * ε̂_r\n",
        "        bootstrap_data = data.merge(cluster_weights, on=cluster_col, how='left')\n",
        "        bootstrap_residuals = bootstrap_data['v'] * residuals\n",
        "\n",
        "        # 3. Generate bootstrap outcome variable.\n",
        "        # Y_r^(b) = Ŷ_r + ε̂_r^(b)\n",
        "        bootstrap_data[f'{outcome_var}_boot'] = fitted_values + bootstrap_residuals\n",
        "\n",
        "        # 4. Re-estimate the 2SLS model.\n",
        "        # We must re-run both stages for each bootstrap replication.\n",
        "        _, pred_shock_boot = _estimate_first_stage(bootstrap_data, config, task_name=\"Bootstrap First Stage\")\n",
        "        bootstrap_data['shock_hat_boot'] = pred_shock_boot\n",
        "\n",
        "        # Estimate second stage without robust SEs, as we are building the distribution.\n",
        "        formula = f\"{outcome_var}_boot ~ 1 + shock_hat_boot\"\n",
        "        weight_col = config[\"algorithm_config_parameters\"][\"WEIGHT_COLUMN_MUNICIPALITY\"]\n",
        "\n",
        "        boot_model = smf.wls(formula, data=bootstrap_data, weights=bootstrap_data[weight_col]).fit()\n",
        "\n",
        "        # Store the coefficient of interest.\n",
        "        bootstrap_coeffs.append(boot_model.params['shock_hat_boot'])\n",
        "\n",
        "    # 5. Compute the percentile confidence interval.\n",
        "    alpha = 1 - conf_level\n",
        "    lower_bound = np.percentile(bootstrap_coeffs, 100 * alpha / 2)\n",
        "    upper_bound = np.percentile(bootstrap_coeffs, 100 * (1 - alpha / 2))\n",
        "\n",
        "    print(f\"[{task_name}] Bootstrap complete.\")\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_regional_effect_2sls(\n",
        "    event_study_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    outcome_variable: str,\n",
        "    event_year: int\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the 2SLS estimation for a given outcome and event year.\n",
        "\n",
        "    This function performs the full 2SLS estimation pipeline:\n",
        "    1. Estimates the weighted first stage and checks instrument strength.\n",
        "    2. Estimates the weighted second stage with cluster-robust standard errors.\n",
        "    3. Runs a wild cluster bootstrap to compute a robust confidence interval.\n",
        "\n",
        "    Args:\n",
        "        event_study_df (pd.DataFrame): The final, analysis-ready dataset from Task 9.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        outcome_variable (str): The name of the outcome column to use (e.g., 'emp_outcome').\n",
        "        event_year (int): The specific year of the event study to estimate.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the key estimation results:\n",
        "                        'point_estimate', 'cluster_robust_se', 'p_value',\n",
        "                        'f_statistic', 'bootstrap_ci', and 'n_obs'.\n",
        "    \"\"\"\n",
        "    # --- Prepare Data for the Specific Event Year ---\n",
        "    # Filter data to the specified year and drop any rows with missing values\n",
        "    # in the variables required for the model.\n",
        "    model_vars = [\n",
        "        outcome_variable, 'shock', 'shock_hat',\n",
        "        master_config[\"algorithm_config_parameters\"][\"WEIGHT_COLUMN_MUNICIPALITY\"],\n",
        "        master_config[\"algorithm_config_parameters\"][\"CLUSTER_LEVEL\"]\n",
        "    ] + list(master_config[\"algorithm_config_parameters\"][\"INSTRUMENT_SPECIFICATION\"].keys())\n",
        "\n",
        "    year_data = event_study_df.loc[\n",
        "        (slice(None), event_year), :\n",
        "    ].copy().reset_index(level='snapshot_year')\n",
        "\n",
        "    # Temporarily add 'shock_hat' to ensure dropna works correctly.\n",
        "    year_data['shock_hat'] = 0\n",
        "    year_data.dropna(subset=model_vars, inplace=True)\n",
        "\n",
        "    if year_data.empty:\n",
        "        print(f\"WARNING: No valid observations for year {event_year} and outcome {outcome_variable}. Skipping.\")\n",
        "        return {}\n",
        "\n",
        "    # Step 1: Estimate the first stage and get predicted shock.\n",
        "    first_stage_res, predicted_shock = _estimate_first_stage(year_data, master_config)\n",
        "    year_data['shock_hat'] = predicted_shock\n",
        "\n",
        "    # Step 2: Estimate the second stage with cluster-robust SEs.\n",
        "    second_stage_res_robust = _estimate_second_stage(year_data, outcome_variable, master_config)\n",
        "\n",
        "    # Step 3: Run the wild cluster bootstrap for a robust CI.\n",
        "    ci_lower, ci_upper = _run_wild_cluster_bootstrap(\n",
        "        data=year_data,\n",
        "        outcome_var=outcome_variable,\n",
        "        second_stage_results=second_stage_res_robust,\n",
        "        config=master_config\n",
        "    )\n",
        "\n",
        "    # --- Assemble and Return Final Results ---\n",
        "    point_estimate = second_stage_res_robust.params['shock_hat']\n",
        "    cluster_robust_se = second_stage_res_robust.bse['shock_hat']\n",
        "    p_value = second_stage_res_robust.pvalues['shock_hat']\n",
        "    f_statistic = first_stage_res.f_test(list(master_config[\"algorithm_config_parameters\"][\"INSTRUMENT_SPECIFICATION\"].keys())).fvalue\n",
        "\n",
        "    results = {\n",
        "        'event_year': event_year,\n",
        "        'outcome': outcome_variable,\n",
        "        'point_estimate': point_estimate,\n",
        "        'cluster_robust_se': cluster_robust_se,\n",
        "        'p_value': p_value,\n",
        "        'f_statistic': f_statistic,\n",
        "        'bootstrap_ci': (ci_lower, ci_upper),\n",
        "        'n_obs': int(second_stage_res_robust.nobs)\n",
        "    }\n",
        "\n",
        "    print(f\"\\n--- Results for {event_year} | {outcome_variable} ---\")\n",
        "    print(f\"Point Estimate (β^R): {point_estimate:.4f}\")\n",
        "    print(f\"Cluster-Robust SE: {cluster_robust_se:.4f}\")\n",
        "    print(f\"Bootstrap 95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
        "    print(f\"First-Stage F-stat: {f_statistic:.2f}\")\n",
        "    print(\"----------------------------------------\\n\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "I0nAfgS9tD4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Decompose regional employment into flow components (Equation (3))\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Decompose regional employment into flow components (Equation (3))\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Step 1: Helper to Compute Employment Flow Shares\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_employment_flow_shares(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    regional_panel: pd.DataFrame,\n",
        "    event_year: int,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 11, Step 1\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes displacement, inflow, and relocation shares for a given event year.\n",
        "\n",
        "    This function tracks native workers between the base year (1990) and the\n",
        "    specified event year to construct the flow components needed for the\n",
        "    employment decomposition, normalized by baseline native employment.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel from Task 5.\n",
        "        regional_panel (pd.DataFrame): The aggregated municipality-year panel from Task 6.\n",
        "        event_year (int): The post-treatment year to compare against the base year.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by 'Municipality_ID' with columns for\n",
        "                      'displacement_share', 'inflow_share', and 'relocation_share'.\n",
        "    \"\"\"\n",
        "    # Extract the base year from the configuration for comparison.\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "\n",
        "    # Select only native workers and the two relevant years for flow analysis.\n",
        "    # This is the foundational dataset for tracking individual transitions.\n",
        "    native_panel = analysis_panel[\n",
        "        analysis_panel['is_native'] &\n",
        "        analysis_panel.index.get_level_values('snapshot_year').isin([base_year, event_year])\n",
        "    ].copy()\n",
        "\n",
        "    # Pivot the panel to create a wide-format DataFrame. Each row represents a unique\n",
        "    # worker, with columns for their status in both the base and event years.\n",
        "    flow_df = native_panel.reset_index().pivot_table(\n",
        "        index='Worker_ID',\n",
        "        columns='snapshot_year',\n",
        "        values=['is_employed', 'Municipality_ID', 'fte_weight']\n",
        "    )\n",
        "    # Flatten the multi-level column index for easier access.\n",
        "    flow_df.columns = [f'{val}_{year}' for val, year in flow_df.columns]\n",
        "\n",
        "    # --- Classify Worker Flows ---\n",
        "    # Define boolean masks for each flow category. This vectorized approach is highly efficient.\n",
        "\n",
        "    # Condition for being employed in the base year.\n",
        "    employed_base = flow_df[f'is_employed_{base_year}'].fillna(False).astype(bool)\n",
        "    # Condition for being employed in the event year.\n",
        "    employed_event = flow_df[f'is_employed_{event_year}'].fillna(False).astype(bool)\n",
        "\n",
        "    # Displacement: Employed in base year -> Not employed in event year.\n",
        "    # Contribution is the FTE weight from the base year.\n",
        "    flow_df['displacement_fte'] = np.where(\n",
        "        employed_base & ~employed_event,\n",
        "        flow_df[f'fte_weight_{base_year}'],\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # Relocation: Employed in base year -> Employed in a DIFFERENT municipality in event year.\n",
        "    # Contribution is the FTE weight from the base year.\n",
        "    flow_df['relocation_fte'] = np.where(\n",
        "        employed_base & employed_event &\n",
        "        (flow_df[f'Municipality_ID_{base_year}'] != flow_df[f'Municipality_ID_{event_year}']),\n",
        "        flow_df[f'fte_weight_{base_year}'],\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # Inflow: Employed in event year -> but was NOT employed in that SAME municipality in base year.\n",
        "    # This includes those who were non-employed or employed elsewhere in the base year.\n",
        "    # Contribution is the FTE weight from the event year.\n",
        "    flow_df['inflow_fte'] = np.where(\n",
        "        employed_event & (\n",
        "            ~employed_base |\n",
        "            (flow_df[f'Municipality_ID_{base_year}'] != flow_df[f'Municipality_ID_{event_year}'])\n",
        "        ),\n",
        "        flow_df[f'fte_weight_{event_year}'],\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # --- Aggregate Flows by Municipality ---\n",
        "    # For displacement and relocation, we group by the origin municipality (base year).\n",
        "    displacement_agg = flow_df.groupby(f'Municipality_ID_{base_year}')['displacement_fte'].sum()\n",
        "    relocation_agg = flow_df.groupby(f'Municipality_ID_{base_year}')['relocation_fte'].sum()\n",
        "\n",
        "    # For inflows, we group by the destination municipality (event year).\n",
        "    inflow_agg = flow_df.groupby(f'Municipality_ID_{event_year}')['inflow_fte'].sum()\n",
        "\n",
        "    # --- Assemble and Normalize Flow Shares ---\n",
        "    # Get the baseline native employment for normalization from the regional panel.\n",
        "    weight_col = config[\"algorithm_config_parameters\"][\"WEIGHT_COLUMN_MUNICIPALITY\"]\n",
        "    baseline_employment = regional_panel.groupby('Municipality_ID')[weight_col].first()\n",
        "\n",
        "    # Create the final DataFrame of flow shares, indexed by Municipality_ID.\n",
        "    flow_shares = pd.DataFrame(index=baseline_employment.index)\n",
        "    flow_shares['displacement_share'] = displacement_agg\n",
        "    flow_shares['relocation_share'] = relocation_agg\n",
        "    flow_shares['inflow_share'] = inflow_agg\n",
        "\n",
        "    # Normalize by the baseline employment, handling division by zero.\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        for col in flow_shares.columns:\n",
        "            flow_shares[col] = np.divide(flow_shares[col], baseline_employment)\n",
        "\n",
        "    # Replace inf/-inf with NaN and then fill all NaNs with 0.\n",
        "    flow_shares.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    flow_shares.fillna(0, inplace=True)\n",
        "\n",
        "    print(f\"[{task_name}] Employment flow shares computed for year {event_year}.\")\n",
        "    return flow_shares\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def decompose_regional_employment_effect(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    regional_panel: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    event_year: int\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the decomposition of the regional employment effect.\n",
        "\n",
        "    This function first computes the micro-level employment flow shares\n",
        "    (displacement, inflow, relocation) and then estimates the causal effect\n",
        "    of the immigration shock on each component using 2SLS. It verifies that\n",
        "    the sum of the component effects reconciles with the total regional effect.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel from Task 5.\n",
        "        regional_panel (pd.DataFrame): The aggregated municipality-year panel from Task 6.\n",
        "        event_study_df (pd.DataFrame): The analysis-ready dataset from Task 9.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        event_year (int): The specific year of the event study to estimate.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the estimation results for the\n",
        "                        total effect and each of the three flow components.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(analysis_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel` must be a pandas DataFrame.\")\n",
        "    if not isinstance(regional_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`regional_panel` must be a pandas DataFrame.\")\n",
        "    if not isinstance(event_study_df, pd.DataFrame):\n",
        "        raise TypeError(\"`event_study_df` must be a pandas DataFrame.\")\n",
        "    if not isinstance(master_config, dict):\n",
        "        raise TypeError(\"`master_config` must be a dictionary.\")\n",
        "\n",
        "    # Step 1: Compute the employment flow shares for the given event year.\n",
        "    flow_shares_df = _compute_employment_flow_shares(\n",
        "        analysis_panel, regional_panel, event_year, master_config\n",
        "    )\n",
        "\n",
        "    # --- Prepare a consistent analysis sample for all regressions ---\n",
        "    # Merge flow shares into the main event study dataset for the specified year.\n",
        "    analysis_data_year = event_study_df.loc[(slice(None), event_year), :].copy()\n",
        "    analysis_data_year = analysis_data_year.merge(\n",
        "        flow_shares_df, on='Municipality_ID', how='left'\n",
        "    )\n",
        "\n",
        "    # Define all outcome variables for this task.\n",
        "    outcomes_to_estimate = {\n",
        "        'total_effect': 'emp_outcome',\n",
        "        'displacement': 'displacement_share',\n",
        "        'inflow': 'inflow_share',\n",
        "        'relocation': 'relocation_share'\n",
        "    }\n",
        "\n",
        "    # Drop rows with missing values in any of the outcomes to ensure a consistent sample.\n",
        "    # This is the critical step for ensuring the decomposition identity holds.\n",
        "    analysis_data_year.dropna(subset=list(outcomes_to_estimate.values()), inplace=True)\n",
        "\n",
        "    # --- Step 2: Estimate 2SLS for each component ---\n",
        "    results = {}\n",
        "    for name, outcome_col in outcomes_to_estimate.items():\n",
        "        print(f\"\\n--- Estimating effect on: {name} ({outcome_col}) for year {event_year} ---\")\n",
        "        # Reuse the robust 2SLS estimation function from Task 10.\n",
        "        results[name] = estimate_regional_effect_2sls(\n",
        "            event_study_df=analysis_data_year.set_index('Municipality_ID', append=True).swaplevel(0,1),\n",
        "            master_config=master_config,\n",
        "            outcome_variable=outcome_col,\n",
        "            event_year=event_year\n",
        "        )\n",
        "\n",
        "    # --- Step 3: Verify the summation identity ---\n",
        "    # Identity: β^R ≈ (-δ_displacement) + δ_inflow - δ_relocation\n",
        "    total_beta = results['total_effect']['point_estimate']\n",
        "    sum_of_components = (\n",
        "        -results['displacement']['point_estimate'] +\n",
        "        results['inflow']['point_estimate'] -\n",
        "        results['relocation']['point_estimate']\n",
        "    )\n",
        "\n",
        "    # Use a tight absolute tolerance for the verification.\n",
        "    if not np.isclose(total_beta, sum_of_components, atol=1e-8):\n",
        "        raise AssertionError(\n",
        "            \"Decomposition identity check FAILED! \"\n",
        "            f\"Total Effect (β^R) = {total_beta:.8f}, but \"\n",
        "            f\"Sum of Components = {sum_of_components:.8f}. \"\n",
        "            \"This indicates a logical error in flow construction or sample inconsistency.\"\n",
        "        )\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Decomposition Identity Verification PASSED\")\n",
        "        print(f\"Total Effect (β^R): {total_beta:.6f}\")\n",
        "        print(f\"Sum of Components:  {sum_of_components:.6f}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\nTask 11: Decomposition of regional employment effect for year {event_year} completed.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "04xShWrxt3jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Estimate regional and pure wage effects (Equations (5) and (4))\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Estimate regional and pure wage effects (Equations (5) and (4))\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Step 2: Helper to Estimate Pure Wage Effect (FD-IV for Stayers)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _estimate_pure_wage_effect_stayers(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    event_year: int,\n",
        "    task_name: str = \"Task 12, Step 2\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Estimates the pure wage effect (γ^W) for stayers using a FD-IV model.\n",
        "\n",
        "    This function identifies \"stayers\" (workers in the same municipality at\n",
        "    base and event year), constructs a first-differenced dataset of their\n",
        "    wages and age controls, and estimates the effect of the immigration shock\n",
        "    using an individual-level 2SLS model with a wild cluster bootstrap.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel from Task 5.\n",
        "        event_study_df (pd.DataFrame): The analysis-ready dataset from Task 9.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        event_year (int): The specific year of the event study to estimate.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the key estimation results.\n",
        "    \"\"\"\n",
        "    base_year = master_config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "\n",
        "    # --- 1. Identify Stayers and Prepare Data ---\n",
        "    # Filter to native, full-time workers in the two relevant years.\n",
        "    stayers_panel = analysis_panel[\n",
        "        analysis_panel['is_native'] &\n",
        "        analysis_panel['is_full_time'] &\n",
        "        analysis_panel.index.get_level_values('snapshot_year').isin([base_year, event_year])\n",
        "    ].copy()\n",
        "\n",
        "    # Pivot to wide format to identify stayers.\n",
        "    stayers_wide = stayers_panel.reset_index().pivot_table(\n",
        "        index='Worker_ID',\n",
        "        columns='snapshot_year',\n",
        "        values=['Municipality_ID', 'log_wage_imputed', 'age', 'age_sq']\n",
        "    )\n",
        "    stayers_wide.columns = [f'{val}_{year}' for val, year in stayers_wide.columns]\n",
        "\n",
        "    # A stayer is employed in the same municipality in both years.\n",
        "    stayers_wide = stayers_wide.dropna(\n",
        "        subset=[f'Municipality_ID_{base_year}', f'Municipality_ID_{event_year}']\n",
        "    )\n",
        "    stayers_df = stayers_wide[\n",
        "        stayers_wide[f'Municipality_ID_{base_year}'] == stayers_wide[f'Municipality_ID_{event_year}']\n",
        "    ].copy()\n",
        "    stayers_df.rename(columns={f'Municipality_ID_{base_year}': 'Municipality_ID'}, inplace=True)\n",
        "\n",
        "    # --- 2. Construct First-Differenced Variables ---\n",
        "    # Δlog_w_ir = log_w_irt - log_w_ir,1990\n",
        "    stayers_df['delta_log_wage'] = stayers_df[f'log_wage_imputed_{event_year}'] - stayers_df[f'log_wage_imputed_{base_year}']\n",
        "    # Δage_i = age_it - age_i,1990\n",
        "    stayers_df['delta_age'] = stayers_df[f'age_{event_year}'] - stayers_df[f'age_{base_year}']\n",
        "    # Δage_i^2 = (age_it)^2 - (age_i,1990)^2\n",
        "    stayers_df['delta_age_sq'] = stayers_df[f'age_sq_{event_year}'] - stayers_df[f'age_sq_{base_year}']\n",
        "\n",
        "    # --- 3. Merge Shocks, Instruments, and Cluster IDs ---\n",
        "    # Get the single event-year slice from the main event study df.\n",
        "    event_data_year = event_study_df.loc[(slice(None), event_year), :].reset_index()\n",
        "\n",
        "    # Merge the municipality-level variables onto the individual-level stayer data.\n",
        "    iv_cols = ['shock'] + list(master_config[\"algorithm_config_parameters\"][\"INSTRUMENT_SPECIFICATION\"].keys()) + ['District_ID']\n",
        "    stayers_df = stayers_df.merge(\n",
        "        event_data_year[['Municipality_ID'] + iv_cols],\n",
        "        on='Municipality_ID',\n",
        "        how='left'\n",
        "    )\n",
        "    stayers_df.dropna(inplace=True) # Drop if any merge failed or data is missing.\n",
        "\n",
        "    # --- 4. Estimate the FD-IV Model ---\n",
        "    # Prepare data for linearmodels.\n",
        "    dependent = stayers_df['delta_log_wage']\n",
        "    exog = sm.add_constant(stayers_df[['delta_age', 'delta_age_sq']])\n",
        "    endog = stayers_df[['shock']]\n",
        "    instruments = stayers_df[list(master_config[\"algorithm_config_parameters\"][\"INSTRUMENT_SPECIFICATION\"].keys())]\n",
        "    clusters = stayers_df['District_ID']\n",
        "\n",
        "    # Fit the 2SLS model with cluster-robust standard errors.\n",
        "    model = IV2SLS(dependent, exog, endog, instruments).fit(cov_type='clustered', clusters=clusters)\n",
        "\n",
        "    # --- 5. Implement Wild Cluster Bootstrap ---\n",
        "    n_reps = master_config[\"algorithm_config_parameters\"][\"BOOTSTRAP_REPLICATIONS\"]\n",
        "    seed = master_config[\"algorithm_config_parameters\"][\"RANDOM_SEED\"]\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    unique_clusters = clusters.unique()\n",
        "    fitted_values = model.predict()\n",
        "    residuals = model.resids\n",
        "\n",
        "    bootstrap_coeffs = []\n",
        "    print(f\"[{task_name}] Starting wild cluster bootstrap for pure wage effect...\")\n",
        "    for _ in range(n_reps):\n",
        "        # Generate Rademacher weights at the cluster level.\n",
        "        rademacher_weights = rng.choice([-1, 1], size=len(unique_clusters))\n",
        "        cluster_weights = pd.DataFrame({'District_ID': unique_clusters, 'v': rademacher_weights})\n",
        "\n",
        "        # Create bootstrap residuals.\n",
        "        temp_df = stayers_df.merge(cluster_weights, on='District_ID', how='left')\n",
        "        boot_residuals = temp_df['v'].values * residuals.values\n",
        "\n",
        "        # Create bootstrap outcome.\n",
        "        boot_dependent = fitted_values.values.flatten() + boot_residuals\n",
        "\n",
        "        # Re-estimate the model.\n",
        "        boot_model = IV2SLS(boot_dependent, exog, endog, instruments).fit(cov_type='clustered', clusters=clusters)\n",
        "        bootstrap_coeffs.append(boot_model.params['shock'])\n",
        "\n",
        "    # Compute percentile confidence interval.\n",
        "    alpha = 1 - master_config[\"algorithm_config_parameters\"][\"BOOTSTRAP_CONF_LEVEL\"]\n",
        "    ci_lower = np.percentile(bootstrap_coeffs, 100 * alpha / 2)\n",
        "    ci_upper = np.percentile(bootstrap_coeffs, 100 * (1 - alpha / 2))\n",
        "\n",
        "    # --- 6. Assemble and Return Results ---\n",
        "    results = {\n",
        "        'event_year': event_year,\n",
        "        'outcome': 'pure_wage_effect (stayers)',\n",
        "        'point_estimate': model.params['shock'],\n",
        "        'cluster_robust_se': model.std_errors['shock'],\n",
        "        'p_value': model.pvalues['shock'],\n",
        "        'f_statistic': model.first_stage.f_statistic.stat,\n",
        "        'bootstrap_ci': (ci_lower, ci_upper),\n",
        "        'n_obs': int(model.nobs)\n",
        "    }\n",
        "\n",
        "    print(f\"\\n--- Results for {event_year} | Pure Wage Effect (γ^W) ---\")\n",
        "    print(f\"Point Estimate (γ^W): {results['point_estimate']:.4f}\")\n",
        "    print(f\"Cluster-Robust SE: {results['cluster_robust_se']:.4f}\")\n",
        "    print(f\"Bootstrap 95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
        "    print(f\"First-Stage F-stat: {results['f_statistic']:.2f}\")\n",
        "    print(\"--------------------------------------------------\\n\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_wage_effects(\n",
        "    event_study_df: pd.DataFrame,\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    event_year: int\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the estimation of both regional and pure wage effects.\n",
        "\n",
        "    This function serves as the main entry point for Task 12. It estimates:\n",
        "    1. The regional wage effect (γ^R) using a municipality-level 2SLS model.\n",
        "    2. The pure wage effect (γ^W) using an individual-level FD-IV model on stayers.\n",
        "\n",
        "    It returns a dictionary containing the results for both estimations,\n",
        "    allowing for a direct comparison that reveals the impact of workforce\n",
        "    composition changes.\n",
        "\n",
        "    Args:\n",
        "        event_study_df (pd.DataFrame): The final, analysis-ready dataset from Task 9.\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel from Task 5.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        event_year (int): The specific year of the event study to estimate.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary with two keys, 'regional_wage_effect'\n",
        "                                   and 'pure_wage_effect', each containing a\n",
        "                                   detailed dictionary of estimation results.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(event_study_df, pd.DataFrame):\n",
        "        raise TypeError(\"`event_study_df` must be a pandas DataFrame.\")\n",
        "    if not isinstance(analysis_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Step 1: Estimate Regional Wage Effect (γ^R) ---\n",
        "    # Reuse the robust 2SLS estimation function from Task 10.\n",
        "    regional_results = estimate_regional_effect_2sls(\n",
        "        event_study_df=event_study_df,\n",
        "        master_config=master_config,\n",
        "        outcome_variable='wage_outcome',\n",
        "        event_year=event_year\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Estimate Pure Wage Effect (γ^W) ---\n",
        "    pure_results = _estimate_pure_wage_effect_stayers(\n",
        "        analysis_panel=analysis_panel,\n",
        "        event_study_df=event_study_df,\n",
        "        master_config=master_config,\n",
        "        event_year=event_year\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Reconcile and Assemble Final Output ---\n",
        "    final_results = {\n",
        "        'regional_wage_effect': regional_results,\n",
        "        'pure_wage_effect': pure_results\n",
        "    }\n",
        "\n",
        "    # Print a summary comparison.\n",
        "    gamma_R = regional_results.get('point_estimate', np.nan)\n",
        "    gamma_W = pure_results.get('point_estimate', np.nan)\n",
        "    composition_effect = gamma_R - gamma_W\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Wage Effect Reconciliation Summary for {event_year}\")\n",
        "    print(f\"Regional Wage Effect (γ^R): {gamma_R:.4f}\")\n",
        "    print(f\"Pure Wage Effect (γ^W):     {gamma_W:.4f}\")\n",
        "    print(f\"Implied Composition Effect: {composition_effect:.4f}\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(\"Task 12: Estimation of regional and pure wage effects completed successfully.\")\n",
        "    return final_results\n"
      ],
      "metadata": {
        "id": "UDcA5lvsulXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Decompose regional wage change into pure and composition effects (Equations (6)–(7))\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Decompose regional wage change into pure and composition effects\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Step 1: Helper to Compute Wage Decomposition Components\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_wage_decomposition_components(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    event_year: int,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 13, Step 1\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes wage decomposition components using a fully vectorized approach.\n",
        "\n",
        "    This function implements the decomposition of regional wage changes from\n",
        "    Equation (6) of the paper. It replaces the previous inefficient, iterative\n",
        "    method with a high-performance, vectorized algorithm using pandas\n",
        "    `groupby().transform()`. This is the professional standard for this type of\n",
        "    calculation.\n",
        "\n",
        "    The decomposition is:\n",
        "    Δlog(w_r) ≈ (E[w_t|stayer] - E[w_0|stayer])  (Stayers' Growth)\n",
        "                + (E[w_0|stayer] - E[w_0|outflower]) * Pr(outflow) (Outflow Comp.)\n",
        "                - (E[w_t|stayer] - E[w_t|inflower]) * Pr(inflow)   (Inflow Comp.)\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel from Task 5.\n",
        "        event_year (int): The post-treatment year to compare against the base year.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for clear error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by 'Municipality_ID' with columns for\n",
        "                      each decomposition component ('stayers_wage_growth',\n",
        "                      'outflow_composition', 'inflow_composition').\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(analysis_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- 1. Prepare Wide-Format Data for Flow Analysis ---\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "\n",
        "    # Filter to native, full-time workers in the two relevant years.\n",
        "    wage_panel = analysis_panel[\n",
        "        analysis_panel['is_native'] &\n",
        "        analysis_panel['is_full_time'] &\n",
        "        analysis_panel.index.get_level_values('snapshot_year').isin([base_year, event_year])\n",
        "    ].copy()\n",
        "\n",
        "    # Pivot to a wide format for individual-level flow classification.\n",
        "    flow_df = wage_panel.reset_index().pivot_table(\n",
        "        index='Worker_ID',\n",
        "        columns='snapshot_year',\n",
        "        values=['Municipality_ID', 'log_wage_imputed']\n",
        "    )\n",
        "    flow_df.columns = [f'{val}_{year}' for val, year in flow_df.columns]\n",
        "\n",
        "    # Define key columns for convenience.\n",
        "    muni_base_col, muni_event_col = f'Municipality_ID_{base_year}', f'Municipality_ID_{event_year}'\n",
        "    wage_base_col, wage_event_col = f'log_wage_imputed_{base_year}', f'log_wage_imputed_{event_year}'\n",
        "\n",
        "    # --- 2. Define Flow Group Masks ---\n",
        "    is_incumbent = flow_df[muni_base_col].notna()\n",
        "    is_employed_event = flow_df[muni_event_col].notna()\n",
        "    is_stayer = is_incumbent & is_employed_event & (flow_df[muni_base_col] == flow_df[muni_event_col])\n",
        "    is_outflower = is_incumbent & ~is_stayer\n",
        "    is_inflower = is_employed_event & ~is_stayer\n",
        "\n",
        "    # --- 3. Broadcast Group Means and Proportions using transform() ---\n",
        "\n",
        "    # a. Calculate conditional means for stayers and outflowers at origin (base year muni).\n",
        "    flow_df['mean_wage_stayers_base'] = flow_df.where(is_stayer)[wage_base_col].groupby(flow_df[muni_base_col]).transform('mean')\n",
        "    flow_df['mean_wage_outflowers_base'] = flow_df.where(is_outflower)[wage_base_col].groupby(flow_df[muni_base_col]).transform('mean')\n",
        "\n",
        "    # b. Calculate conditional means for stayers and inflowers at destination (event year muni).\n",
        "    flow_df['mean_wage_stayers_event'] = flow_df.where(is_stayer)[wage_event_col].groupby(flow_df[muni_event_col]).transform('mean')\n",
        "    flow_df['mean_wage_inflowers_event'] = flow_df.where(is_inflower)[wage_event_col].groupby(flow_df[muni_event_col]).transform('mean')\n",
        "\n",
        "    # c. Calculate proportions.\n",
        "    flow_df['n_incumbents'] = is_incumbent.groupby(flow_df[muni_base_col]).transform('sum')\n",
        "    flow_df['n_outflowers'] = is_outflower.groupby(flow_df[muni_base_col]).transform('sum')\n",
        "    flow_df['pr_outflow'] = flow_df['n_outflowers'] / flow_df['n_incumbents']\n",
        "\n",
        "    flow_df['n_employees_event'] = is_employed_event.groupby(flow_df[muni_event_col]).transform('sum')\n",
        "    flow_df['n_inflowers'] = is_inflower.groupby(flow_df[muni_event_col]).transform('sum')\n",
        "    flow_df['pr_inflow'] = flow_df['n_inflowers'] / flow_df['n_employees_event']\n",
        "\n",
        "    # --- 4. Calculate Components at the Individual Level ---\n",
        "\n",
        "    # Component (i): Stayers' wage growth. This is calculated directly on the stayer subset.\n",
        "    stayers_df = flow_df[is_stayer].copy()\n",
        "    stayers_df['stayers_wage_growth'] = stayers_df[wage_event_col] - stayers_df[wage_base_col]\n",
        "\n",
        "    # Component (ii): Outflow composition term.\n",
        "    flow_df['outflow_composition'] = (flow_df['mean_wage_stayers_base'] - flow_df['mean_wage_outflowers_base']) * flow_df['pr_outflow']\n",
        "\n",
        "    # Component (iii): Inflow composition term.\n",
        "    flow_df['inflow_composition'] = (flow_df['mean_wage_stayers_event'] - flow_df['mean_wage_inflowers_event']) * flow_df['pr_inflow']\n",
        "\n",
        "    # --- 5. Final Aggregation to Municipality Level ---\n",
        "\n",
        "    # Aggregate the stayers' growth component.\n",
        "    stayers_agg = stayers_df.groupby(muni_base_col)[['stayers_wage_growth']].mean()\n",
        "\n",
        "    # Aggregate the composition components (they are constant within a municipality, so 'first' is efficient).\n",
        "    outflow_agg = flow_df.groupby(muni_base_col)[['outflow_composition']].first()\n",
        "    inflow_agg = flow_df.groupby(muni_event_col)[['inflow_composition']].first()\n",
        "\n",
        "    # Combine all components into a single DataFrame.\n",
        "    components_df = pd.concat([stayers_agg, outflow_agg, inflow_agg], axis=1)\n",
        "\n",
        "    # Ensure all municipalities from the analysis are present, filling missing with 0.\n",
        "    all_munis = analysis_panel['Municipality_ID'].dropna().unique()\n",
        "    components_df = components_df.reindex(all_munis).fillna(0)\n",
        "\n",
        "    print(f\"[{task_name}] Wage decomposition components computed for year {event_year} using vectorized approach.\")\n",
        "    return components_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def decompose_regional_wage_effect(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    wage_effect_results: Dict[str, Dict[str, Any]],\n",
        "    master_config: Dict[str, Any],\n",
        "    event_year: int\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the decomposition of the regional wage effect.\n",
        "\n",
        "    This function implements the decomposition from Equations (6) and (7),\n",
        "    estimating the causal effect of immigration on each component of wage\n",
        "    change (stayers' growth, outflow selection, inflow selection) and\n",
        "    reconciling them with the total regional and pure wage effects.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        wage_effect_results (Dict): The results from Task 12, containing the\n",
        "                                    estimated γ^R and γ^W.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        event_year (int): The specific year of the event study to estimate.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the estimation results for\n",
        "                        each component of the wage decomposition.\n",
        "    \"\"\"\n",
        "    # Step 1: Compute the wage decomposition components at the municipality level.\n",
        "    components_df = _compute_wage_decomposition_components(\n",
        "        analysis_panel, event_year, master_config\n",
        "    )\n",
        "\n",
        "    # Prepare a consistent analysis sample.\n",
        "    analysis_data_year = event_study_df.loc[(slice(None), event_year), :].copy()\n",
        "    analysis_data_year = analysis_data_year.merge(\n",
        "        components_df, on='Municipality_ID', how='left'\n",
        "    )\n",
        "\n",
        "    outcomes_to_estimate = {\n",
        "        'stayers_growth': 'stayers_wage_growth',\n",
        "        'outflow_comp': 'outflow_composition',\n",
        "        'inflow_comp': 'inflow_composition'\n",
        "    }\n",
        "    analysis_data_year.dropna(subset=list(outcomes_to_estimate.values()), inplace=True)\n",
        "\n",
        "    # Step 2: Estimate 2SLS for each component.\n",
        "    results = {}\n",
        "    for name, outcome_col in outcomes_to_estimate.items():\n",
        "        print(f\"\\n--- Estimating effect on: {name} ({outcome_col}) for year {event_year} ---\")\n",
        "        results[name] = estimate_regional_effect_2sls(\n",
        "            event_study_df=analysis_data_year.set_index('Municipality_ID', append=True).swaplevel(0,1),\n",
        "            master_config=master_config,\n",
        "            outcome_variable=outcome_col,\n",
        "            event_year=event_year\n",
        "        )\n",
        "\n",
        "    # Step 3: Reconcile and interpret the full decomposition per Equation (7).\n",
        "    gamma_R = wage_effect_results['regional_wage_effect']['point_estimate']\n",
        "    gamma_W = wage_effect_results['pure_wage_effect']['point_estimate']\n",
        "\n",
        "    delta_stayers = results['stayers_growth']['point_estimate']\n",
        "    delta_outflow = results['outflow_comp']['point_estimate']\n",
        "    delta_inflow = results['inflow_comp']['point_estimate']\n",
        "\n",
        "    # The \"age selection\" effect is the difference between the raw stayers' growth\n",
        "    # effect and the age-controlled pure wage effect.\n",
        "    delta_age_selection = delta_stayers - gamma_W\n",
        "\n",
        "    # The total composition effect is the sum of its parts.\n",
        "    total_composition_effect = delta_outflow - delta_inflow + delta_age_selection\n",
        "\n",
        "    # Final identity check: γ^R ≈ γ^W + Total Composition Effect\n",
        "    reconstructed_gamma_R = gamma_W + total_composition_effect\n",
        "\n",
        "    if not np.isclose(gamma_R, reconstructed_gamma_R, atol=1e-8):\n",
        "        raise AssertionError(\n",
        "            \"Wage decomposition identity check FAILED! \"\n",
        "            f\"Regional Effect (γ^R) = {gamma_R:.8f}, but \"\n",
        "            f\"Reconstructed Effect (γ^W + Composition) = {reconstructed_gamma_R:.8f}.\"\n",
        "        )\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Wage Decomposition Identity Verification PASSED\")\n",
        "        print(f\"Regional Effect (γ^R): {gamma_R:.6f}\")\n",
        "        print(f\"Reconstructed (γ^W + Comp): {reconstructed_gamma_R:.6f}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    # Assemble final results dictionary in the format of Table 2.\n",
        "    final_decomposition = {\n",
        "        'regional_wage_effect': wage_effect_results['regional_wage_effect'],\n",
        "        'pure_wage_effect': wage_effect_results['pure_wage_effect'],\n",
        "        'composition_effect_total': total_composition_effect,\n",
        "        'composition_components': {\n",
        "            'outflow_selection': results['outflow_comp'],\n",
        "            'inflow_selection': results['inflow_comp'], # Note: sign is flipped in Table 2\n",
        "            'age_selection': delta_age_selection\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(f\"\\nTask 13: Decomposition of regional wage effect for year {event_year} completed.\")\n",
        "    return final_decomposition\n"
      ],
      "metadata": {
        "id": "DwEq6vJRv--U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Selection-bounding for the pure wage effect (Card-style bounds)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Selection-bounding for the pure wage effect (Card-style bounds)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Step 1: Helper to Estimate the Probit Model for Staying\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _estimate_staying_probit(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    event_year: int,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 14, Step 1\"\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Estimates a probit model for the probability of an incumbent staying.\n",
        "\n",
        "    This function identifies 1990 incumbents and models their probability of\n",
        "    staying in the same municipality by the event year as a function of the\n",
        "    local immigration shock.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        event_year (int): The specific year of the event study.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: A tuple containing:\n",
        "                             - b_hat (float): The coefficient on the shock variable (∂π/∂ΔI_r).\n",
        "                             - pi_hat (float): The average latent index π.\n",
        "    \"\"\"\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "\n",
        "    # Identify incumbents: native, full-time workers employed in 1990.\n",
        "    incumbents = analysis_panel[\n",
        "        (analysis_panel.index.get_level_values('snapshot_year') == base_year) &\n",
        "        analysis_panel['is_native'] &\n",
        "        analysis_panel['is_full_time'] &\n",
        "        analysis_panel['is_employed']\n",
        "    ].copy()\n",
        "\n",
        "    # Identify stayers among the 1990 incumbents.\n",
        "    stayers = analysis_panel[\n",
        "        (analysis_panel.index.get_level_values('snapshot_year') == event_year) &\n",
        "        analysis_panel.index.get_level_values('Worker_ID').isin(incumbents.index.get_level_values('Worker_ID'))\n",
        "    ]\n",
        "\n",
        "    # Create the binary 'is_stayer' dependent variable.\n",
        "    # A stayer is an incumbent who is employed in the same municipality in the event year.\n",
        "    stayer_status = incumbents.reset_index().merge(\n",
        "        stayers.reset_index()[['Worker_ID', 'Municipality_ID']],\n",
        "        on='Worker_ID',\n",
        "        how='left',\n",
        "        suffixes=('_base', '_event')\n",
        "    )\n",
        "    stayer_status['is_stayer'] = (\n",
        "        stayer_status['Municipality_ID_base'] == stayer_status['Municipality_ID_event']\n",
        "    ).astype(int)\n",
        "\n",
        "    # Merge the appropriate shock variable for the event year.\n",
        "    event_data_year = event_study_df.loc[(slice(None), event_year), ['shock']].reset_index()\n",
        "    probit_data = stayer_status.merge(\n",
        "        event_data_year.drop_duplicates(subset=['Municipality_ID']),\n",
        "        left_on='Municipality_ID_base',\n",
        "        right_on='Municipality_ID',\n",
        "        how='left'\n",
        "    ).dropna(subset=['is_stayer', 'shock'])\n",
        "\n",
        "    # Estimate the probit model: Pr(is_stayer=1) = Φ(a + b*shock).\n",
        "    X = sm.add_constant(probit_data['shock'])\n",
        "    y = probit_data['is_stayer']\n",
        "    probit_model = sm.Probit(y, X).fit(disp=0) # disp=0 suppresses convergence messages.\n",
        "\n",
        "    # Extract the key coefficient b_hat = ∂π/∂ΔI_r.\n",
        "    b_hat = probit_model.params['shock']\n",
        "\n",
        "    # Calculate the average latent index π_hat from the overall stayer rate.\n",
        "    # π = Φ⁻¹(η), where η is the share of stayers.\n",
        "    stayer_rate_eta = probit_data['is_stayer'].mean()\n",
        "    pi_hat = norm.ppf(stayer_rate_eta)\n",
        "\n",
        "    print(f\"[{task_name}] Probit model for staying estimated. b_hat = {b_hat:.4f}, pi_hat = {pi_hat:.4f}.\")\n",
        "    return b_hat, pi_hat\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Steps 2 & 3: Helper to Compute Bias Components and Final Bounds\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_selection_bounds(\n",
        "    pure_wage_results: Dict[str, Any],\n",
        "    b_hat: float,\n",
        "    pi_hat: float,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 14, Steps 2 & 3\"\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Computes the components of selection bias and the final bounds.\n",
        "\n",
        "    Args:\n",
        "        pure_wage_results (Dict): The results dictionary from the pure wage effect estimation.\n",
        "        b_hat (float): The probit coefficient on the shock variable.\n",
        "        pi_hat (float): The average latent index from the probit model.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float, float]: A tuple containing:\n",
        "                                    - max_bias (float): The maximum absolute potential bias.\n",
        "                                    - lower_bound (float): The lower bound on γ^W.\n",
        "                                    - upper_bound (float): The upper bound on γ^W.\n",
        "    \"\"\"\n",
        "    # --- Step 2: Compute Residual Variance and IMR Derivative ---\n",
        "    # The FD-IV model object must be passed within the results dictionary.\n",
        "    if 'model_object' not in pure_wage_results:\n",
        "        raise KeyError(\"`model_object` not found in `pure_wage_results`. Cannot access residuals.\")\n",
        "\n",
        "    # σ̂_Δe: Standard deviation of the time-varying wage shocks (residuals).\n",
        "    residuals = pure_wage_results['model_object'].resids\n",
        "    sigma_delta_e = np.std(residuals)\n",
        "\n",
        "    # λ(π): Inverse Mills Ratio at π_hat.\n",
        "    lambda_pi = norm.pdf(pi_hat) / norm.cdf(pi_hat)\n",
        "\n",
        "    # ∂λ(π)/∂π: Derivative of the Inverse Mills Ratio at π_hat.\n",
        "    # Formula: -λ(π) * (λ(π) + π)\n",
        "    d_lambda_d_pi = -lambda_pi * (lambda_pi + pi_hat)\n",
        "\n",
        "    print(f\"[{task_name}] Bias components: σ_Δe={sigma_delta_e:.4f}, ∂λ/∂π={d_lambda_d_pi:.4f}\")\n",
        "\n",
        "    # --- Step 3: Bound the Selection Bias ---\n",
        "    # Bias = ρ * σ̂_Δe * (∂λ/∂π) * (∂π/∂ΔI_r)\n",
        "    # We calculate the maximum absolute bias by setting ρ = ±1.\n",
        "    max_bias = np.abs(sigma_delta_e * d_lambda_d_pi * b_hat)\n",
        "\n",
        "    # Extract the point estimate for the pure wage effect.\n",
        "    gamma_W_hat = pure_wage_results['point_estimate']\n",
        "\n",
        "    # The bounds are the point estimate ± the maximum bias.\n",
        "    lower_bound = gamma_W_hat - max_bias\n",
        "    upper_bound = gamma_W_hat + max_bias\n",
        "\n",
        "    return max_bias, lower_bound, upper_bound\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def bound_pure_wage_effect_selection(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    pure_wage_results: Dict[str, Any],\n",
        "    master_config: Dict[str, Any],\n",
        "    event_year: int\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the selection-bounding exercise for the pure wage effect.\n",
        "\n",
        "    This function implements the Card-style bounding approach to assess the\n",
        "    potential bias in the pure wage effect estimate (γ^W) arising from\n",
        "    selection on time-varying unobservables.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        pure_wage_results (Dict[str, Any]): The detailed results from the pure\n",
        "            wage effect estimation in Task 12. Must include the fitted model\n",
        "            object to access residuals.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        event_year (int): The specific year of the event study to analyze.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the original point estimate,\n",
        "                        the maximum potential bias, and the resulting bounds.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(analysis_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel` must be a pandas DataFrame.\")\n",
        "    if 'point_estimate' not in pure_wage_results:\n",
        "        raise KeyError(\"`pure_wage_results` dictionary is missing key 'point_estimate'.\")\n",
        "\n",
        "    # Step 1: Estimate the probit model for the staying decision.\n",
        "    b_hat, pi_hat = _estimate_staying_probit(\n",
        "        analysis_panel, event_study_df, event_year, master_config\n",
        "    )\n",
        "\n",
        "    # Steps 2 & 3: Compute the bias components and the final bounds.\n",
        "    max_bias, lower_bound, upper_bound = _compute_selection_bounds(\n",
        "        pure_wage_results, b_hat, pi_hat, master_config\n",
        "    )\n",
        "\n",
        "    # --- Assemble and Report Final Results ---\n",
        "    gamma_W_hat = pure_wage_results['point_estimate']\n",
        "\n",
        "    results = {\n",
        "        'event_year': event_year,\n",
        "        'point_estimate_gamma_W': gamma_W_hat,\n",
        "        'max_potential_bias': max_bias,\n",
        "        'lower_bound': lower_bound,\n",
        "        'upper_bound': upper_bound\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Selection Bounding Results for Pure Wage Effect (γ^W) - Year {event_year}\")\n",
        "    print(f\"Original Point Estimate: {gamma_W_hat:.4f}\")\n",
        "    print(f\"Maximum Potential Bias (Δγ̂): ±{max_bias:.4f}\")\n",
        "    print(f\"Resulting 95% Bounds for γ^W: [{lower_bound:.4f}, {upper_bound:.4f}]\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(\"Task 14: Selection-bounding for the pure wage effect completed successfully.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "cs_ZhNxGwr4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Non-employed entrants analysis (employment entry and pure wages)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Non-employed entrants analysis (employment entry and pure wages)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Step 1: Helper to Prepare Non-Employed Entrants Data\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _prepare_non_employed_entrants_data(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    national_wage_series: pd.Series,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 15, Step 1\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Identifies the 1990 non-employed cohort and imputes their counterfactual baseline wage.\n",
        "\n",
        "    This function selects native workers who were not employed in 1990 but had a\n",
        "    prior job between 1986-1989. It then calculates their counterfactual 1990\n",
        "    log wage based on their last observed wage, adjusted for aggregate wage growth.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel from Task 5.\n",
        "        national_wage_series (pd.Series): Series of national mean log wages, indexed by year.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by 'Worker_ID' containing the cohort of\n",
        "                      non-employed entrants from 1990, with their counterfactual\n",
        "                      1990 log wage and origin municipality.\n",
        "    \"\"\"\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "\n",
        "    # Identify the cohort: native workers, non-employed in 1990, with a valid pre-1990 job record.\n",
        "    # The lookback information (`last_pre1990_*`) was attached in Task 5.\n",
        "    entrants_cohort_1990 = analysis_panel.loc[\n",
        "        (slice(None), base_year), :\n",
        "    ][\n",
        "        (~analysis_panel.loc[(slice(None), base_year), 'is_employed']) &\n",
        "        (analysis_panel.loc[(slice(None), base_year), 'is_native']) &\n",
        "        (analysis_panel.loc[(slice(None), base_year), 'last_pre1990_municipality_id'].notna())\n",
        "    ].copy()\n",
        "\n",
        "    if entrants_cohort_1990.empty:\n",
        "        print(f\"[{task_name}] No non-employed entrants with valid lookback history found.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Validate that last observed wages are positive before log transformation.\n",
        "    if (entrants_cohort_1990['last_pre1990_daily_wage'] <= 0).any():\n",
        "        raise ValueError(f\"[{task_name}] Found non-positive 'last_pre1990_daily_wage' values.\")\n",
        "\n",
        "    # --- Impute Counterfactual 1990 Log Wage ---\n",
        "    # Equation: log_w_tilde_ir,1990 = log_w_ir,t* + (log_w_bar_1990 - log_w_bar_t*)\n",
        "\n",
        "    # Get national average log wage for the base year.\n",
        "    log_w_bar_1990 = national_wage_series.loc[base_year]\n",
        "\n",
        "    # Map the national average log wage for the year of the last spell (t*).\n",
        "    entrants_cohort_1990['log_w_bar_t_star'] = entrants_cohort_1990['last_pre1990_spell_year'].map(national_wage_series)\n",
        "\n",
        "    # Calculate the imputed wage.\n",
        "    entrants_cohort_1990['log_wage_imputed_1990'] = (\n",
        "        np.log(entrants_cohort_1990['last_pre1990_daily_wage']) +\n",
        "        (log_w_bar_1990 - entrants_cohort_1990['log_w_bar_t_star'])\n",
        "    )\n",
        "\n",
        "    # Select and rename columns for clarity, setting Worker_ID as the index.\n",
        "    entrants_df = entrants_cohort_1990.reset_index()[\n",
        "        ['Worker_ID', 'last_pre1990_municipality_id', 'log_wage_imputed_1990']\n",
        "    ].rename(columns={'last_pre1990_municipality_id': 'municipality_id_1990'})\n",
        "    entrants_df = entrants_df.set_index('Worker_ID')\n",
        "\n",
        "    print(f\"[{task_name}] Prepared data for {len(entrants_df)} non-employed entrants from 1990.\")\n",
        "    return entrants_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Step 2: Helper to Estimate Effect on Employment Entry\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _estimate_entrant_employment_effect(\n",
        "    entrants_df: pd.DataFrame,\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    event_year: int,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 15, Step 2\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Estimates the 2SLS effect of immigration on the employment entry rate.\n",
        "\n",
        "    This function performs a complete 2SLS estimation to determine the causal\n",
        "    impact of the local immigration shock on the probability that a member of\n",
        "    the 1990 non-employed cohort finds employment by a given event year. The\n",
        "    estimation is conducted at the municipality level, weighted by the number\n",
        "    of non-employed entrants in each 1990 origin municipality. Inference is\n",
        "    based on a from-scratch wild cluster bootstrap to ensure robustness.\n",
        "\n",
        "    Args:\n",
        "        entrants_df (pd.DataFrame): A DataFrame of the 1990 non-employed cohort,\n",
        "            indexed by 'Worker_ID', with their 1990 origin municipality.\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel, used\n",
        "            to determine employment status in the event year.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9,\n",
        "            containing shocks, instruments, and cluster IDs.\n",
        "        event_year (int): The specific year of the event study to analyze.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the key estimation results:\n",
        "                        'point_estimate', 'cluster_robust_se', 'p_value',\n",
        "                        'f_statistic', 'bootstrap_ci', and 'n_obs'.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(entrants_df, pd.DataFrame):\n",
        "        raise TypeError(\"`entrants_df` must be a pandas DataFrame.\")\n",
        "    if not isinstance(analysis_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel` must be a pandas DataFrame.\")\n",
        "    if not isinstance(event_study_df, pd.DataFrame):\n",
        "        raise TypeError(\"`event_study_df` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- 1. Construct the Outcome Variable: Employment Entry Share ---\n",
        "\n",
        "    # Determine which entrants are employed in the event year by merging their\n",
        "    # 1990 cohort data with their status in the event year.\n",
        "    entrants_status_event_year = entrants_df.merge(\n",
        "        analysis_panel.loc[(slice(None), event_year), 'is_employed'].reset_index(),\n",
        "        on='Worker_ID',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Assume entrants not found in the event year panel are not employed.\n",
        "    entrants_status_event_year['is_employed'] = entrants_status_event_year['is_employed'].fillna(False)\n",
        "\n",
        "    # Aggregate at the 1990 origin municipality level to get the outcome and weight.\n",
        "    entry_rate_agg = entrants_status_event_year.groupby('municipality_id_1990').agg(\n",
        "        # The weight for this regression is the number of entrants from each municipality.\n",
        "        weight_entrants=('Worker_ID', 'count'),\n",
        "        # The numerator for the share is the count of those who became employed.\n",
        "        n_reemployed=('is_employed', 'sum')\n",
        "    )\n",
        "\n",
        "    # Calculate the outcome variable: Entry_Share_r = #{re-employed} / #{total entrants}.\n",
        "    entry_rate_agg['entry_share'] = entry_rate_agg['n_reemployed'] / entry_rate_agg['weight_entrants']\n",
        "\n",
        "    # --- 2. Prepare the Final Analysis DataFrame ---\n",
        "\n",
        "    # Start with the main event study data for the specified year.\n",
        "    analysis_data = event_study_df.loc[(slice(None), event_year), :].copy()\n",
        "\n",
        "    # Merge the newly computed outcome and weight variables.\n",
        "    analysis_data = analysis_data.merge(\n",
        "        entry_rate_agg,\n",
        "        left_on='Municipality_ID',\n",
        "        right_index=True,\n",
        "        how='left'\n",
        "    ).fillna({'entry_share': 0, 'weight_entrants': 0}) # Fill municipalities with no entrants.\n",
        "\n",
        "    # Filter to municipalities that had non-employed entrants in 1990 to form the analysis sample.\n",
        "    analysis_data = analysis_data[analysis_data['weight_entrants'] > 0].copy()\n",
        "\n",
        "    if analysis_data.empty:\n",
        "        print(f\"[{task_name}] No municipalities with non-employed entrants found. Cannot estimate.\")\n",
        "        return {}\n",
        "\n",
        "    # --- 3. Full 2SLS Estimation with Custom Weights ---\n",
        "\n",
        "    # Define the list of instrumental variables from the configuration.\n",
        "    iv_names = list(config[\"algorithm_config_parameters\"][\"INSTRUMENT_SPECIFICATION\"].keys())\n",
        "\n",
        "    # Define the R-style formula for the first-stage regression.\n",
        "    formula_1st = f\"shock ~ 1 + {' + '.join(iv_names)}\"\n",
        "\n",
        "    # Estimate the first stage using Weighted Least Squares (WLS).\n",
        "    first_stage = smf.wls(formula_1st, data=analysis_data, weights=analysis_data['weight_entrants']).fit()\n",
        "\n",
        "    # Predict the value of the endogenous variable from the first stage.\n",
        "    analysis_data['shock_hat'] = first_stage.predict(analysis_data)\n",
        "\n",
        "    # Define the R-style formula for the second-stage regression.\n",
        "    formula_2nd = \"entry_share ~ 1 + shock_hat\"\n",
        "\n",
        "    # Estimate the second stage using WLS with the same custom weights.\n",
        "    second_stage = smf.wls(formula_2nd, data=analysis_data, weights=analysis_data['weight_entrants']).fit()\n",
        "\n",
        "    # --- 4. Compute Cluster-Robust Standard Errors ---\n",
        "\n",
        "    # Get the cluster identifier column from the configuration.\n",
        "    cluster_col = config[\"algorithm_config_parameters\"][\"CLUSTER_LEVEL\"]\n",
        "\n",
        "    # Re-calculate standard errors to be robust to within-cluster correlation.\n",
        "    robust_results = second_stage.get_robustcov_results(cov_type='cluster', groups=analysis_data[cluster_col])\n",
        "\n",
        "    # --- 5. Implement Wild Cluster Bootstrap for Inference ---\n",
        "\n",
        "    # Extract bootstrap parameters from the configuration.\n",
        "    n_reps = config[\"algorithm_config_parameters\"][\"BOOTSTRAP_REPLICATIONS\"]\n",
        "    seed = config[\"algorithm_config_parameters\"][\"RANDOM_SEED\"]\n",
        "    conf_level = config[\"algorithm_config_parameters\"][\"BOOTSTRAP_CONF_LEVEL\"]\n",
        "\n",
        "    # Initialize a seeded random number generator for reproducibility.\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Get the array of unique cluster identifiers.\n",
        "    clusters = analysis_data[cluster_col].unique()\n",
        "\n",
        "    # Store the coefficient estimate from each bootstrap replication.\n",
        "    bootstrap_coeffs = []\n",
        "\n",
        "    # Main bootstrap loop.\n",
        "    for _ in range(n_reps):\n",
        "        # a. Generate Rademacher weights (-1 or 1) for each cluster.\n",
        "        rademacher_weights = rng.choice([-1, 1], size=len(clusters))\n",
        "        cluster_weights = pd.DataFrame({cluster_col: clusters, 'v': rademacher_weights})\n",
        "\n",
        "        # b. Create bootstrap residuals by merging weights and multiplying.\n",
        "        boot_data = analysis_data.merge(cluster_weights, on=cluster_col, how='left')\n",
        "        boot_residuals = boot_data['v'] * second_stage.resid\n",
        "\n",
        "        # c. Generate the bootstrap dependent variable: Y_boot = Y_hat + residual_boot.\n",
        "        boot_data['entry_share_boot'] = second_stage.fittedvalues + boot_residuals\n",
        "\n",
        "        # d. Re-estimate the full 2SLS model on the bootstrapped data.\n",
        "        # First stage on bootstrapped data.\n",
        "        fs_boot = smf.wls(formula_1st, data=boot_data, weights=boot_data['weight_entrants']).fit()\n",
        "        boot_data['shock_hat_boot'] = fs_boot.predict(boot_data)\n",
        "\n",
        "        # Second stage on bootstrapped data.\n",
        "        ss_boot = smf.wls(\"entry_share_boot ~ 1 + shock_hat_boot\", data=boot_data, weights=boot_data['weight_entrants']).fit()\n",
        "\n",
        "        # e. Store the coefficient of interest.\n",
        "        bootstrap_coeffs.append(ss_boot.params['shock_hat_boot'])\n",
        "\n",
        "    # --- 6. Compute Percentile Confidence Interval ---\n",
        "\n",
        "    # Calculate the alpha for the specified confidence level.\n",
        "    alpha = 1 - conf_level\n",
        "\n",
        "    # Compute the lower and upper bounds from the bootstrap distribution.\n",
        "    ci_lower = np.percentile(bootstrap_coeffs, 100 * alpha / 2)\n",
        "    ci_upper = np.percentile(bootstrap_coeffs, 100 * (1 - alpha / 2))\n",
        "\n",
        "    # --- 7. Assemble and Return Final Results ---\n",
        "\n",
        "    # Extract all key statistics from the estimation objects.\n",
        "    results = {\n",
        "        'point_estimate': robust_results.params['shock_hat'],\n",
        "        'cluster_robust_se': robust_results.bse['shock_hat'],\n",
        "        'p_value': robust_results.pvalues['shock_hat'],\n",
        "        'f_statistic': first_stage.f_test(iv_names).fvalue,\n",
        "        'bootstrap_ci': (ci_lower, ci_upper),\n",
        "        'n_obs': int(robust_results.nobs)\n",
        "    }\n",
        "\n",
        "    # Print a summary of the final result.\n",
        "    print(f\"[{task_name}] Estimated employment entry effect: {results['point_estimate']:.4f}\")\n",
        "\n",
        "    # Return the structured dictionary of results.\n",
        "    return results\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Step 3: Helper to Estimate Pure Wage Effect for Re-Employed Entrants\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _estimate_entrant_wage_effect(\n",
        "    entrants_df: pd.DataFrame,\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    event_year: int,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 15, Step 3\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Estimates the pure wage effect for non-employed entrants who find a job.\n",
        "\n",
        "    This function implements a First-Difference Instrumental Variable (FD-IV)\n",
        "    model for the cohort of workers who were non-employed in 1990 but became\n",
        "    re-employed by the specified event year. The dependent variable is the\n",
        "    change in log wage relative to the imputed counterfactual 1990 wage. The\n",
        "    model controls for changes in age and instruments the local immigration\n",
        "    shock with distance to the border. Inference is based on a full wild\n",
        "    cluster bootstrap procedure.\n",
        "\n",
        "    Args:\n",
        "        entrants_df (pd.DataFrame): A DataFrame of the 1990 non-employed cohort,\n",
        "            indexed by 'Worker_ID', with their imputed 1990 counterfactual wage.\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel, used\n",
        "            to identify re-employment status and event-year characteristics.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9,\n",
        "            containing shocks, instruments, and cluster IDs.\n",
        "        event_year (int): The specific year of the event study to analyze.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the key estimation results:\n",
        "                        'point_estimate', 'cluster_robust_se', 'p_value',\n",
        "                        'f_statistic', 'bootstrap_ci', and 'n_obs'. Returns an\n",
        "                        empty dictionary if no re-employed entrants are found.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(entrants_df, pd.DataFrame):\n",
        "        raise TypeError(\"`entrants_df` must be a pandas DataFrame.\")\n",
        "    if not isinstance(analysis_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel` must be a pandas DataFrame.\")\n",
        "    if not isinstance(event_study_df, pd.DataFrame):\n",
        "        raise TypeError(\"`event_study_df` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- 1. Identify the Analysis Sample: Re-employed Entrants ---\n",
        "\n",
        "    # Isolate the panel data for the specified event year.\n",
        "    panel_event_year = analysis_panel.loc[(slice(None), event_year), :].reset_index()\n",
        "\n",
        "    # Filter to workers who are employed and full-time in the event year.\n",
        "    reemployed_in_event_year = panel_event_year[\n",
        "        panel_event_year['is_employed'] & panel_event_year['is_full_time']\n",
        "    ]\n",
        "\n",
        "    # Perform an inner merge with the 1990 entrants cohort to get the final sample.\n",
        "    reemployed_entrants = entrants_df.merge(\n",
        "        reemployed_in_event_year,\n",
        "        on='Worker_ID',\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    # If no re-employed entrants are found, exit gracefully.\n",
        "    if reemployed_entrants.empty:\n",
        "        print(f\"[{task_name}] No re-employed entrants found for wage analysis in year {event_year}.\")\n",
        "        return {}\n",
        "\n",
        "    # --- 2. Construct First-Differenced (FD) Variables ---\n",
        "\n",
        "    # Dependent variable: Δlog_w = log_w_event - log_w_imputed_1990\n",
        "    reemployed_entrants['delta_log_wage'] = reemployed_entrants['log_wage_imputed'] - reemployed_entrants['log_wage_imputed_1990']\n",
        "\n",
        "    # To get Δage, we need the age in the base year for this cohort.\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "    base_year_age_map = analysis_panel.loc[(reemployed_entrants['Worker_ID'], base_year), 'age']\n",
        "    base_year_age = reemployed_entrants['Worker_ID'].map(base_year_age_map)\n",
        "\n",
        "    # Control variable 1: Δage = age_event - age_base\n",
        "    reemployed_entrants['delta_age'] = reemployed_entrants['age'] - base_year_age\n",
        "\n",
        "    # Control variable 2: Δage² = age_event² - age_base²\n",
        "    reemployed_entrants['delta_age_sq'] = reemployed_entrants['age']**2 - base_year_age**2\n",
        "\n",
        "    # --- 3. Merge Municipality-Level IVs and Cluster IDs ---\n",
        "\n",
        "    # Extract the relevant slice from the event study data.\n",
        "    event_data_year_slice = event_study_df.loc[(slice(None), event_year), :].reset_index()\n",
        "\n",
        "    # Define the columns to merge: shock, instruments, and cluster ID.\n",
        "    iv_cols = ['shock'] + list(config[\"algorithm_config_parameters\"][\"INSTRUMENT_SPECIFICATION\"].keys()) + [config[\"algorithm_config_parameters\"][\"CLUSTER_LEVEL\"]]\n",
        "\n",
        "    # Merge these variables onto the individual-level FD data based on the event-year municipality.\n",
        "    fd_data = reemployed_entrants.merge(\n",
        "        event_data_year_slice[['Municipality_ID'] + iv_cols],\n",
        "        on='Municipality_ID',\n",
        "        how='left'\n",
        "    ).dropna() # Drop any individuals with missing data for the model.\n",
        "\n",
        "    if fd_data.empty:\n",
        "        print(f\"[{task_name}] No valid observations remain after merging IVs. Cannot estimate.\")\n",
        "        return {}\n",
        "\n",
        "    # --- 4. Estimate the FD-IV Model ---\n",
        "\n",
        "    # Prepare data arrays for linearmodels.\n",
        "    dependent = fd_data['delta_log_wage']\n",
        "    exog = sm.add_constant(fd_data[['delta_age', 'delta_age_sq']])\n",
        "    endog = fd_data[['shock']]\n",
        "    instruments = fd_data[list(config[\"algorithm_config_parameters\"][\"INSTRUMENT_SPECIFICATION\"].keys())]\n",
        "    clusters = fd_data[config[\"algorithm_config_parameters\"][\"CLUSTER_LEVEL\"]]\n",
        "\n",
        "    # Fit the 2SLS model with cluster-robust standard errors.\n",
        "    model = IV2SLS(dependent, exog, endog, instruments).fit(cov_type='clustered', clusters=clusters)\n",
        "\n",
        "    # --- 5. Implement Wild Cluster Bootstrap ---\n",
        "\n",
        "    # Extract bootstrap parameters.\n",
        "    n_reps = config[\"algorithm_config_parameters\"][\"BOOTSTRAP_REPLICATIONS\"]\n",
        "    seed = config[\"algorithm_config_parameters\"][\"RANDOM_SEED\"]\n",
        "    conf_level = config[\"algorithm_config_parameters\"][\"BOOTSTRAP_CONF_LEVEL\"]\n",
        "\n",
        "    # Initialize seeded random number generator.\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Get unique clusters and model residuals.\n",
        "    unique_clusters = clusters.unique()\n",
        "    fitted_values = model.predict()\n",
        "    residuals = model.resids\n",
        "\n",
        "    # Store bootstrap coefficients.\n",
        "    bootstrap_coeffs = []\n",
        "\n",
        "    # Main bootstrap loop.\n",
        "    for _ in range(n_reps):\n",
        "        # a. Generate Rademacher weights for each cluster.\n",
        "        rademacher_weights = rng.choice([-1, 1], size=len(unique_clusters))\n",
        "        cluster_weights = pd.DataFrame({config[\"algorithm_config_parameters\"][\"CLUSTER_LEVEL\"]: unique_clusters, 'v': rademacher_weights})\n",
        "\n",
        "        # b. Create bootstrap residuals.\n",
        "        temp_df = fd_data.merge(cluster_weights, on=config[\"algorithm_config_parameters\"][\"CLUSTER_LEVEL\"], how='left')\n",
        "        boot_residuals = temp_df['v'].values * residuals.values\n",
        "\n",
        "        # c. Create bootstrap dependent variable.\n",
        "        boot_dependent = fitted_values.values.flatten() + boot_residuals\n",
        "\n",
        "        # d. Re-estimate the IV model (without robust SEs, as we build the distribution).\n",
        "        boot_model = IV2SLS(boot_dependent, exog, endog, instruments).fit()\n",
        "\n",
        "        # e. Store the coefficient of interest.\n",
        "        bootstrap_coeffs.append(boot_model.params['shock'])\n",
        "\n",
        "    # --- 6. Compute Percentile Confidence Interval ---\n",
        "\n",
        "    # Calculate alpha.\n",
        "    alpha = 1 - conf_level\n",
        "\n",
        "    # Compute bounds from the bootstrap distribution.\n",
        "    ci_lower = np.percentile(bootstrap_coeffs, 100 * alpha / 2)\n",
        "    ci_upper = np.percentile(bootstrap_coeffs, 100 * (1 - alpha / 2))\n",
        "\n",
        "    # --- 7. Assemble and Return Final Results ---\n",
        "\n",
        "    results = {\n",
        "        'point_estimate': model.params['shock'],\n",
        "        'cluster_robust_se': model.std_errors['shock'],\n",
        "        'p_value': model.pvalues['shock'],\n",
        "        'f_statistic': model.first_stage.f_statistic.stat,\n",
        "        'bootstrap_ci': (ci_lower, ci_upper),\n",
        "        'n_obs': int(model.nobs)\n",
        "    }\n",
        "\n",
        "    # Print a summary of the final result.\n",
        "    print(f\"[{task_name}] Estimated pure wage effect for entrants: {results['point_estimate']:.4f}\")\n",
        "\n",
        "    # Return the structured dictionary of results.\n",
        "    return results\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def analyze_non_employed_entrants(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    regional_panel: pd.DataFrame,\n",
        "    national_wage_series: pd.Series,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    event_year: int\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the analysis of non-employed entrants from 1990.\n",
        "\n",
        "    This function estimates the impact of immigration on two key outcomes for\n",
        "    this group: their probability of re-employment and the pure wage effect\n",
        "    for those who successfully re-enter the labor market.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        regional_panel (pd.DataFrame): The aggregated municipality-year panel.\n",
        "        national_wage_series (pd.Series): Series of national mean log wages.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        event_year (int): The specific year of the event study to analyze.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary containing the results for both\n",
        "                                   the employment entry and pure wage effect estimations.\n",
        "    \"\"\"\n",
        "    # Step 1: Identify the 1990 non-employed cohort and impute counterfactual wages.\n",
        "    entrants_df = _prepare_non_employed_entrants_data(\n",
        "        analysis_panel, national_wage_series, master_config\n",
        "    )\n",
        "\n",
        "    if entrants_df.empty:\n",
        "        print(\"Task 15: No valid non-employed entrants to analyze. Skipping.\")\n",
        "        return {'employment_entry_effect': {}, 'pure_wage_effect': {}}\n",
        "\n",
        "    # Step 2: Estimate the effect on the employment entry rate.\n",
        "    employment_results = _estimate_entrant_employment_effect(\n",
        "        entrants_df, analysis_panel, event_study_df, event_year, master_config\n",
        "    )\n",
        "\n",
        "    # Step 3: Estimate the pure wage effect for re-employed entrants.\n",
        "    wage_results = _estimate_entrant_wage_effect(\n",
        "        entrants_df, analysis_panel, event_study_df, event_year, master_config\n",
        "    )\n",
        "\n",
        "    final_results = {\n",
        "        'employment_entry_effect': employment_results,\n",
        "        'pure_wage_effect': wage_results\n",
        "    }\n",
        "\n",
        "    print(f\"\\nTask 15: Analysis of non-employed entrants for year {event_year} completed.\")\n",
        "    return final_results\n"
      ],
      "metadata": {
        "id": "ekLxRT29xffL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Older workers (50+) analysis (displacement and pure wages)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Older workers (50+) analysis (displacement and pure wages)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Step 1: Helper to Identify the Older Worker Cohort\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _identify_older_worker_cohort(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 16, Step 1\"\n",
        ") -> Set[int]:\n",
        "    \"\"\"\n",
        "    Identifies the cohort of \"older workers\" based on their age in the base year.\n",
        "\n",
        "    An older worker is defined as a native worker who was employed in 1990 and\n",
        "    was aged 50 or over in that year. This function returns the set of their\n",
        "    unique Worker_IDs for consistent filtering in subsequent analyses.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel from Task 5.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        Set[int]: A set of unique Worker_IDs for the older worker cohort.\n",
        "    \"\"\"\n",
        "    # Extract parameters from the configuration.\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "    age_threshold = config[\"sample_selection_parameters\"][\"OLDER_WORKER_AGE_THRESHOLD\"]\n",
        "\n",
        "    # Filter the panel to the base year.\n",
        "    panel_1990 = analysis_panel.loc[(slice(None), base_year), :]\n",
        "\n",
        "    # Identify the cohort based on the specified conditions.\n",
        "    older_worker_cohort = panel_1990[\n",
        "        (panel_1990['is_native']) &\n",
        "        (panel_1990['is_employed']) &\n",
        "        (panel_1990['age'] >= age_threshold)\n",
        "    ]\n",
        "\n",
        "    # Extract the unique Worker_IDs of this cohort.\n",
        "    older_worker_ids = set(older_worker_cohort.index.get_level_values('Worker_ID'))\n",
        "\n",
        "    print(f\"[{task_name}] Identified {len(older_worker_ids)} older workers (age >= {age_threshold} in {base_year}).\")\n",
        "\n",
        "    return older_worker_ids\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Step 2: Helper to Estimate Displacement Effect for Older Incumbents\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _estimate_older_worker_displacement(\n",
        "    older_worker_ids: Set[int],\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    event_year: int,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 16, Step 2\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Estimates the 2SLS effect on the displacement rate of older incumbents.\n",
        "\n",
        "    This function calculates the displacement rate for the pre-defined cohort\n",
        "    of older workers (age 50+ in 1990). The displacement rate for a municipality\n",
        "    is the share of its 1990 older incumbents who are non-employed by the event\n",
        "    year. It then estimates the causal effect of the immigration shock on this\n",
        "    rate using a weighted 2SLS model with a full wild cluster bootstrap.\n",
        "\n",
        "    Args:\n",
        "        older_worker_ids (Set[int]): Set of Worker_IDs for the older worker cohort.\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        event_year (int): The specific year of the event study to analyze.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary of estimation results, including point\n",
        "                        estimate, standard error, p-value, F-statistic, and\n",
        "                        bootstrap confidence interval. Returns an empty\n",
        "                        dictionary if the analysis cannot be run.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(older_worker_ids, set):\n",
        "        raise TypeError(\"`older_worker_ids` must be a set.\")\n",
        "    if not isinstance(analysis_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel` must be a pandas DataFrame.\")\n",
        "    if not isinstance(event_study_df, pd.DataFrame):\n",
        "        raise TypeError(\"`event_study_df` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- 1. Construct Outcome Variable: Displacement Share for Older Workers ---\n",
        "\n",
        "    # Extract the base year for comparison.\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "\n",
        "    # Filter the main panel to only the older worker cohort and the relevant years.\n",
        "    older_worker_panel = analysis_panel[\n",
        "        analysis_panel.index.get_level_values('Worker_ID').isin(older_worker_ids) &\n",
        "        analysis_panel.index.get_level_values('snapshot_year').isin([base_year, event_year])\n",
        "    ]\n",
        "\n",
        "    # Pivot to a wide format to track individual flows.\n",
        "    flow_df = older_worker_panel.reset_index().pivot_table(\n",
        "        index='Worker_ID',\n",
        "        columns='snapshot_year',\n",
        "        values=['is_employed', 'Municipality_ID', 'fte_weight']\n",
        "    )\n",
        "    flow_df.columns = [f'{val}_{year}' for val, year in flow_df.columns]\n",
        "\n",
        "    # Identify displacement: employed in base year and not employed in event year.\n",
        "    flow_df['is_displacement'] = (\n",
        "        flow_df[f'is_employed_{base_year}'].fillna(False) &\n",
        "        ~flow_df[f'is_employed_{event_year}'].fillna(False)\n",
        "    )\n",
        "\n",
        "    # Assign the FTE weight from the base year to displaced workers.\n",
        "    flow_df['displacement_fte'] = np.where(flow_df['is_displacement'], flow_df[f'fte_weight_{base_year}'], 0)\n",
        "\n",
        "    # Aggregate at the 1990 origin municipality level.\n",
        "    displacement_agg = flow_df.groupby(f'Municipality_ID_{base_year}').agg(\n",
        "        # The weight is the total number of older incumbents from that municipality.\n",
        "        weight_older_incumbents=('displacement_fte', 'size'),\n",
        "        # The numerator is the sum of FTEs of displaced older workers.\n",
        "        displaced_fte=('displacement_fte', 'sum')\n",
        "    )\n",
        "\n",
        "    # Calculate the outcome: Displacement_Share_50+_r = #{displaced FTE} / #{total older incumbents}.\n",
        "    displacement_agg['displacement_share_50plus'] = displacement_agg['displaced_fte'] / displacement_agg['weight_older_incumbents']\n",
        "\n",
        "    # --- 2. Prepare Data for 2SLS Estimation ---\n",
        "\n",
        "    # Merge the outcome and custom weight into the main event study data.\n",
        "    analysis_data = event_study_df.loc[(slice(None), event_year), :].copy()\n",
        "    analysis_data = analysis_data.merge(\n",
        "        displacement_agg, left_on='Municipality_ID', right_index=True, how='left'\n",
        "    ).fillna(0)\n",
        "\n",
        "    # Filter to the sample of municipalities that had older incumbents in 1990.\n",
        "    analysis_data = analysis_data[analysis_data['weight_older_incumbents'] > 0].copy()\n",
        "\n",
        "    if analysis_data.empty:\n",
        "        print(f\"[{task_name}] No municipalities with older incumbents found. Cannot estimate.\")\n",
        "        return {}\n",
        "\n",
        "    # --- 3. Full 2SLS + Bootstrap Estimation ---\n",
        "\n",
        "    # Define instruments and model formulas.\n",
        "    iv_names = list(config[\"algorithm_config_parameters\"][\"INSTRUMENT_SPECIFICATION\"].keys())\n",
        "    formula_1st = f\"shock ~ 1 + {' + '.join(iv_names)}\"\n",
        "    formula_2nd = \"displacement_share_50plus ~ 1 + shock_hat\"\n",
        "\n",
        "    # Estimate the first stage with custom weights.\n",
        "    first_stage = smf.wls(formula_1st, data=analysis_data, weights=analysis_data['weight_older_incumbents']).fit()\n",
        "    analysis_data['shock_hat'] = first_stage.predict(analysis_data)\n",
        "\n",
        "    # Estimate the second stage with custom weights.\n",
        "    second_stage = smf.wls(formula_2nd, data=analysis_data, weights=analysis_data['weight_older_incumbents']).fit()\n",
        "\n",
        "    # Compute cluster-robust standard errors.\n",
        "    cluster_col = config[\"algorithm_config_parameters\"][\"CLUSTER_LEVEL\"]\n",
        "    robust_results = second_stage.get_robustcov_results(cov_type='cluster', groups=analysis_data[cluster_col])\n",
        "\n",
        "    # --- 4. Wild Cluster Bootstrap ---\n",
        "\n",
        "    # Extract bootstrap parameters.\n",
        "    n_reps = config[\"algorithm_config_parameters\"][\"BOOTSTRAP_REPLICATIONS\"]\n",
        "    rng = np.random.default_rng(config[\"algorithm_config_parameters\"][\"RANDOM_SEED\"])\n",
        "    clusters = analysis_data[cluster_col].unique()\n",
        "\n",
        "    # Main bootstrap loop.\n",
        "    bootstrap_coeffs = []\n",
        "    for _ in range(n_reps):\n",
        "        # a. Generate Rademacher weights at the cluster level.\n",
        "        rademacher_weights = rng.choice([-1, 1], size=len(clusters))\n",
        "        cluster_weights = pd.DataFrame({cluster_col: clusters, 'v': rademacher_weights})\n",
        "\n",
        "        # b. Create bootstrap residuals and outcome.\n",
        "        boot_data = analysis_data.merge(cluster_weights, on=cluster_col, how='left')\n",
        "        boot_residuals = boot_data['v'] * second_stage.resid\n",
        "        boot_data['outcome_boot'] = second_stage.fittedvalues + boot_residuals\n",
        "\n",
        "        # c. Re-estimate the full 2SLS model.\n",
        "        fs_boot = smf.wls(formula_1st, data=boot_data, weights=boot_data['weight_older_incumbents']).fit()\n",
        "        boot_data['shock_hat_boot'] = fs_boot.predict(boot_data)\n",
        "        ss_boot = smf.wls(\"outcome_boot ~ 1 + shock_hat_boot\", data=boot_data, weights=boot_data['weight_older_incumbents']).fit()\n",
        "\n",
        "        # d. Store the coefficient of interest.\n",
        "        bootstrap_coeffs.append(ss_boot.params['shock_hat_boot'])\n",
        "\n",
        "    # --- 5. Compute Confidence Interval and Assemble Results ---\n",
        "\n",
        "    # Compute percentile confidence interval.\n",
        "    alpha = 1 - config[\"algorithm_config_parameters\"][\"BOOTSTRAP_CONF_LEVEL\"]\n",
        "    ci_lower = np.percentile(bootstrap_coeffs, 100 * alpha / 2)\n",
        "    ci_upper = np.percentile(bootstrap_coeffs, 100 * (1 - alpha / 2))\n",
        "\n",
        "    # Assemble the final results dictionary.\n",
        "    results = {\n",
        "        'point_estimate': robust_results.params['shock_hat'],\n",
        "        'cluster_robust_se': robust_results.bse['shock_hat'],\n",
        "        'p_value': robust_results.pvalues['shock_hat'],\n",
        "        'f_statistic': first_stage.f_test(iv_names).fvalue,\n",
        "        'bootstrap_ci': (ci_lower, ci_upper),\n",
        "        'n_obs': int(robust_results.nobs)\n",
        "    }\n",
        "\n",
        "    # Print a summary of the final result.\n",
        "    print(f\"[{task_name}] Estimated displacement effect for older workers: {results['point_estimate']:.4f}\")\n",
        "\n",
        "    # Return the structured dictionary of results.\n",
        "    return results\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Step 3: Helper to Estimate Pure Wage Effect for Older Stayers\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _estimate_older_stayer_wage_effect(\n",
        "    older_worker_ids: Set[int],\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    event_year: int,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 16, Step 3\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Estimates the pure wage effect for older stayers using an FD-IV model.\n",
        "\n",
        "    This function identifies \"stayers\" within the older worker cohort, constructs\n",
        "    a first-differenced dataset of their wages and age controls, and estimates\n",
        "    the effect of the immigration shock using an individual-level 2SLS model\n",
        "    with a full wild cluster bootstrap for robust inference.\n",
        "\n",
        "    Args:\n",
        "        older_worker_ids (Set[int]): Set of Worker_IDs for the older worker cohort.\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        event_year (int): The specific year of the event study to analyze.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary of estimation results. Returns an empty\n",
        "                        dictionary if the analysis cannot be run.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(older_worker_ids, set):\n",
        "        raise TypeError(\"`older_worker_ids` must be a set.\")\n",
        "    if not isinstance(analysis_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel` must be a pandas DataFrame.\")\n",
        "    if not isinstance(event_study_df, pd.DataFrame):\n",
        "        raise TypeError(\"`event_study_df` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- 1. Identify the Analysis Sample: Older Stayers ---\n",
        "\n",
        "    # Extract the base year for comparison.\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "\n",
        "    # Filter panel to relevant years.\n",
        "    stayers_panel = analysis_panel[\n",
        "        analysis_panel.index.get_level_values('snapshot_year').isin([base_year, event_year])\n",
        "    ]\n",
        "\n",
        "    # Pivot to identify stayers (workers in the same municipality in both years).\n",
        "    stayers_wide = stayers_panel.reset_index().pivot_table(\n",
        "        index='Worker_ID', columns='snapshot_year', values=['Municipality_ID', 'log_wage_imputed', 'age', 'age_sq']\n",
        "    )\n",
        "    stayers_wide.columns = [f'{val}_{year}' for val, year in stayers_wide.columns]\n",
        "    stayers_df = stayers_wide[\n",
        "        (stayers_wide[f'Municipality_ID_{base_year}'] == stayers_wide[f'Municipality_ID_{event_year}'])\n",
        "    ].dropna()\n",
        "\n",
        "    # Filter the stayers to include only those from the older worker cohort.\n",
        "    older_stayers_df = stayers_df[stayers_df.index.isin(older_worker_ids)].copy()\n",
        "    older_stayers_df.rename(columns={f'Municipality_ID_{base_year}': 'Municipality_ID'}, inplace=True)\n",
        "\n",
        "    if older_stayers_df.empty:\n",
        "        print(f\"[{task_name}] No older stayers found for wage analysis.\")\n",
        "        return {}\n",
        "\n",
        "    # --- 2. Construct First-Differenced (FD) Variables ---\n",
        "\n",
        "    # Dependent variable: Δlog_w = log_w_event - log_w_base\n",
        "    older_stayers_df['delta_log_wage'] = older_stayers_df[f'log_wage_imputed_{event_year}'] - older_stayers_df[f'log_wage_imputed_{base_year}']\n",
        "\n",
        "    # Control variable 1: Δage = age_event - age_base\n",
        "    older_stayers_df['delta_age'] = older_stayers_df[f'age_{event_year}'] - older_stayers_df[f'age_{base_year}']\n",
        "\n",
        "    # Control variable 2: Δage² = age_event² - age_base²\n",
        "    older_stayers_df['delta_age_sq'] = older_stayers_df[f'age_sq_{event_year}'] - older_stayers_df[f'age_sq_{base_year}']\n",
        "\n",
        "    # --- 3. Merge IVs and Cluster IDs ---\n",
        "\n",
        "    # Extract municipality-level variables for the event year.\n",
        "    event_data_year = event_study_df.loc[(slice(None), event_year), :].reset_index()\n",
        "    iv_cols = ['shock'] + list(config[\"algorithm_config_parameters\"][\"INSTRUMENT_SPECIFICATION\"].keys()) + [config[\"algorithm_config_parameters\"][\"CLUSTER_LEVEL\"]]\n",
        "\n",
        "    # Merge onto the individual-level FD data.\n",
        "    fd_data = older_stayers_df.merge(\n",
        "        event_data_year[['Municipality_ID'] + iv_cols], on='Municipality_ID', how='left'\n",
        "    ).dropna()\n",
        "\n",
        "    if fd_data.empty:\n",
        "        print(f\"[{task_name}] No valid observations remain after merging IVs. Cannot estimate.\")\n",
        "        return {}\n",
        "\n",
        "    # --- 4. Estimate FD-IV Model and Bootstrap ---\n",
        "\n",
        "    # Prepare data arrays for linearmodels.\n",
        "    dependent = fd_data['delta_log_wage']\n",
        "    exog = sm.add_constant(fd_data[['delta_age', 'delta_age_sq']])\n",
        "    endog = fd_data[['shock']]\n",
        "    instruments = fd_data[list(config[\"algorithm_config_parameters\"][\"INSTRUMENT_SPECIFICATION\"].keys())]\n",
        "    clusters = fd_data[config[\"algorithm_config_parameters\"][\"CLUSTER_LEVEL\"]]\n",
        "\n",
        "    # Fit the 2SLS model with cluster-robust standard errors.\n",
        "    model = IV2SLS(dependent, exog, endog, instruments).fit(cov_type='clustered', clusters=clusters)\n",
        "\n",
        "    # Bootstrap procedure (identical logic to previous tasks).\n",
        "    n_reps = config[\"algorithm_config_parameters\"][\"BOOTSTRAP_REPLICATIONS\"]\n",
        "    rng = np.random.default_rng(config[\"algorithm_config_parameters\"][\"RANDOM_SEED\"])\n",
        "    unique_clusters = clusters.unique()\n",
        "    bootstrap_coeffs = []\n",
        "    for _ in range(n_reps):\n",
        "        rademacher_weights = rng.choice([-1, 1], size=len(unique_clusters))\n",
        "        cluster_weights = pd.DataFrame({config[\"algorithm_config_parameters\"][\"CLUSTER_LEVEL\"]: unique_clusters, 'v': rademacher_weights})\n",
        "        temp_df = fd_data.merge(cluster_weights, on=config[\"algorithm_config_parameters\"][\"CLUSTER_LEVEL\"], how='left')\n",
        "        boot_residuals = temp_df['v'].values * model.resids.values\n",
        "        boot_dependent = model.predict().values.flatten() + boot_residuals\n",
        "        boot_model = IV2SLS(boot_dependent, exog, endog, instruments).fit()\n",
        "        bootstrap_coeffs.append(boot_model.params['shock'])\n",
        "\n",
        "    # --- 5. Compute Confidence Interval and Assemble Results ---\n",
        "\n",
        "    alpha = 1 - config[\"algorithm_config_parameters\"][\"BOOTSTRAP_CONF_LEVEL\"]\n",
        "    ci_lower = np.percentile(bootstrap_coeffs, 100 * alpha / 2)\n",
        "    ci_upper = np.percentile(bootstrap_coeffs, 100 * (1 - alpha / 2))\n",
        "\n",
        "    # Assemble the final results dictionary.\n",
        "    results = {\n",
        "        'point_estimate': model.params['shock'],\n",
        "        'cluster_robust_se': model.std_errors['shock'],\n",
        "        'p_value': model.pvalues['shock'],\n",
        "        'f_statistic': model.first_stage.f_statistic.stat,\n",
        "        'bootstrap_ci': (ci_lower, ci_upper),\n",
        "        'n_obs': int(model.nobs)\n",
        "    }\n",
        "\n",
        "    # Print a summary of the final result.\n",
        "    print(f\"[{task_name}] Estimated pure wage effect for older stayers: {results['point_estimate']:.4f}\")\n",
        "\n",
        "    # Return the structured dictionary of results.\n",
        "    return results\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def analyze_older_workers(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    regional_panel: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    event_year: int\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the heterogeneity analysis for older workers (age 50+).\n",
        "\n",
        "    This function estimates the impact of immigration on two key outcomes for\n",
        "    this group: their displacement rate and the pure wage effect for those\n",
        "    who remain employed (stayers).\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        regional_panel (pd.DataFrame): The aggregated municipality-year panel.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        event_year (int): The specific year of the event study to analyze.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary containing the results for both\n",
        "                                   the displacement and pure wage effect estimations.\n",
        "    \"\"\"\n",
        "    # Step 1: Identify the cohort of older workers based on their status in 1990.\n",
        "    older_worker_ids = _identify_older_worker_cohort(analysis_panel, master_config)\n",
        "\n",
        "    if not older_worker_ids:\n",
        "        print(\"Task 16: No older worker cohort found. Skipping analysis.\")\n",
        "        return {'displacement_effect': {}, 'pure_wage_effect': {}}\n",
        "\n",
        "    # Step 2: Estimate the displacement effect for this cohort.\n",
        "    displacement_results = _estimate_older_worker_displacement(\n",
        "        older_worker_ids, analysis_panel, event_study_df, event_year, master_config\n",
        "    )\n",
        "\n",
        "    # Step 3: Estimate the pure wage effect for the stayers within this cohort.\n",
        "    wage_results = _estimate_older_stayer_wage_effect(\n",
        "        older_worker_ids, analysis_panel, event_study_df, event_year, master_config\n",
        "    )\n",
        "\n",
        "    final_results = {\n",
        "        'displacement_effect': displacement_results,\n",
        "        'pure_wage_effect': wage_results\n",
        "    }\n",
        "\n",
        "    print(f\"\\nTask 16: Analysis of older workers for year {event_year} completed.\")\n",
        "    return final_results\n"
      ],
      "metadata": {
        "id": "FgO-h6EhyOWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17: Routine vs. abstract occupational analyses (employment and wages by task group)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Routine vs. abstract occupational analyses\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 17, Step 1: Helper to Compute Task-Specific Outcomes\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_task_specific_outcomes(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 17, Step 1\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates data to compute task-specific outcomes at the municipality-year level.\n",
        "\n",
        "    This function calculates employment levels and mean wages separately for\n",
        "    'Routine' and 'Abstract' occupations, computes their change relative to the\n",
        "    base year, and also calculates the change in the share of abstract employment.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by ('Municipality_ID', 'snapshot_year')\n",
        "                      containing all task-specific outcome variables.\n",
        "    \"\"\"\n",
        "    # Extract the base year for creating change-from-baseline outcomes.\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "\n",
        "    # Filter to native, employed workers who have a valid task classification.\n",
        "    task_panel = analysis_panel[\n",
        "        analysis_panel['is_native'] &\n",
        "        analysis_panel['is_employed'] &\n",
        "        analysis_panel['Routine_or_Abstract_Label'].notna()\n",
        "    ].copy()\n",
        "\n",
        "    # --- 1. Aggregate Employment and Wages by Task Group ---\n",
        "    # Define the aggregation operations to be performed on each group.\n",
        "    agg_funcs = {\n",
        "        'fte_weight': 'sum',\n",
        "        # For mean wage, we only consider full-time workers within each group.\n",
        "        'log_wage_imputed': lambda x: x[task_panel.loc[x.index, 'is_full_time']].mean()\n",
        "    }\n",
        "\n",
        "    # Group by municipality, year, and task label, then apply the aggregations.\n",
        "    task_agg = task_panel.groupby(\n",
        "        ['Municipality_ID', 'snapshot_year', 'Routine_or_Abstract_Label']\n",
        "    ).agg(agg_funcs).rename(columns={'log_wage_imputed': 'mean_log_wage'})\n",
        "\n",
        "    # --- 2. Reshape Data to Wide Format ---\n",
        "    # Unstack the task label level to create separate columns for 'Routine' and 'Abstract'.\n",
        "    task_regional = task_agg.unstack(level='Routine_or_Abstract_Label')\n",
        "\n",
        "    # Flatten the multi-level column index for easier access.\n",
        "    task_regional.columns = [f'{val}_{task}' for val, task in task_regional.columns]\n",
        "\n",
        "    # Ensure a balanced panel by reindexing and filling missing values with 0.\n",
        "    full_idx = pd.MultiIndex.from_product(\n",
        "        [analysis_panel['Municipality_ID'].dropna().unique(), analysis_panel.index.get_level_values('snapshot_year').unique()],\n",
        "        names=['Municipality_ID', 'snapshot_year']\n",
        "    )\n",
        "    task_regional = task_regional.reindex(full_idx).fillna(0)\n",
        "\n",
        "    # --- 3. Compute Outcomes Relative to Base Year ---\n",
        "\n",
        "    # Extract the baseline (1990) values for each task-specific measure.\n",
        "    baseline_values = task_regional.loc[(slice(None), base_year), :].copy()\n",
        "    baseline_values = baseline_values.reset_index(level='snapshot_year', drop=True).add_suffix('_base')\n",
        "\n",
        "    # Merge the baseline values back onto the panel.\n",
        "    outcomes_df = task_regional.merge(baseline_values, on='Municipality_ID', how='left').fillna(0)\n",
        "\n",
        "    # Calculate the change-from-baseline outcomes for each task group.\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        for task in ['Routine', 'Abstract']:\n",
        "            # Employment Outcome: (E_t - E_0) / E_0\n",
        "            outcomes_df[f'emp_outcome_{task}'] = np.divide(\n",
        "                outcomes_df[f'fte_weight_{task}'] - outcomes_df[f'fte_weight_{task}_base'],\n",
        "                outcomes_df[f'fte_weight_{task}_base']\n",
        "            )\n",
        "            # Wage Outcome: log(w_t) - log(w_0)\n",
        "            outcomes_df[f'wage_outcome_{task}'] = outcomes_df[f'mean_log_wage_{task}'] - outcomes_df[f'mean_log_wage_{task}_base']\n",
        "\n",
        "    # --- 4. Compute Abstract Share Outcome ---\n",
        "\n",
        "    # Calculate the share of abstract employment in total task-based employment for each year.\n",
        "    outcomes_df['abstract_share'] = np.divide(\n",
        "        outcomes_df['fte_weight_Abstract'],\n",
        "        outcomes_df['fte_weight_Abstract'] + outcomes_df['fte_weight_Routine']\n",
        "    )\n",
        "    # Calculate the baseline abstract share.\n",
        "    outcomes_df['abstract_share_base'] = np.divide(\n",
        "        outcomes_df['fte_weight_Abstract_base'],\n",
        "        outcomes_df['fte_weight_Abstract_base'] + outcomes_df['fte_weight_Routine_base']\n",
        "    )\n",
        "    # The outcome is the change in this share from the baseline.\n",
        "    outcomes_df['abstract_share_outcome'] = outcomes_df['abstract_share'] - outcomes_df['abstract_share_base']\n",
        "\n",
        "    # Clean up any inf/-inf values and set base year outcomes to exactly 0.\n",
        "    outcomes_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    outcomes_df.loc[outcomes_df.index.get_level_values('snapshot_year') == base_year,\n",
        "                    [c for c in outcomes_df.columns if 'outcome' in c]] = 0.0\n",
        "\n",
        "    print(f\"[{task_name}] Task-specific outcomes computed successfully.\")\n",
        "    return outcomes_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 17, Steps 2 & 3: Orchestrator for Task Heterogeneity Analysis\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def analyze_task_heterogeneity(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    event_year: int\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the heterogeneity analysis by occupation task type.\n",
        "\n",
        "    This function estimates the causal impact of immigration separately for\n",
        "    Routine and Abstract occupations on both regional employment and pure wages.\n",
        "    It also estimates the effect on the regional share of abstract employment.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        event_year (int): The specific year of the event study to analyze.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary containing the estimation results\n",
        "                        for each task-specific analysis.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(analysis_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel` must be a pandas DataFrame.\")\n",
        "    if not isinstance(event_study_df, pd.DataFrame):\n",
        "        raise TypeError(\"`event_study_df` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Step 1: Compute Task-Specific Outcomes ---\n",
        "    task_outcomes_df = _compute_task_specific_outcomes(analysis_panel, master_config)\n",
        "\n",
        "    # Prepare a consistent analysis dataset for the event year by merging outcomes.\n",
        "    analysis_data_year = event_study_df.loc[(slice(None), event_year), :].copy()\n",
        "    analysis_data_year = analysis_data_year.merge(\n",
        "        task_outcomes_df, on=['Municipality_ID', 'snapshot_year'], how='left'\n",
        "    )\n",
        "\n",
        "    # Initialize the results dictionary.\n",
        "    results: Dict[str, Any] = {'Routine': {}, 'Abstract': {}}\n",
        "\n",
        "    # --- Step 2: Estimate Task-Specific Regional Employment and Pure Wage Effects ---\n",
        "    for task in ['Routine', 'Abstract']:\n",
        "\n",
        "        # --- Regional Employment Effect ---\n",
        "        print(f\"\\n--- Estimating Regional Employment Effect for '{task}' tasks ---\")\n",
        "\n",
        "        # Define the specific outcome and weight columns for this regression.\n",
        "        outcome_col = f'emp_outcome_{task}'\n",
        "        weight_col = f'fte_weight_{task}_base'\n",
        "\n",
        "        # Prepare the data sample for this specific estimation.\n",
        "        temp_df = analysis_data_year.dropna(subset=[outcome_col, weight_col])\n",
        "        temp_df = temp_df[temp_df[weight_col] > 0].copy()\n",
        "\n",
        "        # Create a temporary config to pass the custom weight column name.\n",
        "        temp_config = master_config.copy()\n",
        "        temp_config['algorithm_config_parameters'] = master_config['algorithm_config_parameters'].copy()\n",
        "        temp_config['algorithm_config_parameters']['WEIGHT_COLUMN_MUNICIPALITY'] = weight_col\n",
        "\n",
        "        # Call the master 2SLS estimator.\n",
        "        results[task]['regional_employment'] = estimate_regional_effect_2sls(\n",
        "            event_study_df=temp_df.set_index('Municipality_ID', append=True).swaplevel(0,1),\n",
        "            master_config=temp_config,\n",
        "            outcome_variable=outcome_col,\n",
        "            event_year=event_year\n",
        "        )\n",
        "\n",
        "        # --- Pure Wage Effect ---\n",
        "        print(f\"\\n--- Estimating Pure Wage Effect for '{task}' tasks ---\")\n",
        "\n",
        "        # Define the population: workers who were in the specified task group in the base year.\n",
        "        base_year = master_config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "        task_cohort_ids = set(\n",
        "            analysis_panel[\n",
        "                (analysis_panel.index.get_level_values('snapshot_year') == base_year) &\n",
        "                (analysis_panel['Routine_or_Abstract_Label'] == task)\n",
        "            ].index.get_level_values('Worker_ID')\n",
        "        )\n",
        "\n",
        "        # Call the FD-IV estimator for older workers, which is generic enough to be reused here.\n",
        "        # We pass the IDs of the relevant cohort to filter the stayer sample.\n",
        "        results[task]['pure_wage'] = _estimate_older_stayer_wage_effect(\n",
        "            older_worker_ids=task_cohort_ids, # Reusing the argument name, but it's just a set of IDs\n",
        "            analysis_panel=analysis_panel,\n",
        "            event_study_df=event_study_df,\n",
        "            event_year=event_year,\n",
        "            config=master_config,\n",
        "            task_name=f\"Task 17, Pure Wage ({task})\"\n",
        "        )\n",
        "\n",
        "    # --- Step 3: Estimate Effect on Abstract Share ---\n",
        "    print(\"\\n--- Estimating Effect on Abstract Employment Share ---\")\n",
        "\n",
        "    # Define the outcome and use the main analysis weight.\n",
        "    outcome_col = 'abstract_share_outcome'\n",
        "    weight_col = master_config[\"algorithm_config_parameters\"][\"WEIGHT_COLUMN_MUNICIPALITY\"]\n",
        "\n",
        "    # Prepare the data sample.\n",
        "    temp_df = analysis_data_year.dropna(subset=[outcome_col, weight_col])\n",
        "    temp_df = temp_df[temp_df[weight_col] > 0].copy()\n",
        "\n",
        "    # Call the master 2SLS estimator with the original config.\n",
        "    results['abstract_share'] = estimate_regional_effect_2sls(\n",
        "        event_study_df=temp_df.set_index('Municipality_ID', append=True).swaplevel(0,1),\n",
        "        master_config=master_config,\n",
        "        outcome_variable=outcome_col,\n",
        "        event_year=event_year\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTask 17: Analysis of task heterogeneity for year {event_year} completed.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "fqklDx4i1IUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18: Routine employment decomposition with within-region occupational switches (Equation (8))\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Routine employment decomposition with occupational switches\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Step 1: Helper to Compute Routine Employment Decomposition Components\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_routine_employment_decomposition(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    event_year: int,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 18, Step 1\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the full decomposition of routine employment change per Equation (8).\n",
        "\n",
        "    This function tracks workers' transitions in both location and task type\n",
        "    between the base year and the event year to construct all flow components,\n",
        "    including within-region upgrading (Routine -> Abstract) and downgrading\n",
        "    (Abstract -> Routine).\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        event_year (int): The post-treatment year to compare against the base year.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by 'Municipality_ID' with columns for\n",
        "                      each decomposition component share and the baseline routine\n",
        "                      employment count used for weighting.\n",
        "    \"\"\"\n",
        "    # Extract the base year from the configuration.\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "\n",
        "    # --- 1. Prepare Wide-Format Data for Flow Analysis ---\n",
        "\n",
        "    # Filter to native workers and the two relevant years.\n",
        "    native_panel = analysis_panel[\n",
        "        analysis_panel['is_native'] &\n",
        "        analysis_panel.index.get_level_values('snapshot_year').isin([base_year, event_year])\n",
        "    ].copy()\n",
        "\n",
        "    # Pivot to a wide format for individual-level flow classification.\n",
        "    flow_df = native_panel.reset_index().pivot_table(\n",
        "        index='Worker_ID',\n",
        "        columns='snapshot_year',\n",
        "        values=['is_employed', 'Municipality_ID', 'fte_weight', 'Routine_or_Abstract_Label']\n",
        "    )\n",
        "    # Flatten the multi-level column index.\n",
        "    flow_df.columns = [f'{val}_{year}' for val, year in flow_df.columns]\n",
        "\n",
        "    # For clarity and robustness, explicitly fill NaNs in status columns.\n",
        "    for year in [base_year, event_year]:\n",
        "        flow_df[f'is_employed_{year}'].fillna(False, inplace=True)\n",
        "        flow_df[f'Routine_or_Abstract_Label_{year}'].fillna('NonEmployed', inplace=True)\n",
        "\n",
        "    # --- 2. Classify Flows and Assign FTE Contributions ---\n",
        "\n",
        "    # Define convenient column name variables.\n",
        "    muni_base, muni_event = f'Municipality_ID_{base_year}', f'Municipality_ID_{event_year}'\n",
        "    task_base, task_event = f'Routine_or_Abstract_Label_{base_year}', f'Routine_or_Abstract_Label_{event_year}'\n",
        "    fte_base, fte_event = f'fte_weight_{base_year}', f'fte_weight_{event_year}'\n",
        "\n",
        "    # a. Displacement (from Routine): R_r,0 -> N_1\n",
        "    # A worker is displaced from a routine job if they were in a routine job in 1990 and non-employed in the event year.\n",
        "    # The contribution is their 1990 FTE weight.\n",
        "    flow_df['displacement_R_fte'] = np.where(\n",
        "        (flow_df[task_base] == 'Routine') & (flow_df[task_event] == 'NonEmployed'),\n",
        "        flow_df[fte_base], 0\n",
        "    )\n",
        "    # b. Relocation (from Routine): R_r,0 -> (R/A)_r',1\n",
        "    # A worker relocates from a routine job if they were in a routine job in 1990 and employed in a different municipality in the event year.\n",
        "    flow_df['relocation_R_fte'] = np.where(\n",
        "        (flow_df[task_base] == 'Routine') & (flow_df[task_event] != 'NonEmployed') & (flow_df[muni_base] != flow_df[muni_event]),\n",
        "        flow_df[fte_base], 0\n",
        "    )\n",
        "    # c. Upgrading: R_r,0 -> A_r,1\n",
        "    # A worker upgrades if they were in a routine job in 1990 and an abstract job in the event year, within the same municipality.\n",
        "    flow_df['upgrade_R_to_A_fte'] = np.where(\n",
        "        (flow_df[task_base] == 'Routine') & (flow_df[task_event] == 'Abstract') & (flow_df[muni_base] == flow_df[muni_event]),\n",
        "        flow_df[fte_base], 0\n",
        "    )\n",
        "    # d. Downgrading: A_r,0 -> R_r,1\n",
        "    # A worker downgrades if they were in an abstract job in 1990 and a routine job in the event year, within the same municipality.\n",
        "    flow_df['downgrade_A_to_R_fte'] = np.where(\n",
        "        (flow_df[task_base] == 'Abstract') & (flow_df[task_event] == 'Routine') & (flow_df[muni_base] == flow_df[muni_event]),\n",
        "        flow_df[fte_event], 0\n",
        "    )\n",
        "    # e. Inflows to Routine: (N/A/R_r')_0 -> R_r,1\n",
        "    # A worker is an inflower to a routine job if they are in a routine job in the event year but were not in a routine job in that same municipality in 1990.\n",
        "    flow_df['inflow_to_R_fte'] = np.where(\n",
        "        (flow_df[task_event] == 'Routine') &\n",
        "        ((flow_df[task_base] != 'Routine') | (flow_df[muni_base] != flow_df[muni_event])),\n",
        "        flow_df[fte_event], 0\n",
        "    )\n",
        "\n",
        "    # --- 3. Aggregate Flows and Normalize ---\n",
        "\n",
        "    # The denominator for all shares is the baseline (1990) native routine employment in each municipality.\n",
        "    base_routine_employment = analysis_panel[\n",
        "        (analysis_panel.index.get_level_values('snapshot_year') == base_year) &\n",
        "        (analysis_panel['Routine_or_Abstract_Label'] == 'Routine') &\n",
        "        (analysis_panel['is_native'])\n",
        "    ].groupby('Municipality_ID')['fte_weight'].sum()\n",
        "    base_routine_employment.name = 'routine_fte_employment_base'\n",
        "\n",
        "    # Aggregate the FTE contributions for each flow type by the relevant municipality.\n",
        "    displacement_agg = flow_df.groupby(muni_base)['displacement_R_fte'].sum()\n",
        "    relocation_agg = flow_df.groupby(muni_base)['relocation_R_fte'].sum()\n",
        "    upgrade_agg = flow_df.groupby(muni_base)['upgrade_R_to_A_fte'].sum()\n",
        "    inflow_agg = flow_df.groupby(muni_event)['inflow_to_R_fte'].sum()\n",
        "    downgrade_agg = flow_df.groupby(muni_base)['downgrade_A_to_R_fte'].sum()\n",
        "\n",
        "    # The \"crowding-out\" effect is defined as the total inflow to routine jobs minus the within-municipality downgrades.\n",
        "    crowding_out_agg = inflow_agg - downgrade_agg\n",
        "\n",
        "    # Assemble all aggregated components into a single DataFrame.\n",
        "    components_df = pd.DataFrame({\n",
        "        'displacement_share': displacement_agg,\n",
        "        'relocation_share': relocation_agg,\n",
        "        'upgrade_share': upgrade_agg,\n",
        "        'crowding_out_share': crowding_out_agg,\n",
        "        'downgrade_share': downgrade_agg\n",
        "    })\n",
        "\n",
        "    # Normalize all flow shares by the baseline routine employment.\n",
        "    components_df = components_df.divide(base_routine_employment, axis=0)\n",
        "\n",
        "    # Add the baseline routine employment itself, which will be used as the weight in regressions.\n",
        "    components_df['routine_fte_employment_base'] = base_routine_employment\n",
        "\n",
        "    # --- 4. Internal Consistency Check ---\n",
        "\n",
        "    # Calculate the total change in routine employment directly from the panel.\n",
        "    total_routine_employment = analysis_panel[\n",
        "        (analysis_panel['Routine_or_Abstract_Label'] == 'Routine') & (analysis_panel['is_native'])\n",
        "    ].groupby(['Municipality_ID', 'snapshot_year'])['fte_weight'].sum().unstack().fillna(0)\n",
        "\n",
        "    # Calculate the percentage change relative to the baseline.\n",
        "    components_df['total_routine_emp_change'] = (\n",
        "        (total_routine_employment[event_year] - total_routine_employment[base_year])\n",
        "    ).divide(base_routine_employment)\n",
        "\n",
        "    # Reconstruct the total change from the components based on the identity in Equation (8).\n",
        "    components_df['identity_check'] = (\n",
        "        -components_df['displacement_share'] +\n",
        "        components_df['crowding_out_share'] -\n",
        "        components_df['relocation_share'] -\n",
        "        components_df['upgrade_share'] +\n",
        "        components_df['downgrade_share']\n",
        "    )\n",
        "\n",
        "    # Assert that the identity holds within a small tolerance for floating-point errors.\n",
        "    if not np.allclose(\n",
        "        components_df['total_routine_emp_change'].fillna(0),\n",
        "        components_df['identity_check'].fillna(0),\n",
        "        atol=1e-8,\n",
        "        equal_nan=True\n",
        "    ):\n",
        "        raise AssertionError(f\"[{task_name}] Internal decomposition identity check failed.\")\n",
        "\n",
        "    print(f\"[{task_name}] Routine employment decomposition components computed and verified.\")\n",
        "\n",
        "    # Return the final components, dropping the temporary check column and filling NaNs.\n",
        "    return components_df.drop(columns=['identity_check']).fillna(0)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# Task 18, Helper for estimating change in abstract intensity for regional stayers\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "def _estimate_regional_stayer_intensity_effect(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    event_year: int,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 18, Step 3\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Estimates the effect on abstract intensity for regional stayers (FD-IV).\n",
        "\n",
        "    This function provides a continuous measure of occupational upgrading. It\n",
        "    focuses on \"regional stayers\" (workers employed in the same municipality\n",
        "    in the base and event years, regardless of occupation) and estimates the\n",
        "    causal effect of the immigration shock on the change in their occupation's\n",
        "    abstract intensity score.\n",
        "\n",
        "    The estimation uses a First-Difference Instrumental Variable (FD-IV) model,\n",
        "    instrumenting the shock with distance variables. Inference is made robust\n",
        "    through a full wild cluster bootstrap procedure.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        event_year (int): The specific year of the event study to analyze.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for clear error reporting.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary of estimation results, including point\n",
        "                        estimate, standard error, p-value, F-statistic, and\n",
        "                        bootstrap confidence interval. Returns an empty\n",
        "                        dictionary if the analysis cannot be run.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(analysis_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel` must be a pandas DataFrame.\")\n",
        "    if not isinstance(event_study_df, pd.DataFrame):\n",
        "        raise TypeError(\"`event_study_df` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- 1. Identify Regional Stayers and Prepare Data ---\n",
        "\n",
        "    # Extract the base year for comparison.\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "\n",
        "    # Filter panel to native workers in the two relevant years.\n",
        "    stayers_panel = analysis_panel[\n",
        "        analysis_panel['is_native'] &\n",
        "        analysis_panel.index.get_level_values('snapshot_year').isin([base_year, event_year])\n",
        "    ]\n",
        "\n",
        "    # Pivot to wide format to identify regional stayers.\n",
        "    stayers_wide = stayers_panel.reset_index().pivot_table(\n",
        "        index='Worker_ID',\n",
        "        columns='snapshot_year',\n",
        "        values=['Municipality_ID', 'Abstract_Intensity']\n",
        "    )\n",
        "    stayers_wide.columns = [f'{val}_{year}' for val, year in stayers_wide.columns]\n",
        "\n",
        "    # A regional stayer was employed in the same municipality in both years.\n",
        "    # We also require non-missing intensity scores in both periods.\n",
        "    stayers_df = stayers_wide[\n",
        "        (stayers_wide[f'Municipality_ID_{base_year}'] == stayers_wide[f'Municipality_ID_{event_year}'])\n",
        "    ].dropna(subset=[\n",
        "        f'Municipality_ID_{base_year}',\n",
        "        f'Abstract_Intensity_{base_year}',\n",
        "        f'Abstract_Intensity_{event_year}'\n",
        "    ])\n",
        "\n",
        "    # Rename the municipality column for merging.\n",
        "    stayers_df.rename(columns={f'Municipality_ID_{base_year}': 'Municipality_ID'}, inplace=True)\n",
        "\n",
        "    if stayers_df.empty:\n",
        "        print(f\"[{task_name}] No regional stayers with valid intensity scores found.\")\n",
        "        return {}\n",
        "\n",
        "    # --- 2. Construct First-Differenced (FD) Outcome Variable ---\n",
        "\n",
        "    # The dependent variable is the change in the abstract intensity score.\n",
        "    # ΔAbstract_Intensity = Abstract_Intensity_event - Abstract_Intensity_base\n",
        "    stayers_df['delta_abstract_intensity'] = stayers_df[f'Abstract_Intensity_{event_year}'] - stayers_df[f'Abstract_Intensity_{base_year}']\n",
        "\n",
        "    # --- 3. Merge Municipality-Level IVs and Cluster IDs ---\n",
        "\n",
        "    # Extract the relevant slice from the event study data.\n",
        "    event_data_year = event_study_df.loc[(slice(None), event_year), :].reset_index()\n",
        "\n",
        "    # Define the columns to merge: shock, instruments, and cluster ID.\n",
        "    cluster_col = config[\"algorithm_config_parameters\"][\"CLUSTER_LEVEL\"]\n",
        "    iv_names = list(config[\"algorithm_config_parameters\"][\"INSTRUMENT_SPECIFICATION\"].keys())\n",
        "    cols_to_merge = ['Municipality_ID', 'shock'] + iv_names + [cluster_col]\n",
        "\n",
        "    # Merge these variables onto the individual-level FD data.\n",
        "    fd_data = stayers_df.merge(\n",
        "        event_data_year[cols_to_merge], on='Municipality_ID', how='left'\n",
        "    ).dropna()\n",
        "\n",
        "    if fd_data.empty:\n",
        "        print(f\"[{task_name}] No valid observations remain after merging IVs. Cannot estimate.\")\n",
        "        return {}\n",
        "\n",
        "    # --- 4. Estimate the FD-IV Model ---\n",
        "\n",
        "    # Prepare data arrays for linearmodels. This model only has an intercept as exog.\n",
        "    dependent = fd_data['delta_abstract_intensity']\n",
        "    exog = sm.add_constant(fd_data[['Municipality_ID']]).drop(columns=['Municipality_ID']) # Creates a constant\n",
        "    endog = fd_data[['shock']]\n",
        "    instruments = fd_data[iv_names]\n",
        "    clusters = fd_data[cluster_col]\n",
        "\n",
        "    # Fit the 2SLS model with cluster-robust standard errors.\n",
        "    model = IV2SLS(dependent, exog, endog, instruments).fit(cov_type='clustered', clusters=clusters)\n",
        "\n",
        "    # --- 5. Implement Wild Cluster Bootstrap ---\n",
        "\n",
        "    # Extract bootstrap parameters.\n",
        "    n_reps = config[\"algorithm_config_parameters\"][\"BOOTSTRAP_REPLICATIONS\"]\n",
        "    rng = np.random.default_rng(config[\"algorithm_config_parameters\"][\"RANDOM_SEED\"])\n",
        "\n",
        "    # Get unique clusters and model residuals.\n",
        "    unique_clusters = clusters.unique()\n",
        "    fitted_values = model.predict()\n",
        "    residuals = model.resids\n",
        "\n",
        "    # Store bootstrap coefficients.\n",
        "    bootstrap_coeffs = []\n",
        "\n",
        "    # Main bootstrap loop.\n",
        "    for _ in range(n_reps):\n",
        "        # a. Generate Rademacher weights for each cluster.\n",
        "        rademacher_weights = rng.choice([-1, 1], size=len(unique_clusters))\n",
        "        cluster_weights = pd.DataFrame({cluster_col: unique_clusters, 'v': rademacher_weights})\n",
        "\n",
        "        # b. Create bootstrap residuals and outcome.\n",
        "        temp_df = fd_data.merge(cluster_weights, on=cluster_col, how='left')\n",
        "        boot_residuals = temp_df['v'].values * residuals.values\n",
        "        boot_dependent = fitted_values.values.flatten() + boot_residuals\n",
        "\n",
        "        # c. Re-estimate the IV model.\n",
        "        boot_model = IV2SLS(boot_dependent, exog, endog, instruments).fit()\n",
        "\n",
        "        # d. Store the coefficient of interest.\n",
        "        bootstrap_coeffs.append(boot_model.params['shock'])\n",
        "\n",
        "    # --- 6. Compute Confidence Interval and Assemble Results ---\n",
        "\n",
        "    alpha = 1 - config[\"algorithm_config_parameters\"][\"BOOTSTRAP_CONF_LEVEL\"]\n",
        "    ci_lower = np.percentile(bootstrap_coeffs, 100 * alpha / 2)\n",
        "    ci_upper = np.percentile(bootstrap_coeffs, 100 * (1 - alpha / 2))\n",
        "\n",
        "    # Assemble the final results dictionary.\n",
        "    results = {\n",
        "        'point_estimate': model.params['shock'],\n",
        "        'cluster_robust_se': model.std_errors['shock'],\n",
        "        'p_value': model.pvalues['shock'],\n",
        "        'f_statistic': model.first_stage.f_statistic.stat,\n",
        "        'bootstrap_ci': (ci_lower, ci_upper),\n",
        "        'n_obs': int(model.nobs)\n",
        "    }\n",
        "\n",
        "    # Print a summary of the final result.\n",
        "    print(f\"[{task_name}] Estimated abstract intensity effect for stayers: {results['point_estimate']:.4f}\")\n",
        "\n",
        "    # Return the structured dictionary of results.\n",
        "    return results\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def decompose_routine_employment(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    event_year: int\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full decomposition of the routine employment effect.\n",
        "\n",
        "    This function provides a comprehensive analysis of how regional routine\n",
        "    employment adjusts to an immigration shock, based on the decomposition in\n",
        "    Equation (8) of the paper. It performs three main operations:\n",
        "\n",
        "    1.  **Component Construction**: It first calls a helper to compute the shares\n",
        "        of all micro-level flows that constitute the change in routine employment.\n",
        "        This includes displacement, crowding-out, relocation, and, critically,\n",
        "        within-region occupational upgrading (Routine -> Abstract) and\n",
        "        downgrading (Abstract -> Routine).\n",
        "\n",
        "    2.  **Causal Estimation (Discrete Flows)**: It systematically estimates the\n",
        "        causal effect of the immigration shock on each of these discrete flow\n",
        "        components using a robust 2SLS procedure with wild cluster bootstrapping.\n",
        "        These estimations are weighted by the baseline routine employment in each\n",
        "        municipality.\n",
        "\n",
        "    3.  **Causal Estimation (Continuous Upgrading)**: As a complementary test, it\n",
        "        estimates the effect of the shock on the change in the continuous\n",
        "        'Abstract_Intensity' measure for regional stayers, using an individual-level\n",
        "        FD-IV model.\n",
        "\n",
        "    Finally, it performs a rigorous identity check to ensure that the sum of the\n",
        "    estimated effects on the components (with appropriate signs) equals the\n",
        "    estimated total effect on routine employment.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel from Task 5.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        event_year (int): The specific year of the event study to analyze.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary containing the detailed estimation\n",
        "                        results for the total routine effect, each of the five\n",
        "                        decomposition components, and the abstract intensity change.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If input arguments are not of the expected type.\n",
        "        AssertionError: If the final decomposition identity check on the\n",
        "                        estimated coefficients fails, indicating a logical or\n",
        "                        sample inconsistency in the analysis.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(analysis_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel` must be a pandas DataFrame.\")\n",
        "    if not isinstance(event_study_df, pd.DataFrame):\n",
        "        raise TypeError(\"`event_study_df` must be a pandas DataFrame.\")\n",
        "    if not isinstance(master_config, dict):\n",
        "        raise TypeError(\"`master_config` must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Compute all flow components based on Equation (8) ---\n",
        "    # This helper function performs the complex data wrangling to create the outcome variables.\n",
        "    components_df = _compute_routine_employment_decomposition(analysis_panel, event_year, master_config)\n",
        "\n",
        "    # --- Step 2: Prepare a consistent analysis dataset for the event year ---\n",
        "    # Merge the newly computed component shares into the main event study data.\n",
        "    analysis_data_year = event_study_df.loc[(slice(None), event_year), :].copy()\n",
        "    analysis_data_year = analysis_data_year.merge(\n",
        "        components_df, on='Municipality_ID', how='left'\n",
        "    ).fillna(0)\n",
        "\n",
        "    # --- Step 3: Estimate 2SLS for each discrete flow component ---\n",
        "\n",
        "    # Initialize a dictionary to store the results of each estimation.\n",
        "    results: Dict[str, Any] = {}\n",
        "\n",
        "    # Define the mapping from the result key to the outcome column name in the DataFrame.\n",
        "    outcomes_to_estimate = {\n",
        "        'total_routine_effect': 'total_routine_emp_change',\n",
        "        'displacement': 'displacement_share',\n",
        "        'crowding_out': 'crowding_out_share',\n",
        "        'relocation': 'relocation_share',\n",
        "        'upgrading': 'upgrade_share',\n",
        "        'downgrading': 'downgrade_share'\n",
        "    }\n",
        "\n",
        "    # The weight for all these regressions is the baseline ROUTINE employment.\n",
        "    weight_col = 'routine_fte_employment_base'\n",
        "\n",
        "    # Create a temporary copy of the config to modify the weight column for the estimator.\n",
        "    # This is a clean way to parameterize the reusable estimation function.\n",
        "    temp_config = master_config.copy()\n",
        "    temp_config['algorithm_config_parameters'] = master_config['algorithm_config_parameters'].copy()\n",
        "    temp_config['algorithm_config_parameters']['WEIGHT_COLUMN_MUNICIPALITY'] = weight_col\n",
        "\n",
        "    # Loop through each outcome and run the full 2SLS estimation.\n",
        "    for name, outcome_col in outcomes_to_estimate.items():\n",
        "        # Define the analysis sample for this specific regression.\n",
        "        # We only use municipalities that had routine workers at baseline.\n",
        "        temp_df = analysis_data_year[analysis_data_year[weight_col] > 0].copy()\n",
        "\n",
        "        # Call the master 2SLS estimation function from Task 10.\n",
        "        results[name] = estimate_regional_effect_2sls(\n",
        "            event_study_df=temp_df.set_index('Municipality_ID', append=True).swaplevel(0,1),\n",
        "            master_config=temp_config,\n",
        "            outcome_variable=outcome_col,\n",
        "            event_year=event_year\n",
        "        )\n",
        "\n",
        "    # --- Step 4: Estimate change in abstract intensity for regional stayers ---\n",
        "\n",
        "    # This provides a continuous measure of upgrading.\n",
        "    results['abstract_intensity'] = _estimate_regional_stayer_intensity_effect(\n",
        "        analysis_panel=analysis_panel,\n",
        "        event_study_df=event_study_df,\n",
        "        event_year=event_year,\n",
        "        config=master_config\n",
        "    )\n",
        "\n",
        "    # --- Step 5: Final Identity Check on Estimated Coefficients ---\n",
        "    # This is a critical validation step to ensure the entire decomposition is consistent.\n",
        "    # Equation (8): ΔE^R ≈ -Disp + Crowd-Out - Reloc - Upgrade + Downgrade\n",
        "\n",
        "    # Extract the point estimate for the total effect on routine employment.\n",
        "    total_effect = results['total_routine_effect']['point_estimate']\n",
        "\n",
        "    # Reconstruct the total effect from the sum of the component effects.\n",
        "    sum_of_components = (\n",
        "        -results['displacement']['point_estimate'] +\n",
        "        results['crowding_out']['point_estimate'] -\n",
        "        results['relocation']['point_estimate'] -\n",
        "        results['upgrading']['point_estimate'] +\n",
        "        results['downgrading']['point_estimate']\n",
        "    )\n",
        "\n",
        "    # Assert that the directly estimated total effect is numerically close to the reconstructed sum.\n",
        "    if not np.isclose(total_effect, sum_of_components, atol=1e-6):\n",
        "        # If this fails, it indicates a serious logical error in the component construction or sample definition.\n",
        "        raise AssertionError(\n",
        "            \"Routine decomposition coefficient identity FAILED! \"\n",
        "            f\"Directly Estimated Total Effect = {total_effect:.6f}, but \"\n",
        "            f\"Sum of Component Effects = {sum_of_components:.6f}.\"\n",
        "        )\n",
        "    else:\n",
        "        # Log the successful validation.\n",
        "        print(\"\\nRoutine decomposition coefficient identity PASSED.\")\n",
        "\n",
        "    # Return the comprehensive dictionary of results.\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "ovzZ8XcTFAsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19: Apprenticeship uptake analysis (Figure 3)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Apprenticeship uptake analysis (Figure 3)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Steps 1 & 2: Helper to Compute Apprenticeship Outcomes\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_apprenticeship_outcomes(\n",
        "    analysis_panel_full: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 19, Steps 1 & 2\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the change in native apprenticeship employment relative to baseline.\n",
        "\n",
        "    This function first aggregates the FTE count of native apprentices for each\n",
        "    municipality-year from the full, unfiltered worker panel. It then calculates\n",
        "    the percentage change of this count for each year relative to the 1990 baseline.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel_full (pd.DataFrame): The complete, unfiltered worker-year\n",
        "            panel from Task 3, which includes apprentice flags.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        task_name (str): The name of the calling task for clear error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by ('Municipality_ID', 'snapshot_year')\n",
        "                      containing the 'apprenticeship_outcome' variable.\n",
        "    \"\"\"\n",
        "    # --- 1. Compute Municipality-Year Apprentice Counts ---\n",
        "\n",
        "    # Filter the full panel to native apprentices. The 'is_apprentice' flag\n",
        "    # was created on the unfiltered panel in Task 3, which is critical here.\n",
        "    apprentice_panel = analysis_panel_full[\n",
        "        analysis_panel_full['is_native'] & analysis_panel_full['is_apprentice']\n",
        "    ].copy()\n",
        "\n",
        "    # Group by municipality and year, and sum the FTE weights.\n",
        "    apprentice_counts = apprentice_panel.groupby(\n",
        "        ['Municipality_ID', 'snapshot_year']\n",
        "    )['fte_weight'].sum().rename('apprenticeship_fte')\n",
        "\n",
        "    # --- 2. Compute Percent Changes Relative to Base Year ---\n",
        "\n",
        "    # Create a balanced panel of apprentice counts by reindexing.\n",
        "    full_idx = pd.MultiIndex.from_product(\n",
        "        [analysis_panel_full['Municipality_ID'].dropna().unique(), analysis_panel_full.index.get_level_values('snapshot_year').unique()],\n",
        "        names=['Municipality_ID', 'snapshot_year']\n",
        "    )\n",
        "    apprentice_counts = apprentice_counts.reindex(full_idx, fill_value=0)\n",
        "\n",
        "    # Extract the baseline (1990) apprentice counts.\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "    baseline_counts = apprentice_counts.loc[(slice(None), base_year)].rename('apprenticeship_fte_base')\n",
        "    baseline_counts = baseline_counts.reset_index(level='snapshot_year', drop=True)\n",
        "\n",
        "    # Merge baseline counts back onto the full series.\n",
        "    outcomes_df = apprentice_counts.to_frame().merge(\n",
        "        baseline_counts, on='Municipality_ID', how='left'\n",
        "    ).fillna(0)\n",
        "\n",
        "    # Calculate the outcome: (Apprenticeships_rt - Apprenticeships_r,1990) / Apprenticeships_r,1990\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        outcomes_df['apprenticeship_outcome'] = np.divide(\n",
        "            outcomes_df['apprenticeship_fte'] - outcomes_df['apprenticeship_fte_base'],\n",
        "            outcomes_df['apprenticeship_fte_base']\n",
        "        )\n",
        "\n",
        "    # Clean up inf/-inf values and set the base year outcome to exactly 0.\n",
        "    outcomes_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    outcomes_df.loc[(slice(None), base_year), 'apprenticeship_outcome'] = 0.0\n",
        "\n",
        "    print(f\"[{task_name}] Apprenticeship outcomes computed successfully.\")\n",
        "\n",
        "    return outcomes_df[['apprenticeship_outcome']]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def analyze_apprenticeship_uptake(\n",
        "    analysis_panel_full: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the apprenticeship uptake analysis for all event-study years.\n",
        "\n",
        "    This function estimates the causal effect of the immigration shock on the\n",
        "    change in native apprenticeship employment for each pre- and post-treatment\n",
        "    year. The results from this function can be used directly to plot Figure 3\n",
        "    from the paper.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel_full (pd.DataFrame): The complete, unfiltered worker-year\n",
        "            panel from Task 3, which is necessary to correctly identify apprentices.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]: A list of result dictionaries, with each dictionary\n",
        "                              containing the full 2SLS estimation output for one\n",
        "                              event year.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(analysis_panel_full, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel_full` must be a pandas DataFrame.\")\n",
        "    if not isinstance(event_study_df, pd.DataFrame):\n",
        "        raise TypeError(\"`event_study_df` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Step 1 & 2: Compute the apprenticeship outcome variable ---\n",
        "    apprenticeship_outcomes = _compute_apprenticeship_outcomes(analysis_panel_full, master_config)\n",
        "\n",
        "    # --- Step 3: Prepare data and run 2SLS for each event year ---\n",
        "\n",
        "    # Merge the new outcome into the main event study dataset.\n",
        "    analysis_data = event_study_df.merge(\n",
        "        apprenticeship_outcomes, on=['Municipality_ID', 'snapshot_year'], how='left'\n",
        "    )\n",
        "\n",
        "    # Define the full range of years for the event-study plot.\n",
        "    event_years = master_config[\"temporal_parameters\"][\"PRE_TREATMENT_YEARS_FOR_TESTING\"] + \\\n",
        "                  master_config[\"temporal_parameters\"][\"POST_TREATMENT_YEARS\"]\n",
        "\n",
        "    # Initialize a list to store the results for each year.\n",
        "    all_results = []\n",
        "\n",
        "    # Loop through each event year and run the estimation.\n",
        "    for year in sorted(event_years):\n",
        "        print(f\"\\n--- Estimating Apprenticeship Effect for Year: {year} ---\")\n",
        "\n",
        "        # Call the master 2SLS estimation function from Task 10.\n",
        "        # The standard weight (total native baseline employment) is used.\n",
        "        year_results = estimate_regional_effect_2sls(\n",
        "            event_study_df=analysis_data,\n",
        "            master_config=master_config,\n",
        "            outcome_variable='apprenticeship_outcome',\n",
        "            event_year=year\n",
        "        )\n",
        "\n",
        "        # Store the results for this year.\n",
        "        if year_results:\n",
        "            all_results.append(year_results)\n",
        "\n",
        "    print(f\"\\nTask 19: Analysis of apprenticeship uptake completed for all event years.\")\n",
        "\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "mCjJrGVCHBfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20: Structural parameter recovery (η̄^P, η̄^E, φ, c; Equations (1a)–(1c))\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Structural parameter recovery\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Step 1: Helper to Compute Efficiency-to-Headcount Scaling 'c'\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_efficiency_scaling_c(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 20, Step 1\"\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the efficiency-to-headcount scaling parameter 'c'.\n",
        "\n",
        "    'c' is the ratio of the immigration shock measured in efficiency units (wage\n",
        "    bill share) to the shock measured in headcounts (FTE share).\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        float: The estimated value of the scaling parameter 'c'.\n",
        "    \"\"\"\n",
        "    # Define the start and end years for the main shock calculation.\n",
        "    shock_years = config[\"temporal_parameters\"][\"EVENT_STUDY_SHOCK_RULES\"][\"1992_to_1995\"]\n",
        "    start_year, end_year = shock_years['start_year'], shock_years['end_year']\n",
        "\n",
        "    # Create a 'wage_bill' proxy for each employed worker.\n",
        "    # Assumption: fte_weight is a valid proxy for the share of the year worked.\n",
        "    panel = analysis_panel[analysis_panel['is_employed']].copy()\n",
        "    panel['wage_bill'] = panel['Daily_Wage_EUR'] * panel['fte_weight']\n",
        "\n",
        "    # --- Numerator: Efficiency Shock (based on wage bill) ---\n",
        "    wage_bill_czech_start = panel.loc[panel.index.get_level_values('snapshot_year') == start_year, 'wage_bill'][panel['is_czech']].sum()\n",
        "    wage_bill_czech_end = panel.loc[panel.index.get_level_values('snapshot_year') == end_year, 'wage_bill'][panel['is_czech']].sum()\n",
        "    wage_bill_total_base = panel.loc[panel.index.get_level_values('snapshot_year') == start_year, 'wage_bill'].sum()\n",
        "\n",
        "    efficiency_shock = (wage_bill_czech_end - wage_bill_czech_start) / wage_bill_total_base\n",
        "\n",
        "    # --- Denominator: Headcount Shock (based on FTE) ---\n",
        "    fte_czech_start = panel.loc[panel.index.get_level_values('snapshot_year') == start_year, 'fte_weight'][panel['is_czech']].sum()\n",
        "    fte_czech_end = panel.loc[panel.index.get_level_values('snapshot_year') == end_year, 'fte_weight'][panel['is_czech']].sum()\n",
        "    fte_total_base = panel.loc[panel.index.get_level_values('snapshot_year') == start_year, 'fte_weight'].sum()\n",
        "\n",
        "    headcount_shock = (fte_czech_end - fte_czech_start) / fte_total_base\n",
        "\n",
        "    # Validate the denominator to prevent division by zero.\n",
        "    if np.isclose(headcount_shock, 0):\n",
        "        raise ValueError(f\"[{task_name}] Headcount shock is zero, cannot compute 'c'.\")\n",
        "\n",
        "    # c = (Efficiency Shock) / (Headcount Shock)\n",
        "    c_val = efficiency_shock / headcount_shock\n",
        "\n",
        "    print(f\"[{task_name}] Efficiency scaling parameter 'c' computed: {c_val:.4f}\")\n",
        "    return c_val\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Step 2: Helper to Recover Labor Supply Elasticities\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _recover_supply_elasticities(\n",
        "    beta_R: float,\n",
        "    gamma_R: float,\n",
        "    gamma_W: float,\n",
        "    task_name: str = \"Task 20, Step 2\"\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Recovers the population- and efficiency-weighted labor supply elasticities.\n",
        "\n",
        "    Args:\n",
        "        beta_R (float): The estimated regional employment effect (β^R).\n",
        "        gamma_R (float): The estimated regional wage effect (γ^R).\n",
        "        gamma_W (float): The estimated pure wage effect (γ^W).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: A tuple containing (eta_P, eta_E).\n",
        "    \"\"\"\n",
        "    # Ensure the pure wage effect is not zero to avoid division errors.\n",
        "    if np.isclose(gamma_W, 0):\n",
        "        raise ValueError(\"Pure wage effect (gamma_W) is zero, cannot recover elasticities.\")\n",
        "\n",
        "    # Recover population-weighted elasticity (η̄^P) from the ratio of employment to pure wage effects.\n",
        "    # Formula: η̄^P = β^R / γ^W\n",
        "    eta_P = beta_R / gamma_W\n",
        "\n",
        "    # Recover efficiency-weighted elasticity (η̄^E) by rearranging Eq. (1c).\n",
        "    # Formula: η̄^E = (γ^R / γ^W) - 1 + η̄^P\n",
        "    eta_E = (gamma_R / gamma_W) - 1 + eta_P\n",
        "\n",
        "    print(f\"[{task_name}] Recovered supply elasticities: η̄^P = {eta_P:.4f}, η̄^E = {eta_E:.4f}\")\n",
        "    return eta_P, eta_E\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Step 3: Helper to Recover Inverse Demand Elasticity\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _recover_demand_elasticity(\n",
        "    gamma_W: float,\n",
        "    beta_R: float,\n",
        "    eta_P: float,\n",
        "    eta_E: float,\n",
        "    c_val: float,\n",
        "    task_name: str = \"Task 20, Step 3\"\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Recovers the inverse labor demand elasticity (φ).\n",
        "\n",
        "    Args:\n",
        "        gamma_W (float): The estimated pure wage effect (γ^W).\n",
        "        beta_R (float): The estimated regional employment effect (β^R).\n",
        "        eta_P (float): The recovered population-weighted supply elasticity.\n",
        "        eta_E (float): The recovered efficiency-weighted supply elasticity.\n",
        "        c_val (float): The efficiency-to-headcount scaling parameter.\n",
        "\n",
        "    Returns:\n",
        "        float: The estimated inverse labor demand elasticity (φ).\n",
        "    \"\"\"\n",
        "    # The denominator of the formula for φ.\n",
        "    denominator = c_val + (eta_E / eta_P) * beta_R\n",
        "\n",
        "    if np.isclose(denominator, 0):\n",
        "        raise ValueError(\"Denominator for φ calculation is zero, cannot recover demand elasticity.\")\n",
        "\n",
        "    # Recover the inverse demand elasticity (φ).\n",
        "    # Formula: φ = γ^W / (c + (η̄^E / η̄^P) * β^R)\n",
        "    phi = gamma_W / denominator\n",
        "\n",
        "    print(f\"[{task_name}] Recovered inverse demand elasticity φ = {phi:.4f}\")\n",
        "    return phi\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def recover_structural_parameters(\n",
        "    reduced_form_estimates: Dict[str, float],\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the recovery of structural labor market parameters.\n",
        "\n",
        "    This function takes the key reduced-form coefficients from the 2SLS\n",
        "    estimations and uses the paper's theoretical model (Equations 1a-1c)\n",
        "    to solve for the underlying structural parameters: the labor supply\n",
        "    elasticities (η̄^P, η̄^E) and the inverse labor demand elasticity (φ).\n",
        "\n",
        "    It also performs a validation exercise to show that using the biased\n",
        "    regional wage effect (γ^R) would lead to implausible structural estimates.\n",
        "\n",
        "    Args:\n",
        "        reduced_form_estimates (Dict[str, float]): A dictionary with the point\n",
        "            estimates for 'beta_R', 'gamma_R', and 'gamma_W'.\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing all recovered structural\n",
        "                        parameters and the results of the validation exercise.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    required_keys = {'beta_R', 'gamma_R', 'gamma_W'}\n",
        "    if not required_keys.issubset(reduced_form_estimates.keys()):\n",
        "        raise KeyError(f\"Input `reduced_form_estimates` is missing required keys: {required_keys - set(reduced_form_estimates.keys())}\")\n",
        "\n",
        "    # Extract reduced-form coefficients.\n",
        "    beta_R = reduced_form_estimates['beta_R']\n",
        "    gamma_R = reduced_form_estimates['gamma_R']\n",
        "    gamma_W = reduced_form_estimates['gamma_W']\n",
        "\n",
        "    # Step 1: Compute the efficiency-to-headcount scaling parameter 'c'.\n",
        "    c_val = _compute_efficiency_scaling_c(analysis_panel, master_config)\n",
        "\n",
        "    # Step 2: Recover the population- and efficiency-weighted labor supply elasticities.\n",
        "    eta_P, eta_E = _recover_supply_elasticities(beta_R, gamma_R, gamma_W)\n",
        "\n",
        "    # Step 3: Recover the inverse labor demand elasticity.\n",
        "    phi = _recover_demand_elasticity(gamma_W, beta_R, eta_P, eta_E, c_val)\n",
        "    demand_elasticity = 1 / phi if not np.isclose(phi, 0) else np.inf\n",
        "\n",
        "    # --- Validation / Sanity Check ---\n",
        "    # Re-calculate parameters under the naive assumption that γ^R = γ^W.\n",
        "    # This implies η̄^E = η̄^P, as the composition effect is assumed to be zero.\n",
        "    print(\"\\n--- Performing validation using naive (biased) regional wage effect ---\")\n",
        "    eta_P_naive = beta_R / gamma_R if not np.isclose(gamma_R, 0) else np.inf\n",
        "    eta_E_naive = eta_P_naive # By assumption\n",
        "    phi_naive = _recover_demand_elasticity(gamma_R, beta_R, eta_P_naive, eta_E_naive, c_val, task_name=\"Validation\")\n",
        "    demand_elasticity_naive = 1 / phi_naive if not np.isclose(phi_naive, 0) else np.inf\n",
        "\n",
        "    # --- Assemble Final Results ---\n",
        "    results = {\n",
        "        'structural_parameters': {\n",
        "            'c_scaling_factor': c_val,\n",
        "            'eta_P_population_supply_elasticity': eta_P,\n",
        "            'eta_E_efficiency_supply_elasticity': eta_E,\n",
        "            'phi_inverse_demand_elasticity': phi,\n",
        "            'demand_elasticity': demand_elasticity\n",
        "        },\n",
        "        'validation_with_regional_effect': {\n",
        "            'eta_P_naive': eta_P_naive,\n",
        "            'phi_naive': phi_naive,\n",
        "            'demand_elasticity_naive': demand_elasticity_naive\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Structural Parameter Recovery Summary\")\n",
        "    print(f\"  Inverse Demand Elasticity (φ): {phi:.4f}\")\n",
        "    print(f\"  Implied Demand Elasticity (1/φ): {demand_elasticity:.4f}\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Naive (Biased) Inverse Demand Elasticity (φ_naive): {phi_naive:.4f}\")\n",
        "    print(f\"  Naive Implied Demand Elasticity: {demand_elasticity_naive:.4f}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nTask 20: Structural parameter recovery completed successfully.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "x4rJTvg9NyyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21: Build orchestrator function for end-to-end pipeline execution\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Build orchestrator function for end-to-end pipeline execution\n",
        "# ==============================================================================\n",
        "\n",
        "def run_full_analysis_pipeline(\n",
        "    raw_data_path: str,\n",
        "    master_config: Dict[str, Any],\n",
        "    main_result_years: List[int] = [1993, 1995],\n",
        "    cache_dir: str = \"./.cache/\",\n",
        "    force_rerun_prep: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire end-to-end research pipeline with caching.\n",
        "\n",
        "    This master function executes the full sequence of tasks required to replicate\n",
        "    the paper's findings, from raw data validation to the final estimation of\n",
        "    structural parameters. It manages the flow of data artifacts between tasks\n",
        "    and implements a robust caching mechanism to avoid re-computing expensive\n",
        "    data preparation steps.\n",
        "\n",
        "    Args:\n",
        "        raw_data_path (str): The file path to the raw, consolidated spell-level\n",
        "                             DataFrame (e.g., a Parquet file).\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary that\n",
        "                                        governs the entire analysis.\n",
        "        main_result_years (List[int]): A list of the primary event years for which\n",
        "                                       to generate the main decomposition tables.\n",
        "        cache_dir (str): A directory path for storing and retrieving intermediate\n",
        "                         data artifacts to speed up subsequent runs.\n",
        "        force_rerun_prep (bool): If True, all data preparation steps (Tasks 3-9)\n",
        "                                 will be re-computed, ignoring any existing cache.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary containing the final results from all\n",
        "                        estimation tasks, organized by analysis type and year.\n",
        "    \"\"\"\n",
        "    # --- 1. Setup: Caching, Logging, and Initial Data Loading ---\n",
        "\n",
        "    # Create the cache directory if it doesn't exist. This is where intermediate\n",
        "    # data preparation artifacts will be stored.\n",
        "    cache_path = Path(cache_dir)\n",
        "    cache_path.mkdir(exist_ok=True)\n",
        "\n",
        "    # Start a timer for the entire pipeline and log the start message.\n",
        "    start_time = time.time()\n",
        "    print(\"=\"*80 + \"\\nSTARTING END-TO-END ANALYSIS PIPELINE\\n\" + \"=\"*80)\n",
        "\n",
        "    # Define a simple helper function for logging task completion and duration.\n",
        "    def log_task_completion(task_name: str, task_start_time: float):\n",
        "        duration = time.time() - task_start_time\n",
        "        print(f\"--- Task '{task_name}' completed in {duration:.2f} seconds. ---\\n\")\n",
        "\n",
        "    # Load the raw data from the specified path. Assuming Parquet for efficiency.\n",
        "    print(f\"Loading raw data from: {raw_data_path}...\")\n",
        "    consolidated_df_raw = pd.read_parquet(raw_data_path)\n",
        "\n",
        "    # --- 2. Validation Phase (Always runs, no caching) ---\n",
        "\n",
        "    # Start the timer for the validation phase.\n",
        "    task_start = time.time()\n",
        "    # Task 1: Validate the raw data schema, dtypes, and logical integrity.\n",
        "    validate_consolidated_df_raw(consolidated_df_raw, master_config)\n",
        "    log_task_completion(\"1: Validate Raw Data\", task_start)\n",
        "\n",
        "    task_start = time.time()\n",
        "    # Task 2: Validate the master config and load/validate all auxiliary data artifacts.\n",
        "    validated_artifacts = validate_artifacts_and_config(master_config, consolidated_df_raw)\n",
        "    log_task_completion(\"2: Validate Config & Artifacts\", task_start)\n",
        "\n",
        "    # --- 3. Data Preparation Phase (with Caching) ---\n",
        "\n",
        "    # This dictionary will hold all major data artifacts as they are created.\n",
        "    artifacts = validated_artifacts\n",
        "    # This flag controls the cache waterfall; if one step is re-run, all subsequent steps must also be re-run.\n",
        "    is_cache_valid = not force_rerun_prep\n",
        "\n",
        "    # Define the full data preparation pipeline as a list of tasks.\n",
        "    # Each tuple contains: (name, function, input_map, output_keys, cache_filename)\n",
        "    prep_pipeline = [\n",
        "        (\"3: Cleanse & Canonicalize\", cleanse_and_canonicalize_spells,\n",
        "         {'consolidated_df_raw': consolidated_df_raw},\n",
        "         ['df_normalized', 'panel_full_with_flags', 'panel_main_analysis'], 'task3.pkl'),\n",
        "        (\"4: Impute Wages\", impute_censored_wages,\n",
        "         {'worker_year_panel': 'panel_main_analysis'},\n",
        "         'panel_main_with_wages', 'task4.pkl'),\n",
        "        (\"5: Build Analysis Panel\", build_analysis_panel,\n",
        "         {'analysis_panel_employed': 'panel_main_with_wages', 'all_spells_cleansed': 'df_normalized'},\n",
        "         'analysis_panel', 'task5.pkl'),\n",
        "        (\"6: Aggregate Regional Panel\", aggregate_to_regional_panel,\n",
        "         {'analysis_panel': 'analysis_panel'},\n",
        "         ['regional_panel', 'national_wage_series'], 'task6.pkl'),\n",
        "        (\"7: Construct Shock\", construct_immigration_shock,\n",
        "         {'analysis_panel': 'analysis_panel', 'regional_panel': 'regional_panel'},\n",
        "         'shock_df', 'task7.pkl'),\n",
        "        (\"8: Construct Instruments\", construct_instrumental_variables,\n",
        "         {'analysis_panel': 'analysis_panel'},\n",
        "         'instruments_df', 'task8.pkl'),\n",
        "        (\"9: Prepare Event Study DF\", prepare_event_study_dataset,\n",
        "         {'regional_panel': 'regional_panel', 'shock_df': 'shock_df',\n",
        "          'instruments_df': 'instruments_df', 'analysis_panel': 'analysis_panel'},\n",
        "         'event_study_df', 'task9.pkl')\n",
        "    ]\n",
        "\n",
        "    # Execute the preparation pipeline.\n",
        "    for name, func, kwargs_map, out_keys, cache_file in prep_pipeline:\n",
        "        task_start = time.time()\n",
        "        cache_filepath = cache_path / cache_file\n",
        "\n",
        "        # Check if a valid cached result exists.\n",
        "        if is_cache_valid and cache_filepath.exists():\n",
        "            print(f\"Loading cached result for Task '{name}'...\")\n",
        "            with open(cache_filepath, 'rb') as f: result = pickle.load(f)\n",
        "        else:\n",
        "            print(f\"Running Task '{name}'...\")\n",
        "            # If we run any task, all subsequent tasks must also be run.\n",
        "            is_cache_valid = False\n",
        "            # Resolve the function's arguments from the artifacts dictionary.\n",
        "            kwargs = {k: artifacts.get(v, v) if isinstance(v, str) else v for k, v in kwargs_map.items()}\n",
        "            kwargs['master_config'] = master_config\n",
        "            # Execute the task function.\n",
        "            result = func(**kwargs)\n",
        "            # Save the result to the cache.\n",
        "            with open(cache_filepath, 'wb') as f: pickle.dump(result, f)\n",
        "\n",
        "        # Unpack the results into the artifacts dictionary for downstream use.\n",
        "        if isinstance(out_keys, list):\n",
        "            for i, key in enumerate(out_keys): artifacts[key] = result[i]\n",
        "        else:\n",
        "            artifacts[out_keys] = result\n",
        "        log_task_completion(name, task_start)\n",
        "\n",
        "    # --- 4. Estimation Phase (No Caching) ---\n",
        "\n",
        "    # Initialize the final results dictionary.\n",
        "    final_results: Dict[str, Any] = {'main_tables': {}, 'event_studies': {}}\n",
        "\n",
        "    # Run main decomposition and heterogeneity analyses for the specified years.\n",
        "    for year in main_result_years:\n",
        "        year_results: Dict[str, Any] = {}\n",
        "        print(f\"\\n{'='*30} RUNNING ESTIMATIONS FOR YEAR: {year} {'='*30}\\n\")\n",
        "\n",
        "        # Task 10 (as part of Task 11) & 11: Decompose regional employment.\n",
        "        year_results['employment_decomposition'] = decompose_regional_employment_effect(\n",
        "            artifacts['analysis_panel'], artifacts['regional_panel'], artifacts['event_study_df'], master_config, year\n",
        "        )\n",
        "        # Task 12 & 13: Estimate and decompose wage effects.\n",
        "        wage_effects = estimate_wage_effects(\n",
        "            artifacts['event_study_df'], artifacts['analysis_panel'], master_config, year\n",
        "        )\n",
        "        year_results['wage_decomposition'] = decompose_regional_wage_effect(\n",
        "            artifacts['analysis_panel'], artifacts['event_study_df'], wage_effects, master_config, year\n",
        "        )\n",
        "        # Task 14: Run selection bounding on the pure wage effect.\n",
        "        year_results['selection_bounds'] = bound_pure_wage_effect_selection(\n",
        "            artifacts['analysis_panel'], artifacts['event_study_df'], wage_effects['pure_wage_effect'], master_config, year\n",
        "        )\n",
        "        # Task 15: Analyze non-employed entrants.\n",
        "        year_results['non_employed_entrants'] = analyze_non_employed_entrants(\n",
        "            artifacts['analysis_panel'], artifacts['regional_panel'], artifacts['national_wage_series'], artifacts['event_study_df'], master_config, year\n",
        "        )\n",
        "        # Task 16: Analyze older workers.\n",
        "        year_results['older_workers'] = analyze_older_workers(\n",
        "            artifacts['analysis_panel'], artifacts['regional_panel'], artifacts['event_study_df'], master_config, year\n",
        "        )\n",
        "        # Task 17 & 18: Analyze task heterogeneity and decompose routine employment.\n",
        "        year_results['task_heterogeneity'] = analyze_task_heterogeneity(\n",
        "            artifacts['analysis_panel'], artifacts['event_study_df'], master_config, year\n",
        "        )\n",
        "        year_results['routine_decomposition'] = decompose_routine_employment(\n",
        "            artifacts['analysis_panel'], artifacts['event_study_df'], master_config, year\n",
        "        )\n",
        "        # Store all results for this year.\n",
        "        final_results['main_tables'][year] = year_results\n",
        "\n",
        "    # Run event study analyses that loop over all years.\n",
        "    final_results['event_studies']['apprenticeship_uptake'] = analyze_apprenticeship_uptake(\n",
        "        artifacts['panel_full_with_flags'], artifacts['event_study_df'], master_config\n",
        "    )\n",
        "\n",
        "    # Run final structural parameter recovery using results from the first main year.\n",
        "    main_year = main_result_years[0]\n",
        "    final_results['structural_parameters'] = recover_structural_parameters(\n",
        "        reduced_form_estimates={\n",
        "            'beta_R': final_results['main_tables'][main_year]['employment_decomposition']['total_effect']['point_estimate'],\n",
        "            'gamma_R': final_results['main_tables'][main_year]['wage_decomposition']['regional_wage_effect']['point_estimate'],\n",
        "            'gamma_W': final_results['main_tables'][main_year]['wage_decomposition']['pure_wage_effect']['point_estimate']\n",
        "        },\n",
        "        analysis_panel=artifacts['analysis_panel'],\n",
        "        master_config=master_config\n",
        "    )\n",
        "\n",
        "    # --- 5. Finalization ---\n",
        "\n",
        "    # Calculate and log the total runtime of the pipeline.\n",
        "    total_duration = time.time() - start_time\n",
        "    print(\"=\"*80 + f\"\\nENTIRE PIPELINE COMPLETED in {total_duration / 60:.2f} minutes.\\n\" + \"=\"*80)\n",
        "\n",
        "    # Return the comprehensive dictionary of all results.\n",
        "    return final_results\n"
      ],
      "metadata": {
        "id": "Gxc_i4S2Oshv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22: Robustness: Event-study pre-trends and placebo tests\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 22: Robustness: Event-study pre-trends and placebo tests\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Step 1: Helper to Estimate Pre-Period Coefficients\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _estimate_pre_trends(\n",
        "    event_study_df: pd.DataFrame,\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    outcomes_to_test: Dict[str, str],\n",
        "    task_name: str = \"Task 22, Step 1\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Estimates placebo effects for key outcomes in the pre-treatment period.\n",
        "\n",
        "    This function iterates through the pre-treatment years and runs the\n",
        "    appropriate 2SLS estimation for each specified outcome to test the\n",
        "    parallel trends assumption.\n",
        "\n",
        "    Args:\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        analysis_panel (pd.DataFrame): The full worker-year panel for individual-level models.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        outcomes_to_test (Dict[str, str]): A dictionary mapping outcome names\n",
        "            (e.g., 'emp_outcome') to the estimation type ('regional' or 'pure_wage').\n",
        "        task_name (str): The name of the calling task for clear error reporting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the full estimation results for\n",
        "                      each outcome in each pre-treatment year.\n",
        "    \"\"\"\n",
        "    # Extract the list of pre-treatment years to test.\n",
        "    pre_treatment_years = master_config[\"temporal_parameters\"][\"PRE_TREATMENT_YEARS_FOR_TESTING\"]\n",
        "\n",
        "    # Store results from each estimation.\n",
        "    all_results = []\n",
        "\n",
        "    # Loop through each outcome and each pre-treatment year.\n",
        "    for outcome_var, model_type in outcomes_to_test.items():\n",
        "        for year in pre_treatment_years:\n",
        "            print(f\"\\n--- Estimating Pre-Trend for '{outcome_var}' ({model_type}) in Year: {year} ---\")\n",
        "\n",
        "            if model_type == 'regional':\n",
        "                # Call the master regional 2SLS estimation function.\n",
        "                year_results = estimate_regional_effect_2sls(\n",
        "                    event_study_df=event_study_df,\n",
        "                    master_config=master_config,\n",
        "                    outcome_variable=outcome_var,\n",
        "                    event_year=year\n",
        "                )\n",
        "            elif model_type == 'pure_wage':\n",
        "                # Call the master individual-level FD-IV estimation function.\n",
        "                year_results = _estimate_pure_wage_effect_stayers(\n",
        "                    analysis_panel=analysis_panel,\n",
        "                    event_study_df=event_study_df,\n",
        "                    master_config=master_config,\n",
        "                    event_year=year\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown model_type '{model_type}' for pre-trend estimation.\")\n",
        "\n",
        "            # Store the results if the estimation was successful.\n",
        "            if year_results:\n",
        "                all_results.append(year_results)\n",
        "\n",
        "    print(f\"[{task_name}] Pre-trend estimation completed.\")\n",
        "\n",
        "    # Convert the list of result dictionaries into a DataFrame.\n",
        "    return pd.DataFrame(all_results)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Step 2: Helper to Construct Dynamic Event-Study Plots\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _plot_event_study(\n",
        "    results_df: pd.DataFrame,\n",
        "    title: str,\n",
        "    y_label: str,\n",
        "    base_year: int = 1990\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates a publication-quality event-study plot from estimation results.\n",
        "\n",
        "    This function takes a tidy DataFrame of regression results (containing point\n",
        "    estimates and confidence intervals for multiple years) and creates a\n",
        "    standard event-study plot. It visualizes the dynamic treatment effects over\n",
        "    time, clearly demarcating the pre- and post-treatment periods.\n",
        "\n",
        "    Args:\n",
        "        results_df (pd.DataFrame): A DataFrame where each row contains the\n",
        "            estimation results for a specific 'event_year'. Must contain the\n",
        "            columns 'event_year', 'point_estimate', and 'bootstrap_ci'.\n",
        "        title (str): The main title for the plot.\n",
        "        y_label (str): The label for the y-axis, describing the coefficient's\n",
        "                       interpretation.\n",
        "        base_year (int): The base year of the study (the year before treatment\n",
        "                         starts), used to draw a demarcation line.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If `results_df` is missing required columns.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check if the input DataFrame is valid and contains the necessary columns.\n",
        "    if not isinstance(results_df, pd.DataFrame) or results_df.empty:\n",
        "        # If data is missing, print a warning and exit gracefully.\n",
        "        print(f\"Warning: No data provided for plotting '{title}'. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    # Define the set of columns required for plotting.\n",
        "    required_cols = {'event_year', 'point_estimate', 'bootstrap_ci'}\n",
        "    # Assert that all required columns are present.\n",
        "    if not required_cols.issubset(results_df.columns):\n",
        "        raise ValueError(f\"Input results_df is missing required columns. Expected: {required_cols}\")\n",
        "\n",
        "    # --- Data Preparation ---\n",
        "\n",
        "    # Sort the data by year to ensure lines are plotted chronologically.\n",
        "    plot_data = results_df.sort_values('event_year').copy()\n",
        "\n",
        "    # Unpack the confidence interval tuple into separate 'ci_lower' and 'ci_upper' columns.\n",
        "    plot_data[['ci_lower', 'ci_upper']] = pd.DataFrame(plot_data['bootstrap_ci'].tolist(), index=plot_data.index)\n",
        "\n",
        "    # --- Plotting ---\n",
        "\n",
        "    # Set a professional plot style.\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "    # Create the figure and axes objects for plotting.\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "    # Plot the point estimates for each year as a line with markers.\n",
        "    ax.plot(plot_data['event_year'], plot_data['point_estimate'], marker='s', linestyle='-', color='black', label='Point Estimate')\n",
        "\n",
        "    # Plot the 95% confidence interval as a shaded region for visual clarity.\n",
        "    ax.fill_between(plot_data['event_year'], plot_data['ci_lower'], plot_data['ci_upper'], color='blue', alpha=0.15, label='95% Bootstrap CI')\n",
        "\n",
        "    # Plot the bounds of the confidence interval as dashed lines.\n",
        "    ax.plot(plot_data['event_year'], plot_data['ci_lower'], linestyle='--', color='blue', linewidth=0.8)\n",
        "    ax.plot(plot_data['event_year'], plot_data['ci_upper'], linestyle='--', color='blue', linewidth=0.8)\n",
        "\n",
        "    # Add a horizontal line at y=0, which serves as the null hypothesis reference.\n",
        "    ax.axhline(0, color='black', linestyle='-', linewidth=0.8)\n",
        "\n",
        "    # Add a vertical line to clearly separate the pre- and post-treatment periods.\n",
        "    ax.axvline(base_year + 0.5, color='red', linestyle='--', linewidth=1.2, label=f'Treatment Start (Post-{base_year})')\n",
        "\n",
        "    # --- Formatting and Finalization ---\n",
        "\n",
        "    # Set labels and title with appropriate font sizes.\n",
        "    ax.set_xlabel(\"Year\", fontsize=12)\n",
        "    ax.set_ylabel(y_label, fontsize=12)\n",
        "    ax.set_title(title, fontsize=14, weight='bold')\n",
        "\n",
        "    # Add a legend to identify the plot elements.\n",
        "    ax.legend(fontsize=10)\n",
        "\n",
        "    # Customize the grid for a cleaner look.\n",
        "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "    # Customize tick label sizes.\n",
        "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
        "\n",
        "    # Ensure the x-axis ticks are integers representing years.\n",
        "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\n",
        "    # Adjust layout to prevent labels from overlapping.\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Display the final plot.\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_robustness_checks(\n",
        "    event_study_df: pd.DataFrame,\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    main_estimation_results: Dict[str, Any],\n",
        "    master_config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of key robustness checks for the main results.\n",
        "\n",
        "    This function performs two main actions to validate the identification strategy:\n",
        "    1.  **Pre-Trend Estimation**: It estimates \"placebo\" effects for the main\n",
        "        outcomes in the pre-treatment period (1987-1989). The identifying\n",
        "        assumption of parallel trends requires these coefficients to be\n",
        "        statistically indistinguishable from zero.\n",
        "    2.  **Event-Study Visualization**: It combines the pre-trend results with the\n",
        "        post-treatment results and generates dynamic event-study plots (like\n",
        "        Figure 1 in the paper) to visualize the full time path of the estimated\n",
        "        effects.\n",
        "\n",
        "    Args:\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        analysis_panel (pd.DataFrame): The full worker-year panel, required for\n",
        "            any individual-level pre-trend estimations.\n",
        "        main_estimation_results (Dict[str, Any]): The results from the main\n",
        "            estimation tasks (e.g., from the Task 21 orchestrator), which contain\n",
        "            the post-treatment coefficients.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(event_study_df, pd.DataFrame):\n",
        "        raise TypeError(\"`event_study_df` must be a pandas DataFrame.\")\n",
        "    if not isinstance(main_estimation_results, dict):\n",
        "        raise TypeError(\"`main_estimation_results` must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Estimate Pre-Period Coefficients ---\n",
        "\n",
        "    # Define the main outcomes for which to test for pre-trends.\n",
        "    outcomes_to_test = {\n",
        "        'emp_outcome': 'regional',\n",
        "        'wage_outcome': 'regional',\n",
        "    }\n",
        "\n",
        "    # Call the helper to run the 2SLS regressions for each pre-treatment year.\n",
        "    pre_trend_results_df = _estimate_pre_trends(\n",
        "        event_study_df, analysis_panel, master_config, outcomes_to_test\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Construct Dynamic Event-Study Plots ---\n",
        "\n",
        "    # Extract the corresponding post-treatment results from the main results dictionary.\n",
        "    post_trend_results = []\n",
        "    for year, year_data in main_estimation_results.get('main_tables', {}).items():\n",
        "        # Extract the total regional employment effect.\n",
        "        if 'employment_decomposition' in year_data:\n",
        "            res = year_data['employment_decomposition']['total_effect'].copy()\n",
        "            res['outcome'] = 'emp_outcome'  # Add an outcome identifier for plotting.\n",
        "            post_trend_results.append(res)\n",
        "        # Extract the regional wage effect.\n",
        "        if 'wage_decomposition' in year_data:\n",
        "            res = year_data['wage_decomposition']['regional_wage_effect'].copy()\n",
        "            res['outcome'] = 'wage_outcome' # Add an outcome identifier for plotting.\n",
        "            post_trend_results.append(res)\n",
        "\n",
        "    # Combine the pre-trend and post-trend results into a single DataFrame.\n",
        "    full_event_study_results = pd.concat(\n",
        "        [pre_trend_results_df, pd.DataFrame(post_trend_results)],\n",
        "        ignore_index=True\n",
        "    )\n",
        "\n",
        "    # Generate the plot for the regional employment effect (replicating Figure 1A logic).\n",
        "    _plot_event_study(\n",
        "        results_df=full_event_study_results[full_event_study_results['outcome'] == 'emp_outcome'],\n",
        "        title='Event Study: Impact of Immigration on Regional Native Employment',\n",
        "        y_label='Coefficient (Effect on % Change in Native Employment)',\n",
        "        base_year=master_config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "    )\n",
        "\n",
        "    # Generate the plot for the regional wage effect (replicating Figure 1B logic).\n",
        "    _plot_event_study(\n",
        "        results_df=full_event_study_results[full_event_study_results['outcome'] == 'wage_outcome'],\n",
        "        title='Event Study: Impact of Immigration on Regional Native Wages',\n",
        "        y_label='Coefficient (Effect on Change in Mean Log Wages)',\n",
        "        base_year=master_config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "    )\n",
        "\n",
        "    # Log the successful completion of the task.\n",
        "    print(\"\\nTask 22: Robustness checks completed successfully.\")\n"
      ],
      "metadata": {
        "id": "g3NxAfBWQwEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23: Robustness: Alternative specifications and sensitivity\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Robustness: Alternative specifications and sensitivity\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Step 1: Helper for First-Stage Interaction Test\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def test_first_stage_interactions(\n",
        "    event_study_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    event_year: int,\n",
        "    task_name: str = \"Task 23, Step 1\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Tests an alternative first-stage specification with interactions.\n",
        "\n",
        "    This function re-estimates the main regional employment effect using an\n",
        "    alternative first stage where the instruments are interacted with a\n",
        "    border region indicator. This is a robustness check on the instrument spec.\n",
        "\n",
        "    Args:\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        event_year (int): The specific year to run the test for.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The full estimation results from the 2SLS model with\n",
        "                        the interactive first stage.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- {task_name}: Running First-Stage Interaction Test for {event_year} ---\")\n",
        "\n",
        "    # Create a copy of the data to modify.\n",
        "    data = event_study_df.copy()\n",
        "\n",
        "    # Create the 'Is_Border_Region' flag.\n",
        "    treated_districts = set(master_config[\"geographic_policy_parameters\"][\"TREATED_DISTRICT_IDS\"])\n",
        "    data['Is_Border_Region'] = data['District_ID'].isin(treated_districts).astype(int)\n",
        "\n",
        "    # Create the interaction term instruments.\n",
        "    data['dist_x_border'] = data['distance_to_border'] * data['Is_Border_Region']\n",
        "    data['dist_sq_x_border'] = data['distance_to_border_sq'] * data['Is_Border_Region']\n",
        "\n",
        "    # Create a temporary config with the modified instrument specification.\n",
        "    temp_config = copy.deepcopy(master_config)\n",
        "    temp_config['algorithm_config_parameters']['INSTRUMENT_SPECIFICATION'] = {\n",
        "        'dist_x_border': 1,\n",
        "        'dist_sq_x_border': 2\n",
        "    }\n",
        "\n",
        "    # Re-run the main 2SLS estimation with the new instrument set.\n",
        "    results = estimate_regional_effect_2sls(\n",
        "        event_study_df=data,\n",
        "        master_config=temp_config,\n",
        "        outcome_variable='emp_outcome',\n",
        "        event_year=event_year\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Step 2: Helper for Parameter Perturbation Sensitivity\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def test_parameter_perturbation(\n",
        "    perturbation_scenario: Dict[str, Any],\n",
        "    raw_data_path: str,\n",
        "    master_config: Dict[str, Any],\n",
        "    event_year: int,\n",
        "    base_cache_dir: str,\n",
        "    task_name: str = \"Task 23, Step 2\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Re-runs the entire pipeline with a perturbed configuration to test sensitivity.\n",
        "\n",
        "    This function provides a framework for testing the robustness of the main\n",
        "    results to changes in key methodological parameters (e.g., FTE weights).\n",
        "    It works by creating a modified copy of the master configuration, clearing\n",
        "    any old cache for this specific scenario, and then executing the entire\n",
        "    end-to-end analysis pipeline from scratch. This is computationally intensive\n",
        "    but provides the most rigorous test of parameter sensitivity.\n",
        "\n",
        "    Args:\n",
        "        perturbation_scenario (Dict[str, Any]): A dictionary defining the test case.\n",
        "            It should have one key (the scenario name) and a value which is another\n",
        "            dictionary mapping dot-separated config paths to their new values.\n",
        "            Example: {'fte_low': {'algorithm_config_parameters.PART_TIME_EQUIVALENCY_WEIGHTS': new_weights}}\n",
        "        raw_data_path (str): The file path to the raw data, needed to re-run the pipeline.\n",
        "        master_config (Dict[str, Any]): The baseline master configuration dictionary.\n",
        "        event_year (int): The specific event year to extract the final result for.\n",
        "        base_cache_dir (str): The root directory for caching, from which a unique\n",
        "                              subdirectory for this scenario will be created.\n",
        "        task_name (str): The name of the calling task for clear error reporting.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The key estimation result (e.g., the regional employment\n",
        "                        effect dictionary) from the pipeline run with the\n",
        "                        perturbed configuration.\n",
        "    \"\"\"\n",
        "    # --- 1. Prepare Perturbed Configuration ---\n",
        "\n",
        "    # Extract the scenario name and the dictionary of parameter changes.\n",
        "    scenario_name = list(perturbation_scenario.keys())[0]\n",
        "    param_changes = list(perturbation_scenario.values())[0]\n",
        "    print(f\"\\n--- {task_name}: Testing Sensitivity to Parameter Perturbation: {scenario_name} ---\")\n",
        "\n",
        "    # Create a deep copy of the master config to avoid modifying the original.\n",
        "    perturbed_config = copy.deepcopy(master_config)\n",
        "\n",
        "    # Apply the specified changes to the copied config.\n",
        "    # This loop navigates the nested dictionary using a dot-separated path.\n",
        "    for path, value in param_changes.items():\n",
        "        keys = path.split('.')\n",
        "        d = perturbed_config\n",
        "        for key in keys[:-1]:\n",
        "            d = d[key]\n",
        "        d[keys[-1]] = value\n",
        "\n",
        "    # --- 2. Set Up and Run the Pipeline ---\n",
        "\n",
        "    # Define a unique cache directory for this sensitivity run to avoid conflicts.\n",
        "    scenario_cache_dir = Path(base_cache_dir) / f\"sensitivity_{scenario_name}\"\n",
        "\n",
        "    # Clear any old cache for this specific scenario to ensure a clean run.\n",
        "    if scenario_cache_dir.exists():\n",
        "        shutil.rmtree(scenario_cache_dir)\n",
        "\n",
        "    # Re-run the entire end-to-end analysis pipeline with the modified config.\n",
        "    # We must force rerun of the data preparation to ensure the parameter change propagates.\n",
        "    full_results = run_full_analysis_pipeline(\n",
        "        raw_data_path=raw_data_path,\n",
        "        master_config=perturbed_config,\n",
        "        main_result_years=[event_year],\n",
        "        cache_dir=str(scenario_cache_dir),\n",
        "        force_rerun_prep=True\n",
        "    )\n",
        "\n",
        "    # --- 3. Extract and Return Key Result ---\n",
        "\n",
        "    # Extract the key result of interest (the total regional employment effect)\n",
        "    # from the comprehensive results dictionary returned by the pipeline.\n",
        "    key_result = full_results['main_tables'][event_year]['employment_decomposition']['total_effect']\n",
        "\n",
        "    # Return the result for this sensitivity scenario.\n",
        "    return key_result\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Step 3: Helper for Sample Restriction Sensitivity\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def test_sample_restriction_sensitivity(\n",
        "    event_study_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    event_year: int,\n",
        "    restriction: Dict[str, Any],\n",
        "    task_name: str = \"Task 23, Step 3\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Re-runs the main estimation on a programmatically restricted sample.\n",
        "\n",
        "    This function tests the sensitivity of the main findings to the sample\n",
        "    definition by applying additional filters to the final analysis dataset\n",
        "    before re-running the 2SLS estimation.\n",
        "\n",
        "    Args:\n",
        "        event_study_df (pd.DataFrame): The main, fully prepared analysis dataset.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        event_year (int): The specific event year to run the estimation for.\n",
        "        restriction (Dict[str, Any]): A dictionary defining the restrictions to apply.\n",
        "            Supported keys: 'distance_band' (e.g., [10, 60]) or\n",
        "            'min_muni_size' (e.g., 100).\n",
        "        task_name (str): The name of the calling task for clear error reporting.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The estimation results dictionary for the restricted sample.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(event_study_df, pd.DataFrame):\n",
        "        raise TypeError(\"`event_study_df` must be a pandas DataFrame.\")\n",
        "\n",
        "    print(f\"\\n--- {task_name}: Testing Sensitivity to Sample Restriction: {restriction} ---\")\n",
        "\n",
        "    # --- 1. Apply Sample Restrictions ---\n",
        "\n",
        "    # Work on a copy of the DataFrame to avoid modifying the original.\n",
        "    restricted_df = event_study_df.copy()\n",
        "\n",
        "    # Apply a distance band filter if specified.\n",
        "    if 'distance_band' in restriction:\n",
        "        min_dist, max_dist = restriction['distance_band']\n",
        "        restricted_df = restricted_df[\n",
        "            (restricted_df['distance_to_border'] >= min_dist) &\n",
        "            (restricted_df['distance_to_border'] <= max_dist)\n",
        "        ]\n",
        "\n",
        "    # Apply a minimum municipality size filter if specified.\n",
        "    if 'min_muni_size' in restriction:\n",
        "        weight_col = master_config[\"algorithm_config_parameters\"][\"WEIGHT_COLUMN_MUNICIPALITY\"]\n",
        "        restricted_df = restricted_df[restricted_df[weight_col] >= restriction['min_muni_size']]\n",
        "\n",
        "    # --- 2. Re-run Estimation ---\n",
        "\n",
        "    # Re-run the main regional employment effect estimation on the newly restricted data.\n",
        "    results = estimate_regional_effect_2sls(\n",
        "        event_study_df=restricted_df,\n",
        "        master_config=master_config,\n",
        "        outcome_variable='emp_outcome',\n",
        "        event_year=event_year\n",
        "    )\n",
        "\n",
        "    # Return the results from this sensitivity run.\n",
        "    return results\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_sensitivity_analyses(\n",
        "    raw_data_path: str,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    event_year: int = 1993\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of alternative specification and sensitivity checks.\n",
        "\n",
        "    This function runs a series of robustness checks to test the stability of\n",
        "    the main findings. It includes:\n",
        "    1.  An alternative first-stage specification with instrument interactions.\n",
        "    2.  Sensitivity to parameter choices (e.g., FTE weights) by re-running the\n",
        "        entire pipeline with a modified configuration.\n",
        "    3.  Sensitivity to sample restrictions based on distance and municipality size\n",
        "        by re-running the final estimation on a filtered dataset.\n",
        "\n",
        "    Args:\n",
        "        raw_data_path (str): Path to the raw data, needed for parameter perturbation tests.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        event_year (int): The primary event year for which to run the checks.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary containing the results from all\n",
        "                        sensitivity analyses performed.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(event_study_df, pd.DataFrame):\n",
        "        raise TypeError(\"`event_study_df` must be a pandas DataFrame.\")\n",
        "\n",
        "    # Initialize the main results dictionary.\n",
        "    sensitivity_results: Dict[str, Any] = {}\n",
        "\n",
        "    # --- Step 1: First-Stage Interaction Test ---\n",
        "    # This tests if the instrument's power is robust to a more flexible spec.\n",
        "    sensitivity_results['first_stage_interaction'] = test_first_stage_interactions(\n",
        "        event_study_df, master_config, event_year\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Parameter Perturbation ---\n",
        "    # This is computationally expensive as it re-runs the entire pipeline for each scenario.\n",
        "    param_scenarios = [\n",
        "        {'fte_weights_low': {'algorithm_config_parameters.PART_TIME_EQUIVALENCY_WEIGHTS': {1: 1.0, 5: 0.60, 6: 0.40}}},\n",
        "        {'fte_weights_high': {'algorithm_config_parameters.PART_TIME_EQUIVALENCY_WEIGHTS': {1: 1.0, 5: 0.75, 6: 0.60}}}\n",
        "    ]\n",
        "    param_results = {}\n",
        "    for scenario in param_scenarios:\n",
        "        scenario_name = list(scenario.keys())[0]\n",
        "        param_results[scenario_name] = test_parameter_perturbation(\n",
        "            scenario, raw_data_path, master_config, event_year, master_config.get('cache_dir', './.cache/')\n",
        "        )\n",
        "    sensitivity_results['parameter_perturbations'] = param_results\n",
        "\n",
        "    # --- Step 3: Sample Restriction Sensitivity ---\n",
        "    # This tests if the results are driven by municipalities that are very close/far or very small.\n",
        "    sample_restriction_scenarios = {\n",
        "        'distance_band_10_60km': {'distance_band': [10, 60]},\n",
        "        'min_size_100FTE': {'min_muni_size': 100}\n",
        "    }\n",
        "\n",
        "    restriction_results = {}\n",
        "    for name, restriction in sample_restriction_scenarios.items():\n",
        "        restriction_results[name] = test_sample_restriction_sensitivity(\n",
        "            event_study_df, master_config, event_year, restriction\n",
        "        )\n",
        "    sensitivity_results['sample_restrictions'] = restriction_results\n",
        "\n",
        "    # Log the successful completion of the task.\n",
        "    print(\"\\nTask 23: Sensitivity analyses completed successfully.\")\n",
        "\n",
        "    # Return the comprehensive dictionary of all sensitivity results.\n",
        "    return sensitivity_results\n"
      ],
      "metadata": {
        "id": "lb9h5lqwXz29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24: Robustness: Pseudo-panel cross-check (Equations (C5.1)–(C5.2))\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 24: Robustness: Pseudo-panel cross-check\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 24, Step 1: Helper to Construct the Pseudo-Panel\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _construct_pseudo_panel(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    task_name: str = \"Task 24, Step 1\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs a pseudo-panel by aggregating individual data into cells.\n",
        "\n",
        "    Cells are defined by the interaction of municipality, year, and observable\n",
        "    worker characteristics (education, age group, gender). The function\n",
        "    calculates the mean log wage and cell size for each cell.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame where each row represents a pseudo-panel cell,\n",
        "                      containing the mean log wage and cell size.\n",
        "    \"\"\"\n",
        "    # Define the demographic groups for cell construction.\n",
        "    group_cols = ['education_group', 'age_group', 'gender_group']\n",
        "\n",
        "    # Filter to the relevant sample: native, full-time, employed workers.\n",
        "    pseudo_panel_sample = analysis_panel[\n",
        "        analysis_panel['is_native'] &\n",
        "        analysis_panel['is_full_time'] &\n",
        "        analysis_panel['is_employed']\n",
        "    ].copy()\n",
        "\n",
        "    # Group by cell identifiers and aggregate.\n",
        "    cell_df = pseudo_panel_sample.groupby(\n",
        "        ['Municipality_ID', 'snapshot_year'] + group_cols\n",
        "    ).agg(\n",
        "        mean_log_wage=('log_wage_imputed', 'mean'),\n",
        "        cell_size=('Worker_ID', 'count')\n",
        "    )\n",
        "\n",
        "    # --- Filter out small cells based on baseline year size ---\n",
        "    base_year = config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "    min_cell_size = config[\"algorithm_config_parameters\"][\"MIN_PSEUDO_PANEL_CELL_SIZE\"]\n",
        "\n",
        "    # Get the cell sizes for the baseline year.\n",
        "    baseline_cell_sizes = cell_df.loc[\n",
        "        (slice(None), base_year, slice(None), slice(None), slice(None)), 'cell_size'\n",
        "    ].rename('baseline_cell_size')\n",
        "\n",
        "    # Merge baseline sizes back to the main cell DataFrame.\n",
        "    cell_df = cell_df.reset_index().merge(\n",
        "        baseline_cell_sizes.reset_index(),\n",
        "        on=['Municipality_ID'] + group_cols,\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Filter to keep only cells that meet the minimum size requirement in the baseline year.\n",
        "    initial_cells = len(cell_df)\n",
        "    cell_df_filtered = cell_df[cell_df['baseline_cell_size'] >= min_cell_size].copy()\n",
        "    final_cells = len(cell_df_filtered)\n",
        "\n",
        "    print(f\"[{task_name}] Constructed pseudo-panel. Kept {final_cells} of {initial_cells} \"\n",
        "          f\"cell-year observations after applying min size filter of {min_cell_size}.\")\n",
        "\n",
        "    return cell_df_filtered.set_index(['Municipality_ID', 'snapshot_year'] + group_cols)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 24, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_pseudo_panel_check(\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    event_study_df: pd.DataFrame,\n",
        "    master_config: Dict[str, Any],\n",
        "    event_year: int\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the pseudo-panel robustness check as per Appendix C.5.\n",
        "\n",
        "    This function estimates the wage effect of immigration using a pseudo-panel\n",
        "    of demographic cells, which mimics an analysis using repeated cross-sectional\n",
        "    data where only observable characteristics can be controlled for. The result\n",
        "    (γ^PP) is then compared to the regional (γ^R) and pure (γ^W) wage effects.\n",
        "\n",
        "    Args:\n",
        "        analysis_panel (pd.DataFrame): The fully enriched worker-year panel.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        event_year (int): The specific year of the event study to analyze.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The full 2SLS estimation results for the pseudo-panel model.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(analysis_panel, pd.DataFrame):\n",
        "        raise TypeError(\"`analysis_panel` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Step 1: Construct the Pseudo-Panel ---\n",
        "    pseudo_panel_df = _construct_pseudo_panel(analysis_panel, master_config)\n",
        "\n",
        "    # --- Step 2: First-Difference and Prepare for Estimation ---\n",
        "    base_year = master_config[\"temporal_parameters\"][\"BASE_YEAR\"]\n",
        "\n",
        "    # Extract data for the base and event years.\n",
        "    pseudo_panel_base = pseudo_panel_df.loc[(slice(None), base_year), :]\n",
        "    pseudo_panel_event = pseudo_panel_df.loc[(slice(None), event_year), :]\n",
        "\n",
        "    # Align the two time periods for differencing.\n",
        "    merged_panel = pseudo_panel_base.merge(\n",
        "        pseudo_panel_event,\n",
        "        on=['Municipality_ID'] + master_config[\"algorithm_config_parameters\"][\"PSEUDO_PANEL_GROUPS\"].keys(),\n",
        "        suffixes=('_base', '_event'),\n",
        "        how='inner' # Keep only cells that exist in both years.\n",
        "    )\n",
        "\n",
        "    # Outcome: Δlog_w̄_krt = log_w̄_krt - log_w̄_kr,1990\n",
        "    merged_panel['pseudo_panel_outcome'] = merged_panel['mean_log_wage_event'] - merged_panel['mean_log_wage_base']\n",
        "\n",
        "    # The weight is the baseline cell size.\n",
        "    weight_col = 'baseline_cell_size_base'\n",
        "    merged_panel.rename(columns={'baseline_cell_size_base': weight_col}, inplace=True)\n",
        "\n",
        "    # Merge municipality-level shocks, instruments, and cluster IDs.\n",
        "    # The unit of observation is now the cell (Municipality_ID + group).\n",
        "    event_data_year = event_study_df.loc[(slice(None), event_year), :].reset_index()\n",
        "\n",
        "    analysis_data = merged_panel.reset_index().merge(\n",
        "        event_data_year, on='Municipality_ID', how='left'\n",
        "    ).dropna()\n",
        "\n",
        "    # --- Step 3: Estimate 2SLS at the Cell Level ---\n",
        "\n",
        "    # Create a temporary config to pass the custom weight column name.\n",
        "    temp_config = copy.deepcopy(master_config)\n",
        "    temp_config['algorithm_config_parameters']['WEIGHT_COLUMN_MUNICIPALITY'] = weight_col\n",
        "\n",
        "    # Call the master 2SLS estimator. The data must be temporarily reshaped\n",
        "    # to match the expected input format (MultiIndex with year).\n",
        "    analysis_data['snapshot_year'] = event_year\n",
        "    analysis_data = analysis_data.set_index(['Municipality_ID', 'snapshot_year'])\n",
        "\n",
        "    print(f\"\\n--- Estimating Pseudo-Panel Model (γ^PP) for Year: {event_year} ---\")\n",
        "    results = estimate_regional_effect_2sls(\n",
        "        event_study_df=analysis_data,\n",
        "        master_config=temp_config,\n",
        "        outcome_variable='pseudo_panel_outcome',\n",
        "        event_year=event_year\n",
        "    )\n",
        "\n",
        "    print(\"\\nTask 24: Pseudo-panel cross-check completed successfully.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "2LW3y7R3eb6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 25: Compile final outputs, tables, and figures\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 25: Compile final outputs, tables, and figures\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 25, Step 1: Helpers to Format and Create Publication Tables\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _format_result(\n",
        "    result: Dict[str, Any],\n",
        "    flip_sign: bool = False\n",
        ") -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Formats a single estimation result into a (point_estimate, std_error) string tuple.\n",
        "\n",
        "    This utility takes a dictionary of estimation results, formats the point\n",
        "    estimate and standard error to three decimal places, and adds significance\n",
        "    stars. Stars (***, **, *) are determined by conventional p-value thresholds\n",
        "    if the bootstrap confidence interval, the most robust measure of uncertainty,\n",
        "    does not contain zero.\n",
        "\n",
        "    Args:\n",
        "        result (Dict[str, Any]): A dictionary containing at least 'point_estimate',\n",
        "                                 'cluster_robust_se', 'bootstrap_ci', and 'p_value'.\n",
        "        flip_sign (bool): If True, the sign of the point estimate is flipped\n",
        "                          before formatting. This is required for components like\n",
        "                          displacement that enter the decomposition identity with\n",
        "                          a negative sign.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[str, str]: A tuple containing two strings:\n",
        "                         - The formatted point estimate with significance stars.\n",
        "                         - The formatted standard error enclosed in parentheses.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Handle cases where the result dictionary is empty or incomplete, returning placeholders.\n",
        "    if not result or 'point_estimate' not in result or 'cluster_robust_se' not in result:\n",
        "        return \"-\", \"(-)\"\n",
        "\n",
        "    # --- Data Extraction ---\n",
        "\n",
        "    # Extract the point estimate and optionally flip its sign for decomposition tables.\n",
        "    pe = result['point_estimate'] * (-1 if flip_sign else 1)\n",
        "\n",
        "    # Extract the cluster-robust standard error.\n",
        "    se = result['cluster_robust_se']\n",
        "\n",
        "    # Safely extract the bootstrap confidence interval.\n",
        "    ci = result.get('bootstrap_ci')\n",
        "\n",
        "    # --- Significance Star Determination ---\n",
        "\n",
        "    # Initialize the significance stars string.\n",
        "    stars = \"\"\n",
        "\n",
        "    # Determine significance only if a valid bootstrap confidence interval is provided.\n",
        "    if ci and len(ci) == 2:\n",
        "        # The effect is statistically significant if the confidence interval does not include zero.\n",
        "        # This is a more robust check than relying solely on the p-value.\n",
        "        if (ci[0] > 0 and ci[1] > 0) or (ci[0] < 0 and ci[1] < 0):\n",
        "            # Use conventional p-value thresholds for the star notation (*** for p<0.01, etc.).\n",
        "            p_val = result.get('p_value', 1.0)\n",
        "            if p_val < 0.01: stars = \"***\"\n",
        "            elif p_val < 0.05: stars = \"**\"\n",
        "            elif p_val < 0.10: stars = \"*\"\n",
        "\n",
        "    # --- Formatting and Return ---\n",
        "\n",
        "    # Return the formatted point estimate and standard error as a tuple of strings.\n",
        "    return f\"{pe:.3f}{stars}\", f\"({se:.3f})\"\n",
        "\n",
        "def create_table_1(\n",
        "    results: Dict[str, Any],\n",
        "    year: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a pandas DataFrame that replicates Table 1 (Employment Decomposition).\n",
        "\n",
        "    This function navigates the nested results dictionary to find the employment\n",
        "    decomposition estimates for a specific year. It then uses the `_format_result`\n",
        "    utility to format each component and assembles them into a publication-quality\n",
        "    DataFrame that mirrors the structure of the paper's Table 1.\n",
        "\n",
        "    Args:\n",
        "        results (Dict[str, Any]): The comprehensive results dictionary from the\n",
        "                                  main analysis pipeline.\n",
        "        year (int): The specific event year for which to generate the table.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A formatted DataFrame ready for display or export.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If the required 'employment_decomposition' results are not\n",
        "                  found for the specified year in the input dictionary.\n",
        "    \"\"\"\n",
        "    # --- Data Extraction ---\n",
        "\n",
        "    # Navigate to the relevant section of the results dictionary.\n",
        "    # A KeyError will be raised if the path is invalid, providing a clear error.\n",
        "    try:\n",
        "        data = results['main_tables'][year]['employment_decomposition']\n",
        "    except KeyError:\n",
        "        raise KeyError(f\"Could not find 'employment_decomposition' results for year {year}.\")\n",
        "\n",
        "    # --- Table Construction ---\n",
        "\n",
        "    # Construct the table data by formatting each component.\n",
        "    # Note the sign flips for displacement and relocation, as per the paper's\n",
        "    # decomposition identity: ΔE ≈ -Disp + CrowdOut - Reloc.\n",
        "    table_data = {\n",
        "        \"Regional Effect\": _format_result(data.get('total_effect')),\n",
        "        \"Displacement (-)\": _format_result(data.get('displacement'), flip_sign=True),\n",
        "        \"Crowding-Out (+)\": _format_result(data.get('inflow')),\n",
        "        \"Relocation (-)\": _format_result(data.get('relocation'), flip_sign=True),\n",
        "    }\n",
        "\n",
        "    # Create the pandas DataFrame from the formatted data.\n",
        "    df = pd.DataFrame(table_data, index=['Coefficient', 'Std. Error'])\n",
        "\n",
        "    # Set a descriptive name for the columns index.\n",
        "    df.columns.name = f\"Table 1 (Replica): Employment Decomposition ({year} vs 1990)\"\n",
        "\n",
        "    # Return the final formatted table.\n",
        "    return df\n",
        "\n",
        "def create_table_2(\n",
        "    results: Dict[str, Any],\n",
        "    year: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a pandas DataFrame that replicates Table 2 (Wage Decomposition).\n",
        "\n",
        "    This function navigates the nested results dictionary to find the wage\n",
        "    decomposition estimates for a specific year. It formats each component\n",
        "    (regional effect, pure effect, and composition terms) and assembles them\n",
        "    into a publication-quality DataFrame mirroring the paper's Table 2.\n",
        "\n",
        "    Args:\n",
        "        results (Dict[str, Any]): The comprehensive results dictionary from the\n",
        "                                  main analysis pipeline.\n",
        "        year (int): The specific event year for which to generate the table.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A formatted DataFrame ready for display or export.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If the required 'wage_decomposition' results are not\n",
        "                  found for the specified year in the input dictionary.\n",
        "    \"\"\"\n",
        "    # --- Data Extraction ---\n",
        "\n",
        "    # Navigate to the relevant section of the results dictionary.\n",
        "    try:\n",
        "        data = results['main_tables'][year]['wage_decomposition']\n",
        "        comp_components = data.get('composition_components', {})\n",
        "    except KeyError:\n",
        "        raise KeyError(f\"Could not find 'wage_decomposition' results for year {year}.\")\n",
        "\n",
        "    # --- Table Construction ---\n",
        "\n",
        "    # Construct the table data by formatting each component.\n",
        "    # Note the sign flip for the inflow selection term as per the paper's presentation.\n",
        "    table_data = {\n",
        "        \"Regional Wage Effect (γ^R)\": _format_result(data.get('regional_wage_effect')),\n",
        "        \"Pure Wage Effect (γ^W)\": _format_result(data.get('pure_wage_effect')),\n",
        "        \"Compositional Effect\": (f\"{data.get('composition_effect_total', 0.0):.3f}\", \"\"),\n",
        "        \"  - Outflow Selection\": _format_result(comp_components.get('outflow_selection')),\n",
        "        \"  - Inflow Selection\": _format_result(comp_components.get('inflow_selection'), flip_sign=True),\n",
        "        \"  - Age Selection\": (f\"{comp_components.get('age_selection', 0.0):.3f}\", \"\"),\n",
        "    }\n",
        "\n",
        "    # Create the pandas DataFrame from the formatted data.\n",
        "    df = pd.DataFrame(table_data, index=['Coefficient', 'Std. Error'])\n",
        "\n",
        "    # Set a descriptive name for the columns index.\n",
        "    df.columns.name = f\"Table 2 (Replica): Wage Decomposition ({year} vs 1990)\"\n",
        "\n",
        "    # Return the final formatted table.\n",
        "    return df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 25, Step 2: Helpers to Generate and Plot Event Studies\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_full_event_study(\n",
        "    outcomes_to_run: Dict[str, str],\n",
        "    event_study_df: pd.DataFrame,\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Runs estimations for multiple outcomes over all event study years.\n",
        "\n",
        "    This is a computationally intensive helper function that generates the full\n",
        "    time-series of coefficients needed for creating event-study plots. It iterates\n",
        "    through a given set of outcomes and all pre- and post-treatment years,\n",
        "    calling the appropriate master estimation function for each combination.\n",
        "\n",
        "    Args:\n",
        "        outcomes_to_run (Dict[str, str]): A dictionary mapping the name of an\n",
        "            outcome column in `event_study_df` (e.g., 'emp_outcome') to the\n",
        "            required model type ('regional' for municipality-level 2SLS or\n",
        "            'pure_wage' for individual-level FD-IV).\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        analysis_panel (pd.DataFrame): The full worker-year panel, required for\n",
        "            individual-level models like the pure wage effect.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A tidy DataFrame where each row contains the complete\n",
        "                      estimation results for a single outcome in a single year.\n",
        "                      Includes an 'outcome_name' column for easy filtering.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unknown model_type is provided in `outcomes_to_run`.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(outcomes_to_run, dict):\n",
        "        raise TypeError(\"`outcomes_to_run` must be a dictionary.\")\n",
        "\n",
        "    # --- 1. Define the Time Period ---\n",
        "\n",
        "    # Define the full range of years for the event-study plot from the config.\n",
        "    event_years = master_config[\"temporal_parameters\"][\"PRE_TREATMENT_YEARS_FOR_TESTING\"] + \\\n",
        "                  master_config[\"temporal_parameters\"][\"POST_TREATMENT_YEARS\"]\n",
        "\n",
        "    # --- 2. Iterative Estimation ---\n",
        "\n",
        "    # Initialize a list to store the dictionary of results from each estimation run.\n",
        "    all_results: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Loop through each outcome and each year specified for the analysis.\n",
        "    for outcome, model_type in outcomes_to_run.items():\n",
        "        for year in sorted(event_years):\n",
        "            # Select the appropriate estimation function based on the required model type.\n",
        "            if model_type == 'regional':\n",
        "                # For regional outcomes, call the municipality-level 2SLS estimator.\n",
        "                res = estimate_regional_effect_2sls(event_study_df, master_config, outcome, year)\n",
        "            elif model_type == 'pure_wage':\n",
        "                # For the pure wage effect, call the individual-level FD-IV estimator.\n",
        "                res = _estimate_pure_wage_effect_stayers(analysis_panel, event_study_df, master_config, year)\n",
        "            else:\n",
        "                # Raise an error for unsupported model types.\n",
        "                raise ValueError(f\"Unknown model_type '{model_type}' for pre-trend estimation.\")\n",
        "\n",
        "            # If the estimation was successful (returned a non-empty result),\n",
        "            # add an identifier for the outcome and append it to the results list.\n",
        "            if res:\n",
        "                res['outcome_name'] = outcome\n",
        "                all_results.append(res)\n",
        "\n",
        "    # --- 3. Finalization ---\n",
        "\n",
        "    # Convert the list of result dictionaries into a single, tidy DataFrame.\n",
        "    return pd.DataFrame(all_results)\n",
        "\n",
        "def _plot_multi_series_event_study(\n",
        "    results_map: Dict[str, pd.DataFrame],\n",
        "    title: str,\n",
        "    y_label: str,\n",
        "    base_year: int = 1990\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates a publication-quality event-study plot with multiple series.\n",
        "\n",
        "    This function takes a dictionary mapping series names to their corresponding\n",
        "    results DataFrames and plots them on the same axes for comparison. It is\n",
        "    designed to replicate the style of figures like Figure 1B in the paper.\n",
        "\n",
        "    Args:\n",
        "        results_map (Dict[str, pd.DataFrame]): A dictionary where keys are the\n",
        "            legend labels for each series (e.g., 'Regional Wage Effect') and\n",
        "            values are the DataFrames containing the estimation results for that\n",
        "            series over time.\n",
        "        title (str): The main title for the plot.\n",
        "        y_label (str): The label for the y-axis.\n",
        "        base_year (int): The base year, used to draw the treatment start line.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a DataFrame in `results_map` is missing required columns.\n",
        "    \"\"\"\n",
        "    # --- Plotting Setup ---\n",
        "\n",
        "    # Set a professional plot style.\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "    # Create the figure and axes objects.\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "    # Define color and marker cycles for plotting multiple series.\n",
        "    colors = ['black', 'blue', 'green', 'purple']\n",
        "    markers = ['s', 'o', '^', 'D']\n",
        "\n",
        "    # --- Loop and Plot Each Series ---\n",
        "\n",
        "    # Iterate through the provided dictionary of result series.\n",
        "    for i, (series_name, df) in enumerate(results_map.items()):\n",
        "        # Skip if the DataFrame for a series is empty.\n",
        "        if df.empty:\n",
        "            print(f\"Warning: No data for series '{series_name}'. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Validate that the DataFrame has the required columns.\n",
        "        required_cols = {'event_year', 'point_estimate', 'bootstrap_ci'}\n",
        "        if not required_cols.issubset(df.columns):\n",
        "            raise ValueError(f\"DataFrame for series '{series_name}' is missing required columns.\")\n",
        "\n",
        "        # Prepare data for plotting.\n",
        "        df = df.sort_values('event_year').copy()\n",
        "        df[['ci_lower', 'ci_upper']] = pd.DataFrame(df['bootstrap_ci'].tolist(), index=df.index)\n",
        "\n",
        "        # Plot the point estimates for the current series.\n",
        "        ax.plot(df['event_year'], df['point_estimate'], marker=markers[i % len(markers)],\n",
        "                linestyle='-', color=colors[i % len(colors)], label=series_name)\n",
        "\n",
        "        # Plot the 95% confidence interval as a shaded region.\n",
        "        ax.fill_between(df['event_year'], df['ci_lower'], df['ci_upper'], color=colors[i % len(colors)], alpha=0.1)\n",
        "\n",
        "    # --- Formatting and Finalization ---\n",
        "\n",
        "    # Add a horizontal reference line at y=0 (null effect).\n",
        "    ax.axhline(0, color='black', linestyle='-', linewidth=0.8)\n",
        "\n",
        "    # Add a vertical reference line to demarcate the start of the treatment.\n",
        "    ax.axvline(base_year + 0.5, color='red', linestyle='--', linewidth=1.2, label=f'Treatment Start')\n",
        "\n",
        "    # Set labels and title with appropriate font sizes.\n",
        "    ax.set_xlabel(\"Year\", fontsize=12)\n",
        "    ax.set_ylabel(y_label, fontsize=12)\n",
        "    ax.set_title(title, fontsize=14, weight='bold')\n",
        "\n",
        "    # Add a legend.\n",
        "    ax.legend(fontsize=10)\n",
        "\n",
        "    # Ensure the x-axis ticks are integers representing years.\n",
        "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\n",
        "    # Adjust layout and display the plot.\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 25, Step 3: Helper to Document Structural Parameters\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _summarize_structural_parameters(\n",
        "    results: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Prints a formatted summary of the recovered structural parameters from Task 20.\n",
        "\n",
        "    This function extracts the final structural estimates from the results\n",
        "    dictionary and presents them in a clear, human-readable format. It also\n",
        "    includes the crucial validation check that contrasts the main results with\n",
        "    the biased estimates that would be obtained using the naive regional wage effect.\n",
        "\n",
        "    Args:\n",
        "        results (Dict[str, Any]): The comprehensive results dictionary from the\n",
        "                                  main analysis pipeline.\n",
        "    \"\"\"\n",
        "    # --- Data Extraction ---\n",
        "\n",
        "    # Safely navigate to the structural parameters section of the results.\n",
        "    params = results.get('structural_parameters')\n",
        "\n",
        "    # If the parameters are not found, print a message and exit.\n",
        "    if not params:\n",
        "        print(\"Structural parameters not found in results dictionary.\")\n",
        "        return\n",
        "\n",
        "    # --- Print Formatted Summary ---\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\nSummary of Recovered Structural Parameters (Task 20)\\n\" + \"=\"*80)\n",
        "    print(\"Main Results (using Pure Wage Effect γ^W):\")\n",
        "    print(f\"  Efficiency Scaling Factor (c):                                {params['structural_parameters']['c_scaling_factor']:.3f}\")\n",
        "    print(f\"  Population-Weighted Supply Elasticity (η̄^P):                  {params['structural_parameters']['eta_P_population_supply_elasticity']:.3f}\")\n",
        "    print(f\"  Efficiency-Weighted Supply Elasticity (η̄^E):                  {params['structural_parameters']['eta_E_efficiency_supply_elasticity']:.3f}\")\n",
        "    print(f\"  Inverse Labor Demand Elasticity (φ):                          {params['structural_parameters']['phi_inverse_demand_elasticity']:.3f}\")\n",
        "    print(f\"  IMPLIED LABOR DEMAND ELASTICITY (1/φ):                        {params['structural_parameters']['demand_elasticity']:.3f}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(\"Validation Check (using naive Regional Wage Effect γ^R):\")\n",
        "    print(f\"  Naive Inverse Demand Elasticity (φ_naive):                    {params['validation_with_regional_effect']['phi_naive']:.3f}\")\n",
        "    print(f\"  NAIVE IMPLIED DEMAND ELASTICITY:                              {params['validation_with_regional_effect']['demand_elasticity_naive']:.3f}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 25, Step 3 Helper Functions\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _generate_all_figure_data(\n",
        "    event_study_df: pd.DataFrame,\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    regional_panel: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates the full time-series of estimation results needed for all event-study figures.\n",
        "\n",
        "    This is a computationally intensive helper that runs the key estimations for\n",
        "    every pre- and post-treatment year to produce a tidy DataFrame of results\n",
        "    suitable for plotting.\n",
        "\n",
        "    Args:\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        analysis_panel (pd.DataFrame): The full worker-year panel.\n",
        "        regional_panel (pd.DataFrame): The aggregated municipality-year panel.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A tidy DataFrame with all estimation results across all years.\n",
        "    \"\"\"\n",
        "    # Define the full range of years for the event-study plot.\n",
        "    event_years = master_config[\"temporal_parameters\"][\"PRE_TREATMENT_YEARS_FOR_TESTING\"] + \\\n",
        "                  master_config[\"temporal_parameters\"][\"POST_TREATMENT_YEARS\"]\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    # Loop through each year to generate the full time series of coefficients.\n",
        "    for year in sorted(event_years):\n",
        "        print(f\"\\n--- Generating Figure Data for Year: {year} ---\")\n",
        "\n",
        "        # --- Regional Employment Effect ---\n",
        "        res_emp = estimate_regional_effect_2sls(event_study_df, master_config, 'emp_outcome', year)\n",
        "        if res_emp:\n",
        "            res_emp['outcome_name'] = 'Regional Employment'\n",
        "            all_results.append(res_emp)\n",
        "\n",
        "        # --- Regional Wage Effect ---\n",
        "        res_wage = estimate_regional_effect_2sls(event_study_df, master_config, 'wage_outcome', year)\n",
        "        if res_wage:\n",
        "            res_wage['outcome_name'] = 'Regional Wage'\n",
        "            all_results.append(res_wage)\n",
        "\n",
        "        # --- Pure Wage Effect ---\n",
        "        res_pure_wage = _estimate_pure_wage_effect_stayers(analysis_panel, event_study_df, master_config, year)\n",
        "        if res_pure_wage:\n",
        "            res_pure_wage['outcome_name'] = 'Pure Wage'\n",
        "            all_results.append(res_pure_wage)\n",
        "\n",
        "        # --- Displacement Effect ---\n",
        "        # This requires re-computing the displacement share for each year.\n",
        "        disp_df = _compute_employment_flow_shares(analysis_panel, regional_panel, year, master_config)\n",
        "        temp_event_df = event_study_df.merge(disp_df[['displacement_share']], on='Municipality_ID', how='left')\n",
        "        res_disp = estimate_regional_effect_2sls(temp_event_df, master_config, 'displacement_share', year)\n",
        "        if res_disp:\n",
        "            res_disp['outcome_name'] = 'Displacement'\n",
        "            all_results.append(res_disp)\n",
        "\n",
        "    return pd.DataFrame(all_results)\n",
        "\n",
        "def _generate_all_figure_data(\n",
        "    event_study_df: pd.DataFrame,\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    regional_panel: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates the full time-series of estimation results needed for all event-study figures.\n",
        "\n",
        "    This is a computationally intensive helper that runs the key estimations for\n",
        "    every pre- and post-treatment year to produce a tidy DataFrame of results\n",
        "    suitable for plotting.\n",
        "\n",
        "    Args:\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset from Task 9.\n",
        "        analysis_panel (pd.DataFrame): The full worker-year panel.\n",
        "        regional_panel (pd.DataFrame): The aggregated municipality-year panel.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A tidy DataFrame with all estimation results across all years.\n",
        "    \"\"\"\n",
        "    # Define the full range of years for the event-study plot.\n",
        "    event_years = master_config[\"temporal_parameters\"][\"PRE_TREATMENT_YEARS_FOR_TESTING\"] + \\\n",
        "                  master_config[\"temporal_parameters\"][\"POST_TREATMENT_YEARS\"]\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    # Loop through each year to generate the full time series of coefficients.\n",
        "    for year in sorted(event_years):\n",
        "        print(f\"\\n--- Generating Figure Data for Year: {year} ---\")\n",
        "\n",
        "        # --- Regional Employment Effect ---\n",
        "        res_emp = estimate_regional_effect_2sls(event_study_df, master_config, 'emp_outcome', year)\n",
        "        if res_emp:\n",
        "            res_emp['outcome_name'] = 'Regional Employment'\n",
        "            all_results.append(res_emp)\n",
        "\n",
        "        # --- Regional Wage Effect ---\n",
        "        res_wage = estimate_regional_effect_2sls(event_study_df, master_config, 'wage_outcome', year)\n",
        "        if res_wage:\n",
        "            res_wage['outcome_name'] = 'Regional Wage'\n",
        "            all_results.append(res_wage)\n",
        "\n",
        "        # --- Pure Wage Effect ---\n",
        "        res_pure_wage = _estimate_pure_wage_effect_stayers(analysis_panel, event_study_df, master_config, year)\n",
        "        if res_pure_wage:\n",
        "            res_pure_wage['outcome_name'] = 'Pure Wage'\n",
        "            all_results.append(res_pure_wage)\n",
        "\n",
        "        # --- Displacement Effect ---\n",
        "        # This requires re-computing the displacement share for each year.\n",
        "        disp_df = _compute_employment_flow_shares(analysis_panel, regional_panel, year, master_config)\n",
        "        temp_event_df = event_study_df.merge(disp_df[['displacement_share']], on='Municipality_ID', how='left')\n",
        "        res_disp = estimate_regional_effect_2sls(temp_event_df, master_config, 'displacement_share', year)\n",
        "        if res_disp:\n",
        "            res_disp['outcome_name'] = 'Displacement'\n",
        "            all_results.append(res_disp)\n",
        "\n",
        "    return pd.DataFrame(all_results)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 25, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compile_final_outputs(\n",
        "    final_results: Dict[str, Any],\n",
        "    event_study_df: pd.DataFrame,\n",
        "    analysis_panel: pd.DataFrame,\n",
        "    regional_panel: pd.DataFrame,\n",
        "    master_config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation of all final tables, figures, and summaries.\n",
        "\n",
        "    This function serves as the final reporting layer of the entire analysis\n",
        "    pipeline. It takes the comprehensive results dictionary from the main\n",
        "    orchestrator and uses a series of specialized helper functions to generate\n",
        "    and display publication-quality outputs that replicate the key exhibits\n",
        "    from the paper.\n",
        "\n",
        "    Args:\n",
        "        final_results (Dict[str, Any]): The nested dictionary of results from\n",
        "            the `run_full_analysis_pipeline` function.\n",
        "        event_study_df (pd.DataFrame): The main analysis dataset, needed to\n",
        "            generate full event-study series for plotting.\n",
        "        analysis_panel (pd.DataFrame): The full worker-year panel, needed for\n",
        "            individual-level estimations for plotting.\n",
        "        regional_panel (pd.DataFrame): The aggregated municipality-year panel.\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Reproduce Main Tables ---\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\nGENERATING PUBLICATION TABLES\\n\" + \"=\"*80)\n",
        "    for year in final_results.get('main_tables', {}).keys():\n",
        "        print(create_table_1(final_results, year).to_string())\n",
        "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "        print(create_table_2(final_results, year).to_string())\n",
        "        # ... calls to create other tables (e.g., Table 3, 4) would follow here ...\n",
        "\n",
        "    # --- Step 2: Reproduce Main Figures ---\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\nGENERATING EVENT-STUDY FIGURES\\n\" + \"=\"*80)\n",
        "\n",
        "    # a. Generate all time-series data required for the figures in one go.\n",
        "    figure_data_df = _generate_all_figure_data(\n",
        "        event_study_df, analysis_panel, regional_panel, master_config\n",
        "    )\n",
        "\n",
        "    # b. Plot Figure 1A (Regional Employment vs. Displacement).\n",
        "    # The paper plots -displacement, so we flip the sign of the point estimate and CIs.\n",
        "    disp_plot_data = figure_data_df[figure_data_df['outcome_name'] == 'Displacement'].copy()\n",
        "    disp_plot_data['point_estimate'] *= -1\n",
        "    disp_plot_data['bootstrap_ci'] = disp_plot_data['bootstrap_ci'].apply(lambda x: (-x[1], -x[0]))\n",
        "\n",
        "    _plot_multi_series_event_study(\n",
        "        results_map={\n",
        "            'Regional Employment Effect': figure_data_df[figure_data_df['outcome_name'] == 'Regional Employment'],\n",
        "            'Displacement Effect (-)': disp_plot_data\n",
        "        },\n",
        "        title='Figure 1A (Replica): Impact on Regional Employment and Displacement',\n",
        "        y_label='Coefficient'\n",
        "    )\n",
        "\n",
        "    # c. Plot Figure 1B (Regional Wage vs. Pure Wage).\n",
        "    _plot_multi_series_event_study(\n",
        "        results_map={\n",
        "            'Regional Wage Effect': figure_data_df[figure_data_df['outcome_name'] == 'Regional Wage'],\n",
        "            'Pure Wage Effect': figure_data_df[figure_data_df['outcome_name'] == 'Pure Wage']\n",
        "        },\n",
        "        title='Figure 1B (Replica): Regional vs. Pure Wage Effects',\n",
        "        y_label='Coefficient'\n",
        "    )\n",
        "\n",
        "    # d. Plot Figure 3 (Apprenticeship Uptake).\n",
        "    fig3_results_df = pd.DataFrame(final_results.get('event_studies', {}).get('apprenticeship_uptake', []))\n",
        "    _plot_multi_series_event_study(\n",
        "        results_map={'Apprenticeship Uptake': fig3_results_df},\n",
        "        title='Figure 3 (Replica): Impact on Native Apprenticeships',\n",
        "        y_label='Coefficient'\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Document Structural Parameters ---\n",
        "    _summarize_structural_parameters(final_results)\n",
        "\n",
        "    print(\"\\nTask 25: Final output compilation completed successfully.\")\n"
      ],
      "metadata": {
        "id": "vBz29QFefuBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Final Task: Top-Level Orchestrator for Full Project Execution\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Top-Level Orchestrator, helper function for data preparation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_and_cache_prep_pipeline(\n",
        "    initial_artifacts: Dict[str, Any],\n",
        "    master_config: Dict[str, Any],\n",
        "    cache_path: Path,\n",
        "    force_rerun_prep: bool\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the full data preparation pipeline (Tasks 3-9) with robust caching.\n",
        "\n",
        "    This function manages the sequential execution of all data preparation tasks,\n",
        "    from cleansing raw data to creating the final event-study DataFrame. It\n",
        "    implements a waterfall caching logic: if a step is re-run, all subsequent\n",
        "    steps are also re-run to ensure data consistency.\n",
        "\n",
        "    Args:\n",
        "        initial_artifacts (Dict[str, Any]): Dictionary containing the initial\n",
        "            validated artifacts from Task 1 & 2 (e.g., raw data, auxiliary tables).\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        cache_path (Path): The path to the cache directory.\n",
        "        force_rerun_prep (bool): If True, ignores all caches and re-runs all steps.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The dictionary of artifacts, now populated with all\n",
        "                        DataFrames from the preparation pipeline.\n",
        "    \"\"\"\n",
        "    # This dictionary will hold all major data artifacts.\n",
        "    artifacts = initial_artifacts.copy()\n",
        "\n",
        "    # This flag controls the cache waterfall. If it becomes False, all subsequent steps must re-run.\n",
        "    is_cache_valid = not force_rerun_prep\n",
        "\n",
        "    # Define the full data preparation pipeline as a list of tasks.\n",
        "    # Each tuple: (name, function, input_map, output_keys, cache_filename)\n",
        "    prep_pipeline = [\n",
        "        (\"3: Cleanse & Canonicalize\", cleanse_and_canonicalize_spells,\n",
        "         {'consolidated_df_raw': artifacts['consolidated_df_raw']},\n",
        "         ['df_normalized', 'panel_full_with_flags', 'panel_main_analysis'], 'task3.pkl'),\n",
        "        (\"4: Impute Wages\", impute_censored_wages,\n",
        "         {'worker_year_panel': 'panel_main_analysis'},\n",
        "         'panel_main_with_wages', 'task4.pkl'),\n",
        "        (\"5: Build Analysis Panel\", build_analysis_panel,\n",
        "         {'analysis_panel_employed': 'panel_main_with_wages', 'all_spells_cleansed': 'df_normalized', 'validated_artifacts': artifacts},\n",
        "         'analysis_panel', 'task5.pkl'),\n",
        "        (\"6: Aggregate Regional Panel\", aggregate_to_regional_panel,\n",
        "         {'analysis_panel': 'analysis_panel'},\n",
        "         ['regional_panel', 'national_wage_series'], 'task6.pkl'),\n",
        "        (\"7: Construct Shock\", construct_immigration_shock,\n",
        "         {'analysis_panel': 'analysis_panel', 'regional_panel': 'regional_panel'},\n",
        "         'shock_df', 'task7.pkl'),\n",
        "        (\"8: Construct Instruments\", construct_instrumental_variables,\n",
        "         {'analysis_panel': 'analysis_panel', 'validated_artifacts': artifacts},\n",
        "         'instruments_df', 'task8.pkl'),\n",
        "        (\"9: Prepare Event Study DF\", prepare_event_study_dataset,\n",
        "         {'regional_panel': 'regional_panel', 'shock_df': 'shock_df',\n",
        "          'instruments_df': 'instruments_df', 'analysis_panel': 'analysis_panel'},\n",
        "         'event_study_df', 'task9.pkl')\n",
        "    ]\n",
        "\n",
        "    # Execute the preparation pipeline.\n",
        "    for name, func, kwargs_map, out_keys, cache_file in prep_pipeline:\n",
        "        task_start = time.time()\n",
        "        cache_filepath = cache_path / cache_file\n",
        "\n",
        "        if is_cache_valid and cache_filepath.exists():\n",
        "            print(f\"Loading cached result for Task '{name}'...\")\n",
        "            with open(cache_filepath, 'rb') as f: result = pickle.load(f)\n",
        "        else:\n",
        "            print(f\"Running Task '{name}'...\")\n",
        "            is_cache_valid = False # Invalidate cache for all subsequent steps\n",
        "            kwargs = {k: artifacts[v] for k, v in kwargs_map.items()}\n",
        "            kwargs['master_config'] = master_config\n",
        "            result = func(**kwargs)\n",
        "            with open(cache_filepath, 'wb') as f: pickle.dump(result, f)\n",
        "\n",
        "        if isinstance(out_keys, list):\n",
        "            for i, key in enumerate(out_keys): artifacts[key] = result[i]\n",
        "        else:\n",
        "            artifacts[out_keys] = result\n",
        "\n",
        "        duration = time.time() - task_start\n",
        "        print(f\"--- Task '{name}' completed in {duration:.2f} seconds. ---\\n\")\n",
        "\n",
        "    return artifacts\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Top-Level Orchestrator: Main Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def execute_decomposition_toolkit_pipeline(\n",
        "    raw_data_path: str,\n",
        "    master_config: Dict[str, Any],\n",
        "    main_result_years: List[int] = [1993, 1995],\n",
        "    cache_dir: str = \"./.cache/\",\n",
        "    force_rerun_prep: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete, end-to-end analysis pipeline for the study.\n",
        "\n",
        "    This top-level orchestrator serves as the main entry point for the entire\n",
        "    replication project. It manages the full workflow, from initial data\n",
        "    validation to the final generation of tables and figures. The pipeline is\n",
        "    divided into four distinct phases:\n",
        "\n",
        "    1.  **Validation**: All input data, auxiliary files, and the master\n",
        "        configuration are rigorously validated before any processing begins.\n",
        "    2.  **Data Preparation**: A sequence of tasks (3-9) that transform the raw\n",
        "        spell data into the analysis-ready artifacts (`analysis_panel`,\n",
        "        `event_study_df`, etc.). This phase is cached to accelerate re-runs.\n",
        "    3.  **Main Analysis**: The core estimation tasks (10-20) are run to produce\n",
        "        the main findings for the specified result years.\n",
        "    4.  **Robustness & Output**: A comprehensive suite of robustness checks\n",
        "        (Tasks 22-24) is executed, and the final results are compiled into\n",
        "        publication-quality tables and figures (Task 25).\n",
        "\n",
        "    Args:\n",
        "        raw_data_path (str): The file path to the raw, consolidated spell-level\n",
        "                             DataFrame (e.g., a Parquet file).\n",
        "        master_config (Dict[str, Any]): The master configuration dictionary that\n",
        "                                        governs the entire analysis.\n",
        "        main_result_years (List[int]): A list of the primary event years for which\n",
        "                                       to generate the main decomposition tables.\n",
        "        cache_dir (str): A directory path for storing and retrieving intermediate\n",
        "                         data artifacts to speed up subsequent runs.\n",
        "        force_rerun_prep (bool): If True, all data preparation steps will be\n",
        "                                 re-computed, ignoring any existing cache.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive, nested dictionary containing all generated\n",
        "                        artifacts and estimation results from the entire pipeline.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Propagates any exception that occurs during the pipeline\n",
        "                   execution after logging a failure message.\n",
        "    \"\"\"\n",
        "    # --- Phase 0: Setup ---\n",
        "\n",
        "    # Record the start time to measure total execution duration.\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Print a banner to indicate the start of the pipeline.\n",
        "    print(\"=\"*80 + \"\\nSTARTING TOP-LEVEL ORCHESTRATOR\\n\" + \"=\"*80)\n",
        "\n",
        "    # This dictionary will hold all major data artifacts as they are created.\n",
        "    artifacts: Dict[str, Any] = {}\n",
        "\n",
        "    # This dictionary will hold all final estimation results.\n",
        "    all_results: Dict[str, Any] = {'main_tables': {}, 'event_studies': {}, 'robustness_checks': {}}\n",
        "\n",
        "    try:\n",
        "        # --- Phase 1: Validation ---\n",
        "\n",
        "        # Start timer for this phase.\n",
        "        phase_start = time.time()\n",
        "        print(\"--- Phase 1: Validating Inputs ---\")\n",
        "\n",
        "        # Load the raw data from the specified path.\n",
        "        consolidated_df_raw = pd.read_parquet(raw_data_path)\n",
        "        artifacts['consolidated_df_raw'] = consolidated_df_raw\n",
        "\n",
        "        # Run Task 1: Validate the raw data schema and integrity.\n",
        "        validate_consolidated_df_raw(consolidated_df_raw, master_config)\n",
        "\n",
        "        # Run Task 2: Validate the config and load/validate all auxiliary data artifacts.\n",
        "        artifacts.update(validate_artifacts_and_config(master_config, consolidated_df_raw))\n",
        "\n",
        "        # Log completion of the phase.\n",
        "        print(f\"\\n>>> PHASE 'Validation' completed in {time.time() - phase_start:.2f} seconds. <<<\\n\")\n",
        "\n",
        "        # --- Phase 2: Data Preparation ---\n",
        "\n",
        "        # Start timer for this phase.\n",
        "        phase_start = time.time()\n",
        "        print(\"--- Phase 2: Preparing Data Artifacts (with Caching) ---\")\n",
        "\n",
        "        # Execute the full data preparation pipeline (Tasks 3-9) with caching.\n",
        "        # This populates the `artifacts` dictionary with all necessary DataFrames.\n",
        "        artifacts.update(_run_and_cache_prep_pipeline(\n",
        "            artifacts, master_config, Path(cache_dir), force_rerun_prep\n",
        "        ))\n",
        "\n",
        "        # Log completion of the phase.\n",
        "        print(f\"\\n>>> PHASE 'Data Preparation' completed in {time.time() - phase_start:.2f} seconds. <<<\\n\")\n",
        "\n",
        "        # --- Phase 3: Main Analysis ---\n",
        "\n",
        "        # Start timer for this phase.\n",
        "        phase_start = time.time()\n",
        "        print(\"--- Phase 3: Running Main Analysis ---\")\n",
        "\n",
        "        # Loop through the specified years to generate the main table results.\n",
        "        for year in main_result_years:\n",
        "            # Initialize a dictionary to hold results for the current year.\n",
        "            year_results: Dict[str, Any] = {}\n",
        "            print(f\"\\n{'='*30} RUNNING ESTIMATIONS FOR YEAR: {year} {'='*30}\\n\")\n",
        "\n",
        "            # Task 12: Estimate regional and pure wage effects.\n",
        "            wage_effects = estimate_wage_effects(\n",
        "                artifacts['event_study_df'], artifacts['analysis_panel'], master_config, year\n",
        "            )\n",
        "            year_results['wage_effects'] = wage_effects\n",
        "\n",
        "            # Task 11: Decompose regional employment effect.\n",
        "            year_results['employment_decomposition'] = decompose_regional_employment_effect(\n",
        "                artifacts['analysis_panel'], artifacts['regional_panel'], artifacts['event_study_df'], master_config, year\n",
        "            )\n",
        "            # Task 13: Decompose regional wage effect (depends on Task 12 results).\n",
        "            year_results['wage_decomposition'] = decompose_regional_wage_effect(\n",
        "                artifacts['analysis_panel'], artifacts['event_study_df'], wage_effects, master_config, year\n",
        "            )\n",
        "            # Task 14: Run selection bounding on the pure wage effect.\n",
        "            year_results['selection_bounds'] = bound_pure_wage_effect_selection(\n",
        "                artifacts['analysis_panel'], artifacts['event_study_df'], wage_effects['pure_wage_effect'], master_config, year\n",
        "            )\n",
        "            # Task 15: Analyze non-employed entrants.\n",
        "            year_results['non_employed_entrants'] = analyze_non_employed_entrants(\n",
        "                artifacts['analysis_panel'], artifacts['regional_panel'], artifacts['national_wage_series'], artifacts['event_study_df'], master_config, year\n",
        "            )\n",
        "            # Task 16: Analyze older workers.\n",
        "            year_results['older_workers'] = analyze_older_workers(\n",
        "                artifacts['analysis_panel'], artifacts['regional_panel'], artifacts['event_study_df'], master_config, year\n",
        "            )\n",
        "            # Task 17: Analyze task heterogeneity.\n",
        "            year_results['task_heterogeneity'] = analyze_task_heterogeneity(\n",
        "                artifacts['analysis_panel'], artifacts['event_study_df'], master_config, year\n",
        "            )\n",
        "            # Task 18: Decompose routine employment.\n",
        "            year_results['routine_decomposition'] = decompose_routine_employment(\n",
        "                artifacts['analysis_panel'], artifacts['event_study_df'], master_config, year\n",
        "            )\n",
        "            # Store all results for this year.\n",
        "            all_results['main_tables'][year] = year_results\n",
        "\n",
        "        # Task 19: Run event study for apprenticeship uptake (runs over all years).\n",
        "        all_results['event_studies']['apprenticeship_uptake'] = analyze_apprenticeship_uptake(\n",
        "            artifacts['panel_full_with_flags'], artifacts['event_study_df'], master_config\n",
        "        )\n",
        "\n",
        "        # Task 20: Recover structural parameters using results from the first main year.\n",
        "        main_year = main_result_years[0]\n",
        "        all_results['structural_parameters'] = recover_structural_parameters(\n",
        "            reduced_form_estimates={\n",
        "                'beta_R': all_results['main_tables'][main_year]['employment_decomposition']['total_effect']['point_estimate'],\n",
        "                'gamma_R': all_results['main_tables'][main_year]['wage_effects']['regional_wage_effect']['point_estimate'],\n",
        "                'gamma_W': all_results['main_tables'][main_year]['wage_effects']['pure_wage_effect']['point_estimate']\n",
        "            },\n",
        "            analysis_panel=artifacts['analysis_panel'],\n",
        "            master_config=master_config\n",
        "        )\n",
        "\n",
        "        # Log completion of the phase.\n",
        "        print(f\"\\n>>> PHASE 'Main Analysis' completed in {time.time() - phase_start:.2f} seconds. <<<\\n\")\n",
        "\n",
        "        # --- Phase 4: Robustness & Output ---\n",
        "\n",
        "        # Start timer for this phase.\n",
        "        phase_start = time.time()\n",
        "        print(\"--- Phase 4: Running Robustness Checks and Compiling Outputs ---\")\n",
        "\n",
        "        # Task 22: Run pre-trend analysis and generate event-study plots.\n",
        "        run_robustness_checks(\n",
        "            artifacts['event_study_df'], artifacts['analysis_panel'], all_results, master_config\n",
        "        )\n",
        "        # Task 23: Run sensitivity analyses.\n",
        "        all_results['robustness_checks']['sensitivity'] = run_sensitivity_analyses(\n",
        "            raw_data_path, artifacts['event_study_df'], master_config, main_result_years[0]\n",
        "        )\n",
        "        # Task 24: Run pseudo-panel cross-check.\n",
        "        all_results['robustness_checks']['pseudo_panel'] = run_pseudo_panel_check(\n",
        "            artifacts['analysis_panel'], artifacts['event_study_df'], all_results['main_tables'][main_year]['wage_effects'], master_config, main_result_years[0]\n",
        "        )\n",
        "        # Task 25: Compile all results into final tables and figures.\n",
        "        compile_final_outputs(\n",
        "            all_results, artifacts['event_study_df'], artifacts['analysis_panel'], artifacts['regional_panel'], master_config\n",
        "        )\n",
        "\n",
        "        # Log completion of the phase.\n",
        "        print(f\"\\n>>> PHASE 'Robustness & Output' completed in {time.time() - phase_start:.2f} seconds. <<<\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any exception that occurs during the pipeline, log it, and re-raise.\n",
        "        print(f\"\\n\\nPIPELINE FAILED WITH ERROR: {e}\")\n",
        "        raise\n",
        "\n",
        "    finally:\n",
        "        # --- Finalization ---\n",
        "        # This block runs whether the pipeline succeeded or failed.\n",
        "        total_duration = time.time() - start_time\n",
        "        print(\"=\"*80 + f\"\\nTOP-LEVEL ORCHESTRATOR COMPLETED in {total_duration / 60:.2f} minutes.\\n\" + \"=\"*80)\n",
        "\n",
        "    # Return the comprehensive dictionary of all generated results.\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "-thMTwR_3CBc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}